import os
import sys
from app.core.config import settings
if "npu" in settings.device and ":" in settings.device:
    physical_id = settings.device.split(":")[-1]
    os.environ["ASCEND_RT_VISIBLE_DEVICES"] = physical_id
    print(f"[System] å¼ºåˆ¶éš”ç¦» NPUï¼Œä»…å¯è§ç‰©ç†å¡: {physical_id}")
    # ç‰©ç†å¡å˜äº†ï¼Œé€»è¾‘å¡å·å¿…é¡»å½’é›¶
    TARGET_DEVICE = "npu:0"
    # æˆ–è€…ä½¿ç”¨ transfer_to_npu è¡¥ä¸ï¼Œå†™æˆ "cuda:0"
    # settings.device = "cuda:0"
from torch_npu.contrib import transfer_to_npu # è‡ªåŠ¨è¿ç§»ï¼šå°†cudaAPIæ˜ å°„ä¸ºnpuAPI
import asyncio
import torch
import torch_npu
import torch.nn.functional as F
from transformers import BertTokenizer, BertForSequenceClassification
import whisper
from modelscope.pipelines import pipeline
from modelscope.utils.constant import Tasks
from funasr import AutoModel
from modelscope.pipelines.audio.funasr_pipeline import FunASRPipeline
from app.utils.feature_utils import id2label

# å•ä¾‹ç¼“å­˜
_model_asr = None
_model_emotion = None
_model_online = None
_model_whisper = None
_model_speaker = None
_punct_pipeline = None
# äº”ä½•
_model_bert = None
_tokenizer = None
_bert_device = None

# print("PyTorchç‰ˆæœ¬:", torch.__version__)
# print("NPUè®¾å¤‡æ•°é‡:", torch_npu.npu.device_count())
# print("å½“å‰NPUè®¾å¤‡:", torch_npu.npu.get_device_name(0))
# print("Cuda available:", torch.cuda.is_available())
#
# print("å¼€å§‹")

# çº¿ç¨‹é”
_model_lock = asyncio.Lock()


def get_asr_model():
    return _model_asr


def get_emotion_model():
    return _model_emotion


def get_online_model():
    return _model_online


def get_whisper_model():
    return _model_whisper


def get_speaker_model():
    return _model_speaker


def get_punct_pipeline():
    return _punct_pipeline


def device() -> torch.device:
    if torch.npu.is_available():
        # print(f"æ£€æµ‹åˆ°åä¸º NPU è®¾å¤‡: {torch.npu.get_device_name(0)}")
        return torch.device(TARGET_DEVICE)
    elif torch.cuda.is_available():
        # print(f"æ£€æµ‹åˆ° CUDA è®¾å¤‡: {torch.cuda.get_device_name(0)}")
        return torch.device("cuda")
    else:
        # print("æœªæ£€æµ‹åˆ° GPU/NPU è®¾å¤‡ï¼Œä½¿ç”¨ CPU è¿›è¡Œæ¨ç†")
        return torch.device("cpu")


def _resolve_bert_device() -> torch.device:
    cfg = (settings.bert_device or "").strip().lower()
    if cfg in ("", "auto"):
        return device()
    if cfg == "cpu":
        return torch.device("cpu")
    if cfg.startswith("npu"):
        target = globals().get("TARGET_DEVICE", cfg)
        return torch.device(target)
    return torch.device(cfg)


async def load_models_if_needed():
    """
    æ ¹æ®é…ç½®å¼€å…³æ‡’åŠ è½½æ¨¡å‹ã€‚
    """
    global _model_asr, _model_emotion, _model_online, _model_whisper, _model_speaker, _punct_pipeline

    async with _model_lock:
        if settings.open_spk and _model_asr is None:
            pass
            _model_asr = AutoModel(
                model=settings.asr_model_dir,
                punc_model=settings.punc_model_dir,
                vad_model=settings.vad_model_dir,
                spk_model=settings.spk_model_dir,
                vad_kwargs={"max_single_segment_time": 30000, "max_end_silence_time": 800},
                sentence_timestamp=True,
                ngpu=settings.ngpu,
                device = TARGET_DEVICE,
                # batch_size = 16,
                disable_update = True,
                disable_pbar = True
            )

        if settings.open_emotion and settings.open_spk and _model_emotion is None:
            _model_emotion = AutoModel(
                model=settings.emotion_model_dir,
                device=TARGET_DEVICE,
                ngpu=settings.ngpu,
                quantize=True,
                disable_update=True,
                disable_pbar=True
            )

        if settings.open_online and _model_online is None:
            _model_online = AutoModel(
                model=settings.asr_online_model_dir,
                device=TARGET_DEVICE,
                ngpu=settings.ngpu,
                quantize=True,
                disable_update=True,
                disable_pbar=True
            )

        if settings.open_online and _punct_pipeline is None:
            print(f"ğŸš€ [Punctuation] æ­£åœ¨åŠ è½½æ ‡ç‚¹æ¨¡å‹ (Direct Class Mode)...")

            # ä¼˜å…ˆä½¿ç”¨ç›´æ¥ç±»å®ä¾‹åŒ– (æœ€ç¨³å¥)
            if FunASRPipeline is not None:
                _punct_pipeline = FunASRPipeline(
                    model=settings.asr_online_punc_model_dir,
                    device=f"cuda:{TARGET_DEVICE.split(':')[-1]}",  # AssertionError: device should be either cpu, cuda, gpu, gpu:X or cuda:X where X is the ordinal for gpu device.
                    disable_update=True,
                    disable_pbar=True
                )
            # å…œåº•
            else:
                _punct_pipeline = pipeline(
                    task=Tasks.punctuation,
                    model=settings.asr_online_punc_model_dir,
                    device=TARGET_DEVICE,
                    disable_update=True,
                    disable_pbar=True
                )

        whisper_model_path = os.path.join(settings.whisper_model_dir, "large-v3-turbo.pt")
        _model_whisper = whisper.load_model(
            name=whisper_model_path,
            device=TARGET_DEVICE,
        )


# ---------- äº”ä½•åˆ†ç±» ----------
def _ensure_bert_loaded():
    global _model_bert, _tokenizer, _bert_device
    if _model_bert is None or _tokenizer is None:
        _bert_device = _resolve_bert_device()
        _model_bert = BertForSequenceClassification.from_pretrained(
            pretrained_model_name_or_path=settings.bert_model_dir
        ).to(_bert_device).eval()
        _tokenizer = BertTokenizer.from_pretrained(
            pretrained_model_name_or_path=settings.bert_model_tokenizer
        )


def predict_fivewh(text: str) -> tuple[str, int, float]:
    """
    æ•™å¸ˆæé—®5ä½•ï¼ˆæ˜¯ä½•ã€ä¸ºä½•ã€è‹¥ä½•ã€ç”±ä½•ã€å¦‚ä½•ã€éæé—®ï¼‰ berté¢„æµ‹ï¼ˆä¸­æ–‡ï¼‰
    """
    _ensure_bert_loaded()
    inputs = _tokenizer(text, return_tensors="pt", truncation=True, padding=True, max_length=128).to(_bert_device)
    with torch.no_grad():
        logits = _model_bert(**inputs).logits
        probs = F.softmax(logits, dim=1)
        confidence, predicted = torch.max(probs, dim=1)
    return id2label[predicted.item()], predicted.item(), confidence.item()
