import sys
import asyncio
import importlib.util
import os

# =========================================================================
# ğŸš‘ ç´§æ€¥ç¯å¢ƒæ¸…æ´—å™¨ (Namespace Scrubber)
# å¿…é¡»æ”¾åœ¨ä»»ä½•å…¶ä»– import ä¹‹å‰æ‰§è¡Œï¼
# =========================================================================
print("ğŸ§¹ [System] Starting Pre-flight Namespace Check...")

# 1. å¼ºåˆ¶æ¸…ç† transformers.utils å¯¹ modelscope.utils çš„æ±¡æŸ“
# å¦‚æœå†…å­˜ä¸­ modelscope.utils å·²ç»è¢«æŒ‡å‘äº† transformers.utilsï¼Œå¼ºåˆ¶åˆ é™¤å®ƒ
conflicted_modules = ['modelscope.utils', 'modelscope.utils.device']
for mod_name in conflicted_modules:
    if mod_name in sys.modules:
        # è·å–å½“å‰æ¨¡å—å¯¹è±¡
        mod = sys.modules[mod_name]
        # æ£€æŸ¥å®ƒæ˜¯å¦å®é™…ä¸Šæ˜¯ transformers çš„ä¸€éƒ¨åˆ†
        if 'transformers' in str(mod):
            print(f"ğŸš¨ Detected collision: {mod_name} points to {mod} -> DELETING")
            del sys.modules[mod_name]

# 2. å°è¯•æ‰‹åŠ¨ä»æ–‡ä»¶è·¯å¾„åŠ è½½ modelscope.utils.device
# ç»•è¿‡ Python æ··ä¹±çš„ import ç¼“å­˜æœºåˆ¶
try:
    import modelscope

    # è·å– modelscope åŒ…çš„å®‰è£…è·¯å¾„
    ms_path = os.path.dirname(modelscope.__file__)
    device_py_path = os.path.join(ms_path, 'utils', 'device.py')

    if os.path.exists(device_py_path):
        spec = importlib.util.spec_from_file_location("modelscope.utils.device", device_py_path)
        if spec and spec.loader:
            module = importlib.util.module_from_spec(spec)
            sys.modules["modelscope.utils.device"] = module
            spec.loader.exec_module(module)
            print("âœ… Manually loaded modelscope.utils.device from file system.")
    else:
        print(f"âš ï¸ Warning: Could not find {device_py_path}")

except Exception as e:
    print(f"âš ï¸ Manual load failed: {e}. Falling back to standard import.")

# =========================================================================
# ğŸ æ­£å¸¸å¯¼å…¥æµç¨‹ (å¸¦è¡¥ä¸)
# =========================================================================

try:
    # å®šä¹‰ç©ºæ ¡éªŒå‡½æ•°
    def _bypass_verify_device(device):
        pass


    # å¯¼å…¥ modelscope ç»„ä»¶
    import modelscope.pipelines.base as modelbase
    import modelscope.utils.device as modeldevice  # ç°åœ¨è¿™è¡Œåº”è¯¥èƒ½æ­£å¸¸å·¥ä½œäº†
    from modelscope.pipelines import pipeline
    from modelscope.utils.constant import Tasks

    # âš¡ï¸ ç«‹å³åº”ç”¨ NPU é€‚é…è¡¥ä¸
    print("ğŸ”§ Applying NPU Patch to ModelScope...")
    modeldevice.verify_device = _bypass_verify_device
    modelbase.verify_device = _bypass_verify_device

    # åŒé‡ä¿é™©ï¼šç›´æ¥ä¿®æ”¹ sys.modules é‡Œçš„å¯¹è±¡
    if 'modelscope.utils.device' in sys.modules:
        sys.modules['modelscope.utils.device'].verify_device = _bypass_verify_device

except ImportError as e:
    print(f"âŒ Critical Import Error: {e}")
    # æ‰“å°è¯¦ç»†è°ƒè¯•ä¿¡æ¯
    if 'modelscope.utils' in sys.modules:
        print(f"DEBUG: modelscope.utils points to: {sys.modules['modelscope.utils']}")
    raise e

# --- NPU æ£€æµ‹ ---
import torch

try:
    import torch_npu
except ImportError:
    is_npu_available = False
else:
    is_npu_available = True

# --- å…¶ä»–åº“å¯¼å…¥ (Transformers å¿…é¡»æ”¾åœ¨è¿™é‡Œï¼Œä¸èƒ½ææ—©) ---
import torch.nn.functional as F
from transformers import BertTokenizer, BertForSequenceClassification
from faster_whisper import WhisperModel
from funasr import AutoModel
import pyannote.audio.core.task

from app.core.config import settings
from app.utils.feature_utils import id2label

# --- ä¸šåŠ¡é€»è¾‘ ---
_model_asr = None
_model_emotion = None
_model_online = None
_model_whisper = None
_model_speaker = None
_punct_pipeline = None
_model_bert = None
_tokenizer = None
_model_lock = asyncio.Lock()


def device() -> torch.device:
    if is_npu_available and torch.npu.is_available():
        print(f"æ£€æµ‹åˆ°åä¸º NPU è®¾å¤‡: {torch.npu.get_device_name(0)}")
        return torch.device(settings.device)
    elif torch.cuda.is_available():
        print(f"æ£€æµ‹åˆ° CUDA è®¾å¤‡: {torch.cuda.get_device_name(0)}")
        return torch.device("cuda")
    else:
        print("æœªæ£€æµ‹åˆ° GPU è®¾å¤‡ï¼Œä½¿ç”¨ CPU è¿›è¡Œæ¨ç†")
        return torch.device("cpu")

async def load_models_if_needed():
    """
    æ ¹æ®é…ç½®å¼€å…³æ‡’åŠ è½½æ¨¡å‹ã€‚
    """
    global _model_asr, _model_emotion, _model_online, _model_whisper, _model_speaker, _punct_pipeline

    # å†æ¬¡ç¡®è®¤è¡¥ä¸ç”Ÿæ•ˆï¼ˆé˜²æ­¢è¿è¡Œæ—¶è¢«è¦†ç›–ï¼‰
    try:
        modeldevice.verify_device = _bypass_verify_device
        modelbase.verify_device = _bypass_verify_device
    except:
        pass

    # PyTorch ç™½åå•
    try:
        safe_classes = [
            torch.torch_version.TorchVersion,
            pyannote.audio.core.task.Specifications,
            pyannote.audio.core.task.Problem,
            pyannote.audio.core.task.Resolution,
        ]
        torch.serialization.add_safe_globals(safe_classes)
        print("âœ… å·²æˆåŠŸæ·»åŠ æ¨¡å‹å®‰å…¨ç™½åå•")
    except:
        pass

    async with _model_lock:
        if settings.open_spk and _model_asr is None:
            _model_asr = AutoModel(
                model=settings.asr_model_dir,
                device=settings.device,
                ngpu=settings.ngpu,
                punc_model=settings.punc_model_dir,
                vad_model=settings.vad_model_dir,
                spk_model=settings.spk_model_dir,
                vad_kwargs={"max_single_segment_time": 30000, "max_end_silence_time": 800},
                sentence_timestamp=True,
                disable_update=True,
                disable_pbar=True
            )

        if settings.open_emotion and settings.open_spk and _model_emotion is None:
            _model_emotion = AutoModel(
                model=settings.emotion_model_dir,
                device=settings.device,
                ngpu=settings.ngpu,
                disable_update=True,
                disable_pbar=True
            )

        if settings.open_online and _model_online is None:
            _model_online = AutoModel(
                model=settings.asr_online_model_dir,
                device=settings.device,
                ngpu=settings.ngpu,
                disable_update=True,
                disable_pbar=True
            )

        if settings.open_mul_lang and _model_whisper is None:
            _model_whisper = WhisperModel(
                settings.whisper_model_dir,
                compute_type=settings.compute_type,
                device="cuda" if torch.cuda.is_available() else "cpu",
                device_index=int(settings.device.split(":")[-1]) if ":" in settings.device else 0
            )


        if settings.open_online and _punct_pipeline is None:
            pass
            # _punct_pipeline = pipeline(
            #     task=Tasks.punctuation,
            #     model=settings.asr_online_punc_model_dir,
            #     disable_update=True,
            #     device=settings.device
            # )



def get_asr_model():
    return _model_asr


def get_emotion_model():
    return _model_emotion


def get_online_model():
    return _model_online


def get_whisper_model():
    return _model_whisper


def get_speaker_model():
    return _model_speaker


def get_punct_pipeline():
    return _punct_pipeline


# ---------- äº”ä½•åˆ†ç±» ----------
def _ensure_bert_loaded():
    global _model_bert, _tokenizer
    if _model_bert is None or _tokenizer is None:
        _model_bert = BertForSequenceClassification.from_pretrained(
            pretrained_model_name_or_path=settings.bert_model_dir
        ).to(device()).eval()
        _tokenizer = BertTokenizer.from_pretrained(
            pretrained_model_name_or_path=settings.bert_model_tokenizer
        )


def predict_fivewh(text: str) -> tuple[str, int, float]:
    """
    æ•™å¸ˆæé—®5ä½•ï¼ˆæ˜¯ä½•ã€ä¸ºä½•ã€è‹¥ä½•ã€ç”±ä½•ã€å¦‚ä½•ã€éæé—®ï¼‰ berté¢„æµ‹ï¼ˆä¸­æ–‡ï¼‰
    """
    _ensure_bert_loaded()
    inputs = _tokenizer(text, return_tensors="pt", truncation=True, padding=True, max_length=128).to(device())
    with torch.no_grad():
        logits = _model_bert(**inputs).logits
        probs = F.softmax(logits, dim=1)
        confidence, predicted = torch.max(probs, dim=1)

    return id2label[predicted.item()], predicted.item(), confidence.item()
