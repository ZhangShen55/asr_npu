import sys
import asyncio
import torch

# =========================================================================
# 1. å…³é”®å¯¼å…¥åŒº (é¡ºåºä¸¥ç¦å˜åŠ¨)
# =========================================================================

# [è§„åˆ™] ModelScope å¿…é¡»æœ€å…ˆå¯¼å…¥ï¼Œé˜²æ­¢ Transformers æŠ¢å  "utils" å‘½åç©ºé—´
try:
    # import modelscope.pipelines.base as modelbase
    # import modelscope.utils.device as modeldevice
    from modelscope.pipelines import pipeline
    from modelscope.utils.constant import Tasks
    # [æ–°å¢] ç›´æ¥å¯¼å…¥ FunASR ç®¡é“ç±»ï¼Œè§£å†³ "KeyError" æ³¨å†Œè¡¨æ‰¾ä¸åˆ°çš„é—®é¢˜
    # from modelscope.pipelines.audio.funasr_pipeline import FunASRPipeline
except ImportError as e:
    print(f"âŒ ç¯å¢ƒä¾èµ–é”™è¯¯: {e}")
    raise e

# [è§„åˆ™] æ¥ç€å¯¼å…¥ NPU æ”¯æŒ
try:
    import torch_npu
except ImportError:
    is_npu_available = False
else:
    is_npu_available = True

# [è§„åˆ™] æœ€åå¯¼å…¥å…¶ä»–ä¸šåŠ¡åº“
import torch.nn.functional as F
from transformers import BertTokenizer, BertForSequenceClassification
from faster_whisper import WhisperModel
from funasr import AutoModel
import pyannote.audio.core.task

from app.core.config import settings
from app.utils.feature_utils import id2label

print("PyTorchç‰ˆæœ¬:", torch.__version__)
print("NPUè®¾å¤‡æ•°é‡:", torch_npu.npu.device_count())
print("å½“å‰NPUè®¾å¤‡:", torch_npu.npu.get_device_name(0))
print("Cuda available:", torch.cuda.is_available())

print("å¼€å§‹")


# =========================================================================
# 2. å…¨å±€å˜é‡ä¸è¡¥ä¸
# =========================================================================

# å•ä¾‹ç¼“å­˜
_model_asr = None
_model_emotion = None
_model_online = None
_model_whisper = None
_model_speaker = None
_punct_pipeline = None
_model_bert = None
_tokenizer = None

# çº¿ç¨‹é”
_model_lock = asyncio.Lock()

# # [NPU é€‚é…è¡¥ä¸] å®šä¹‰ç©ºæ ¡éªŒå‡½æ•°
# def _bypass_verify_device(device):
#     pass
#
# # [ç«‹å³åº”ç”¨è¡¥ä¸] é˜²æ­¢ ModelScope æŠ¥é”™ "device should be cpu/cuda/gpu"
# print("ğŸ”§ [NPUé€‚é…] å·²åº”ç”¨ ModelScope è®¾å¤‡æ ¡éªŒè¡¥ä¸")
# modeldevice.verify_device = _bypass_verify_device
# modelbase.verify_device = _bypass_verify_device


# =========================================================================
# 3. æ ¸å¿ƒåŠŸèƒ½å‡½æ•°
# =========================================================================

def device() -> torch.device:
    if is_npu_available and torch.npu.is_available():
        # åœ¨ Docker æ—¥å¿—ä¸­ç¡®è®¤ NPU æ˜¯å¦æŒ‚è½½æˆåŠŸ
        # print(f"æ£€æµ‹åˆ°åä¸º NPU è®¾å¤‡: {torch.npu.get_device_name(0)}")
        return torch.device(settings.device)
    elif torch.cuda.is_available():
        print(f"æ£€æµ‹åˆ° CUDA è®¾å¤‡: {torch.cuda.get_device_name(0)}")
        return torch.device("cuda")
    else:
        print("æœªæ£€æµ‹åˆ° GPU/NPU è®¾å¤‡ï¼Œä½¿ç”¨ CPU è¿›è¡Œæ¨ç†")
        return torch.device("cpu")


async def load_models_if_needed():
    """
    æ ¹æ®é…ç½®å¼€å…³æ‡’åŠ è½½æ¨¡å‹ã€‚
    """
    global _model_asr, _model_emotion, _model_online, _model_whisper, _model_speaker, _punct_pipeline

    # [NPU å®‰å…¨] æ·»åŠ  PyTorch åºåˆ—åŒ–ç™½åå•
    # try:
    #     safe_classes = [
    #         torch.torch_version.TorchVersion,
    #         pyannote.audio.core.task.Specifications,
    #         pyannote.audio.core.task.Problem,
    #         pyannote.audio.core.task.Resolution,
    #     ]
    #     torch.serialization.add_safe_globals(safe_classes)
    #     # print("âœ… [NPUå®‰å…¨] å·²æ·»åŠ æ¨¡å‹åŠ è½½ç™½åå•")
    # except:
    #     pass

    async with _model_lock:
        # 1. åŠ è½½ ASR ä¸»æ¨¡å‹
        if settings.open_spk and _model_asr is None:
            _model_asr = AutoModel(
                model=settings.asr_model_dir,
                device="npu:0",
                # ngpu=settings.ngpu,
                punc_model=settings.punc_model_dir,
                vad_model=settings.vad_model_dir,
                spk_model=settings.spk_model_dir,
                # vad_kwargs={"max_single_segment_time": 30000, "max_end_silence_time": 800},
                sentence_timestamp=True,
                disable_update=True,
                disable_pbar=False
            )

        # 2. åŠ è½½æƒ…æ„Ÿåˆ†ææ¨¡å‹
        if settings.open_emotion and settings.open_spk and _model_emotion is None:
            _model_emotion = AutoModel(
                model=settings.emotion_model_dir,
                device="npu:0",
                ngpu=settings.ngpu,
                disable_update=True,
                disable_pbar=True
            )

        # 3. åŠ è½½æµå¼æ¨¡å‹
        if settings.open_online and _model_online is None:
            _model_online = AutoModel(
                model=settings.asr_online_model_dir,
                device="npu:0",
                ngpu=settings.ngpu,
                disable_update=True,
                disable_pbar=True
            )

        # 4. åŠ è½½ Whisper (æ³¨æ„ï¼šCPU è¿è¡Œ)
        if settings.open_mul_lang and _model_whisper is None:
            _model_whisper = WhisperModel(
                settings.whisper_model_dir,
                compute_type=settings.compute_type,
                device="cpu",
                # device="cuda" if torch.cuda.is_available() else "cpu",
                device_index=int(settings.device.split(":")[-1]) if ":" in settings.device else 0
            )

        # 5. åŠ è½½æ ‡ç‚¹æ¨¡å‹ (ä½¿ç”¨æ˜¾å¼ç±»å®ä¾‹åŒ–ï¼Œè§£å†³ Registry æŠ¥é”™)
        if settings.open_online and _punct_pipeline is None:
            pass
            # print("ğŸš€ åŠ è½½æ ‡ç‚¹æ¨¡å‹ (Direct Pipeline Mode)...")
            # _punct_pipeline = pipeline(
            #     model=settings.asr_online_punc_model_dir,
            #     disable_update=True,
            #     device=settings.device
            # )


# =========================================================================
# 4. è¾…åŠ© Getter å’Œ ä¸šåŠ¡é€»è¾‘
# =========================================================================

def get_asr_model(): return _model_asr
def get_emotion_model(): return _model_emotion
def get_online_model(): return _model_online
def get_whisper_model(): return _model_whisper
def get_speaker_model(): return _model_speaker
def get_punct_pipeline(): return _punct_pipeline


# ---------- äº”ä½•åˆ†ç±» (BERT) ----------
def _ensure_bert_loaded():
    global _model_bert, _tokenizer
    if _model_bert is None or _tokenizer is None:
        _model_bert = BertForSequenceClassification.from_pretrained(
            pretrained_model_name_or_path=settings.bert_model_dir
        ).to(device()).eval()
        _tokenizer = BertTokenizer.from_pretrained(
            pretrained_model_name_or_path=settings.bert_model_tokenizer
        )

def predict_fivewh(text: str) -> tuple[str, int, float]:
    """
    æ•™å¸ˆæé—®5ä½•ï¼ˆæ˜¯ä½•ã€ä¸ºä½•ã€è‹¥ä½•ã€ç”±ä½•ã€å¦‚ä½•ã€éæé—®ï¼‰ berté¢„æµ‹ï¼ˆä¸­æ–‡ï¼‰
    """
    _ensure_bert_loaded()
    inputs = _tokenizer(text, return_tensors="pt", truncation=True, padding=True, max_length=128).to(device())
    with torch.no_grad():
        logits = _model_bert(**inputs).logits
        probs = F.softmax(logits, dim=1)
        confidence, predicted = torch.max(probs, dim=1)

    return id2label[predicted.item()], predicted.item(), confidence.item()