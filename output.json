{
  "version": "1",
  "pip_version": "25.3",
  "install": [
    {
      "download_info": {
        "url": "https://pypi.tuna.tsinghua.edu.cn/packages/4e/aa/4ee5bc310d4ecf286e2e3f3d01071b117f9296d8636dd564037323ead74e/funasr-1.2.9-py3-none-any.whl",
        "archive_info": {
          "hash": "sha256=a41189ac766f1a79682ebef75318f938612fe74a41d7defebdc68497c1362bb6",
          "hashes": {
            "sha256": "a41189ac766f1a79682ebef75318f938612fe74a41d7defebdc68497c1362bb6"
          }
        }
      },
      "is_direct": false,
      "is_yanked": false,
      "requested": true,
      "metadata": {
        "metadata_version": "2.1",
        "name": "funasr",
        "version": "1.2.9",
        "summary": "FunASR: A Fundamental End-to-End Speech Recognition Toolkit",
        "description_content_type": "text/markdown",
        "home_page": "https://github.com/alibaba-damo-academy/FunASR.git",
        "author": "Speech Lab of Alibaba Group",
        "author_email": "funasr@list.alibaba-inc.com",
        "license": "The MIT License",
        "license_file": [
          "LICENSE"
        ],
        "classifier": [
          "Programming Language :: Python",
          "Programming Language :: Python :: 3",
          "Programming Language :: Python :: 3.7",
          "Programming Language :: Python :: 3.8",
          "Programming Language :: Python :: 3.9",
          "Development Status :: 5 - Production/Stable",
          "Intended Audience :: Science/Research",
          "Operating System :: POSIX :: Linux",
          "License :: OSI Approved :: Apache Software License",
          "Topic :: Software Development :: Libraries :: Python Modules"
        ],
        "requires_dist": [
          "scipy>=1.4.1",
          "librosa",
          "jamo",
          "PyYAML>=5.1.2",
          "soundfile>=0.12.1",
          "kaldiio>=2.17.0",
          "torch-complex",
          "sentencepiece",
          "jieba",
          "pytorch-wpe",
          "editdistance>=0.5.2",
          "oss2",
          "tqdm",
          "umap-learn",
          "jaconv",
          "hydra-core>=1.3.2",
          "tensorboardX",
          "requests",
          "modelscope",
          "torch-optimizer; extra == \"all\"",
          "fairscale; extra == \"all\"",
          "transformers; extra == \"all\"",
          "openai-whisper; extra == \"all\"",
          "editdistance; extra == \"all\"",
          "transformers>=4.32.0; extra == \"all\"",
          "accelerate; extra == \"all\"",
          "tiktoken; extra == \"all\"",
          "einops; extra == \"all\"",
          "transformers-stream-generator>=0.0.4; extra == \"all\"",
          "scipy; extra == \"all\"",
          "torchvision; extra == \"all\"",
          "pillow; extra == \"all\"",
          "matplotlib; extra == \"all\"",
          "Jinja2; extra == \"doc\"",
          "Sphinx; extra == \"doc\"",
          "sphinx-rtd-theme>=0.2.4; extra == \"doc\"",
          "sphinx-argparse>=0.2.5; extra == \"doc\"",
          "commonmark; extra == \"doc\"",
          "recommonmark>=0.4.0; extra == \"doc\"",
          "nbsphinx>=0.4.2; extra == \"doc\"",
          "sphinx-markdown-tables>=0.0.12; extra == \"doc\"",
          "configargparse>=1.2.1; extra == \"doc\"",
          "transformers>=4.32.0; extra == \"llm\"",
          "accelerate; extra == \"llm\"",
          "tiktoken; extra == \"llm\"",
          "einops; extra == \"llm\"",
          "transformers-stream-generator>=0.0.4; extra == \"llm\"",
          "scipy; extra == \"llm\"",
          "torchvision; extra == \"llm\"",
          "pillow; extra == \"llm\"",
          "matplotlib; extra == \"llm\"",
          "pytest>=3.3.0; extra == \"test\"",
          "pytest-timeouts>=1.2.1; extra == \"test\"",
          "pytest-pythonpath>=0.7.3; extra == \"test\"",
          "pytest-cov>=2.7.1; extra == \"test\"",
          "hacking>=2.0.0; extra == \"test\"",
          "mock>=2.0.0; extra == \"test\"",
          "pycodestyle; extra == \"test\"",
          "jsondiff<2.0.0,>=1.2.0; extra == \"test\"",
          "flake8>=3.7.8; extra == \"test\"",
          "flake8-docstrings>=1.3.1; extra == \"test\"",
          "black; extra == \"test\"",
          "editdistance; extra == \"test\"",
          "editdistance; extra == \"train\""
        ],
        "requires_python": ">=3.7.0",
        "provides_extra": [
          "all",
          "doc",
          "llm",
          "test",
          "train"
        ],
        "description": "[//]: # '<div align=\"left\"><img src=\"docs/images/funasr_logo.jpg\" width=\"400\"/></div>'\n\n([ÁÆÄ‰Ωì‰∏≠Êñá](./README_zh.md)|English)\n\n[//]: # \"# FunASR: A Fundamental End-to-End Speech Recognition Toolkit\"\n\n[![SVG Banners](https://svg-banners.vercel.app/api?type=origin&text1=FunASRü§†&text2=üíñ%20A%20Fundamental%20End-to-End%20Speech%20Recognition%20Toolkit&width=800&height=210)](https://github.com/Akshay090/svg-banners)\n\n[![PyPI](https://img.shields.io/pypi/v/funasr)](https://pypi.org/project/funasr/)\n\n<p align=\"center\">\n<a href=\"https://trendshift.io/repositories/3839\" target=\"_blank\"><img src=\"https://trendshift.io/api/badge/repositories/3839\" alt=\"modelscope%2FFunASR | Trendshift\" style=\"width: 250px; height: 55px;\" width=\"250\" height=\"55\"/></a>\n</p>\n\n<strong>FunASR</strong> hopes to build a bridge between academic research and industrial applications on speech recognition. By supporting the training & finetuning of the industrial-grade speech recognition model, researchers and developers can conduct research and production of speech recognition models more conveniently, and promote the development of speech recognition ecology. ASR for FunÔºÅ\n\n[**Highlights**](#highlights)\n| [**News**](https://github.com/alibaba-damo-academy/FunASR#whats-new)\n| [**Installation**](#installation)\n| [**Quick Start**](#quick-start)\n| [**Tutorial**](https://github.com/alibaba-damo-academy/FunASR/blob/main/docs/tutorial/README.md)\n| [**Runtime**](./runtime/readme.md)\n| [**Model Zoo**](#model-zoo)\n| [**Contact**](#contact)\n\n<a name=\"highlights\"></a>\n\n## Highlights\n\n- FunASR is a fundamental speech recognition toolkit that offers a variety of features, including speech recognition (ASR), Voice Activity Detection (VAD), Punctuation Restoration, Language Models, Speaker Verification, Speaker Diarization and multi-talker ASR. FunASR provides convenient scripts and tutorials, supporting inference and fine-tuning of pre-trained models.\n- We have released a vast collection of academic and industrial pretrained models on the [ModelScope](https://www.modelscope.cn/models?page=1&tasks=auto-speech-recognition) and [huggingface](https://huggingface.co/FunASR), which can be accessed through our [Model Zoo](https://github.com/alibaba-damo-academy/FunASR/blob/main/docs/model_zoo/modelscope_models.md). The representative [Paraformer-large](https://www.modelscope.cn/models/damo/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch/summary), a non-autoregressive end-to-end speech recognition model, has the advantages of high accuracy, high efficiency, and convenient deployment, supporting the rapid construction of speech recognition services. For more details on service deployment, please refer to the [service deployment document](runtime/readme_cn.md).\n\n<a name=\"whats-new\"></a>\n\n## What's new:\n\n- 2025/12/15: [Fun-ASR-Nano-2512](https://modelscope.cn/models/FunAudioLLM/Fun-ASR-Nano-2512) is an end-to-end speech recognition large model trained on tens of millions of hours real speech data. It supports low-latency real-time transcription and covers 31 languages.\n- 2024/10/29: Real-time Transcription Service 1.12 released, The 2pass-offline mode supports the SensevoiceSmal modelÔºõ([docs](runtime/readme.md));\n- 2024/10/10ÔºöAdded support for the Whisper-large-v3-turbo model, a multitasking model that can perform multilingual speech recognition, speech translation, and language identification. It can be downloaded from the [modelscope](examples/industrial_data_pretraining/whisper/demo.py), and [openai](examples/industrial_data_pretraining/whisper/demo_from_openai.py).\n- 2024/09/26: Offline File Transcription Service 4.6, Offline File Transcription Service of English 1.7, Real-time Transcription Service 1.11 released, fix memory leak & Support the SensevoiceSmall onnx modelÔºõFile Transcription Service 2.0 GPU released, Fix GPU memory leak; ([docs](runtime/readme.md));\n- 2024/09/25Ôºökeyword spotting models are new supported. Supports fine-tuning and inference for four models: [fsmn_kws](https://modelscope.cn/models/iic/speech_sanm_kws_phone-xiaoyun-commands-online), [fsmn_kws_mt](https://modelscope.cn/models/iic/speech_sanm_kws_phone-xiaoyun-commands-online), [sanm_kws](https://modelscope.cn/models/iic/speech_sanm_kws_phone-xiaoyun-commands-offline), [sanm_kws_streaming](https://modelscope.cn/models/iic/speech_sanm_kws_phone-xiaoyun-commands-online).\n- 2024/07/04Ôºö[SenseVoice](https://github.com/FunAudioLLM/SenseVoice) is a speech foundation model with multiple speech understanding capabilities, including ASR, LID, SER, and AED.\n- 2024/07/01: Offline File Transcription Service GPU 1.1 released, optimize BladeDISC model compatibility issues; ref to ([docs](runtime/readme.md))\n- 2024/06/27: Offline File Transcription Service GPU 1.0 released, supporting dynamic batch processing and multi-threading concurrency. In the long audio test set, the single-thread RTF is 0.0076, and multi-threads' speedup is 1200+ (compared to 330+ on CPU); ref to ([docs](runtime/readme.md))\n- 2024/05/15Ôºöemotion recognition models are new supported. [emotion2vec+large](https://modelscope.cn/models/iic/emotion2vec_plus_large/summary)Ôºå[emotion2vec+base](https://modelscope.cn/models/iic/emotion2vec_plus_base/summary)Ôºå[emotion2vec+seed](https://modelscope.cn/models/iic/emotion2vec_plus_seed/summary). currently supports the following categories: 0: angry 1: happy 2: neutral 3: sad 4: unknown.\n- 2024/05/15: Offline File Transcription Service 4.5, Offline File Transcription Service of English 1.6, Real-time Transcription Service 1.10 released, adapting to FunASR 1.0 model structureÔºõ([docs](runtime/readme.md))\n\n<details><summary>Full Changelog</summary>\n\n- 2024/03/05ÔºöAdded the Qwen-Audio and Qwen-Audio-Chat large-scale audio-text multimodal models, which have topped multiple audio domain leaderboards. These models support speech dialogue, [usage](examples/industrial_data_pretraining/qwen_audio).\n- 2024/03/05ÔºöAdded support for the Whisper-large-v3 model, a multitasking model that can perform multilingual speech recognition, speech translation, and language identification. It can be downloaded from the[modelscope](examples/industrial_data_pretraining/whisper/demo.py), and [openai](examples/industrial_data_pretraining/whisper/demo_from_openai.py).\n- 2024/03/05: Offline File Transcription Service 4.4, Offline File Transcription Service of English 1.5ÔºåReal-time Transcription Service 1.9 releasedÔºådocker image supports ARM64 platform, update modelscopeÔºõ([docs](runtime/readme.md))\n- 2024/01/30Ôºöfunasr-1.0 has been released ([docs](https://github.com/alibaba-damo-academy/FunASR/discussions/1319))\n- 2024/01/30Ôºöemotion recognition models are new supported. [model link](https://www.modelscope.cn/models/iic/emotion2vec_base_finetuned/summary), modified from [repo](https://github.com/ddlBoJack/emotion2vec).\n- 2024/01/25: Offline File Transcription Service 4.2, Offline File Transcription Service of English 1.3 releasedÔºåoptimized the VAD (Voice Activity Detection) data processing method, significantly reducing peak memory usage, memory leak optimization; Real-time Transcription Service 1.7 releasedÔºåoptimizatized the client-sideÔºõ([docs](runtime/readme.md))\n- 2024/01/09: The Funasr SDK for Windows version 2.0 has been released, featuring support for The offline file transcription service (CPU) of Mandarin 4.1, The offline file transcription service (CPU) of English 1.2, The real-time transcription service (CPU) of Mandarin 1.6. For more details, please refer to the official documentation or release notes([FunASR-Runtime-Windows](https://www.modelscope.cn/models/damo/funasr-runtime-win-cpu-x64/summary))\n- 2024/01/03: File Transcription Service 4.0 released, Added support for 8k models, optimized timestamp mismatch issues and added sentence-level timestamps, improved the effectiveness of English word FST hotwords, supported automated configuration of thread parameters, and fixed known crash issues as well as memory leak problems, refer to ([docs](runtime/readme.md#file-transcription-service-mandarin-cpu)).\n- 2024/01/03: Real-time Transcription Service 1.6 releasedÔºåThe 2pass-offline mode supports Ngram language model decoding and WFST hotwords, while also addressing known crash issues and memory leak problems, ([docs](runtime/readme.md#the-real-time-transcription-service-mandarin-cpu))\n- 2024/01/03: Fixed known crash issues as well as memory leak problems, ([docs](runtime/readme.md#file-transcription-service-english-cpu)).\n- 2023/12/04: The Funasr SDK for Windows version 1.0 has been released, featuring support for The offline file transcription service (CPU) of Mandarin, The offline file transcription service (CPU) of English, The real-time transcription service (CPU) of Mandarin. For more details, please refer to the official documentation or release notes([FunASR-Runtime-Windows](https://www.modelscope.cn/models/damo/funasr-runtime-win-cpu-x64/summary))\n- 2023/11/08: The offline file transcription service 3.0 (CPU) of Mandarin has been released, adding punctuation large model, Ngram language model, and wfst hot words. For detailed information, please refer to [docs](runtime#file-transcription-service-mandarin-cpu).\n- 2023/10/17: The offline file transcription service (CPU) of English has been released. For more details, please refer to ([docs](runtime#file-transcription-service-english-cpu)).\n- 2023/10/13: [SlideSpeech](https://slidespeech.github.io/): A large scale multi-modal audio-visual corpus with a significant amount of real-time synchronized slides.\n- 2023/10/10: The ASR-SpeakersDiarization combined pipeline [Paraformer-VAD-SPK](https://github.com/alibaba-damo-academy/FunASR/blob/main/egs_modelscope/asr_vad_spk/speech_paraformer-large-vad-punc-spk_asr_nat-zh-cn/demo.py) is now released. Experience the model to get recognition results with speaker information.\n- 2023/10/07: [FunCodec](https://github.com/alibaba-damo-academy/FunCodec): A Fundamental, Reproducible and Integrable Open-source Toolkit for Neural Speech Codec.\n- 2023/09/01: The offline file transcription service 2.0 (CPU) of Mandarin has been released, with added support for ffmpeg, timestamp, and hotword models. For more details, please refer to ([docs](runtime#file-transcription-service-mandarin-cpu)).\n- 2023/08/07: The real-time transcription service (CPU) of Mandarin has been released. For more details, please refer to ([docs](runtime#the-real-time-transcription-service-mandarin-cpu)).\n- 2023/07/17: BAT is released, which is a low-latency and low-memory-consumption RNN-T model. For more details, please refer to ([BAT](egs/aishell/bat)).\n- 2023/06/26: ASRU2023 Multi-Channel Multi-Party Meeting Transcription Challenge 2.0 completed the competition and announced the results. For more details, please refer to ([M2MeT2.0](https://alibaba-damo-academy.github.io/FunASR/m2met2/index.html)).\n\n</details>\n\n<a name=\"Installation\"></a>\n\n## Installation\n\n- Requirements\n\n```text\npython>=3.8\ntorch>=1.13\ntorchaudio\n```\n\n- Install for pypi\n\n```shell\npip3 install -U funasr\n```\n\n- Or install from source code\n\n```sh\ngit clone https://github.com/alibaba/FunASR.git && cd FunASR\npip3 install -e ./\n```\n\n- Install modelscope or huggingface_hub for the pretrained models (Optional)\n\n```shell\npip3 install -U modelscope huggingface_hub\n```\n\n## Model Zoo\n\nFunASR has open-sourced a large number of pre-trained models on industrial data. You are free to use, copy, modify, and share FunASR models under the [Model License Agreement](./MODEL_LICENSE). Below are some representative models, for more models please refer to the [Model Zoo](./model_zoo).\n\n(Note: ‚≠ê represents the ModelScope model zoo, ü§ó represents the Huggingface model zoo, üçÄ represents the OpenAI model zoo)\n\n|                                                                                                         Model Name                                                                                                         |                                                                                                                        Task Details                                                                                                                         |          Training Data           | Parameters |\n|:--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------:|:-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------:|:--------------------------------:| :--------: |\n|                   Fun-ASR-Nano <br> ([‚≠ê](https://www.modelscope.cn/models/FunAudioLLM/Fun-ASR-Nano-2512) [ü§ó](https://huggingface.co/FunAudioLLM/Fun-ASR-Nano-2512) )                                                      |Speech recognition supports Chinese, English, and Japanese. Chinese includes support for 7 dialects and 26 regional accents. English and Japanese cover multiple regional accents. Additional features include lyric recognition and rap speech recognition. |    Tens of millions of hours     |  800M  |\n|                                         SenseVoiceSmall <br> ([‚≠ê](https://www.modelscope.cn/models/iic/SenseVoiceSmall) [ü§ó](https://huggingface.co/FunAudioLLM/SenseVoiceSmall) )                                         |                                                              multiple speech understanding capabilities, including ASR, ITN, LID, SER, and AED, support languages such as zh, yue, en, ja, ko                                                               |           300000 hours           |    234M    |\n|           paraformer-zh <br> ([‚≠ê](https://www.modelscope.cn/models/damo/speech_paraformer-large-vad-punc_asr_nat-zh-cn-16k-common-vocab8404-pytorch/summary) [ü§ó](https://huggingface.co/funasr/paraformer-zh) )           |                                                                                                     speech recognition, with timestamps, non-streaming                                                                                                      |      60000 hours, Mandarin       |    220M    |\n| <nobr>paraformer-zh-streaming <br> ( [‚≠ê](https://modelscope.cn/models/damo/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-online/summary) [ü§ó](https://huggingface.co/funasr/paraformer-zh-streaming) )</nobr> |                                                                                                                speech recognition, streaming                                                                                                                |      60000 hours, Mandarin       |    220M    |\n|               paraformer-en <br> ( [‚≠ê](https://www.modelscope.cn/models/damo/speech_paraformer-large-vad-punc_asr_nat-en-16k-common-vocab10020/summary) [ü§ó](https://huggingface.co/funasr/paraformer-en) )                |                                                                                                    speech recognition, without timestamps, non-streaming                                                                                                    |       50000 hours, English       |    220M    |\n|                            conformer-en <br> ( [‚≠ê](https://modelscope.cn/models/damo/speech_conformer_asr-en-16k-vocab4199-pytorch/summary) [ü§ó](https://huggingface.co/funasr/conformer-en) )                             |                                                                                                              speech recognition, non-streaming                                                                                                              |       50000 hours, English       |    220M    |\n|                               ct-punc <br> ( [‚≠ê](https://modelscope.cn/models/damo/punc_ct-transformer_cn-en-common-vocab471067-large/summary) [ü§ó](https://huggingface.co/funasr/ct-punc) )                               |                                                                                                                   punctuation restoration                                                                                                                   |    100M, Mandarin and English    |    290M    |\n|                                   fsmn-vad <br> ( [‚≠ê](https://modelscope.cn/models/damo/speech_fsmn_vad_zh-cn-16k-common-pytorch/summary) [ü§ó](https://huggingface.co/funasr/fsmn-vad) )                                   |                                                                                                                  voice activity detection                                                                                                                   | 5000 hours, Mandarin and English |    0.4M    |\n|                                                              fsmn-kws <br> ( [‚≠ê](https://modelscope.cn/models/iic/speech_charctc_kws_phone-xiaoyun/summary) )                                                              |                                                                                                                 keyword spottingÔºåstreaming                                                                                                                  |       5000 hours, Mandarin       |    0.7M    |\n|                                     fa-zh <br> ( [‚≠ê](https://modelscope.cn/models/damo/speech_timestamp_prediction-v1-16k-offline/summary) [ü§ó](https://huggingface.co/funasr/fa-zh) )                                     |                                                                                                                    timestamp prediction                                                                                                                     |       5000 hours, Mandarin       |    38M     |\n|                                       cam++ <br> ( [‚≠ê](https://modelscope.cn/models/iic/speech_campplus_sv_zh-cn_16k-common/summary) [ü§ó](https://huggingface.co/funasr/campplus) )                                        |                                                                                                              speaker verification/diarization                                                                                                               |            5000 hours            |    7.2M    |\n|                                            Whisper-large-v3 <br> ([‚≠ê](https://www.modelscope.cn/models/iic/Whisper-large-v3/summary) [üçÄ](https://github.com/openai/whisper) )                                             |                                                                                                     speech recognition, with timestamps, non-streaming                                                                                                      |           multilingual           |   1550 M   |\n|                                      Whisper-large-v3-turbo <br> ([‚≠ê](https://www.modelscope.cn/models/iic/Whisper-large-v3-turbo/summary) [üçÄ](https://github.com/openai/whisper) )                                       |                                                                                                     speech recognition, with timestamps, non-streaming                                                                                                      |           multilingual           |   809 M    |\n|                                                Qwen-Audio <br> ([‚≠ê](examples/industrial_data_pretraining/qwen_audio/demo.py) [ü§ó](https://huggingface.co/Qwen/Qwen-Audio) )                                                |                                                                                                         audio-text multimodal models (pretraining)                                                                                                          |           multilingual           |     8B     |\n|                                        Qwen-Audio-Chat <br> ([‚≠ê](examples/industrial_data_pretraining/qwen_audio/demo_chat.py) [ü§ó](https://huggingface.co/Qwen/Qwen-Audio-Chat) )                                         |                                                                                                             audio-text multimodal models (chat)                                                                                                             |           multilingual           |     8B     |\n|                               emotion2vec+large <br> ([‚≠ê](https://modelscope.cn/models/iic/emotion2vec_plus_large/summary) [ü§ó](https://huggingface.co/emotion2vec/emotion2vec_plus_large) )                               |                                                                                                                 speech emotion recongintion                                                                                                                 |           40000 hours            |    300M    |\n\n[//]: #\n[//]: # \"FunASR supports pre-trained or further fine-tuned models for deployment as a service. The CPU version of the Chinese offline file conversion service has been released, details can be found in [docs](funasr/runtime/docs/SDK_tutorial.md). More detailed information about service deployment can be found in the [deployment roadmap](funasr/runtime/readme_cn.md).\"\n\n<a name=\"quick-start\"></a>\n\n## Quick Start\n\nBelow is a quick start tutorial. Test audio files ([Mandarin](https://isv-data.oss-cn-hangzhou.aliyuncs.com/ics/MaaS/ASR/test_audio/vad_example.wav), [English](https://isv-data.oss-cn-hangzhou.aliyuncs.com/ics/MaaS/ASR/test_audio/asr_example_en.wav)).\n\n### Command-line usage\n\n```shell\nfunasr ++model=paraformer-zh ++vad_model=\"fsmn-vad\" ++punc_model=\"ct-punc\" ++input=asr_example_zh.wav\n```\n\nNotes: Support recognition of single audio file, as well as file list in Kaldi-style wav.scp format: `wav_id wav_pat`\n\n### Speech Recognition (Non-streaming)\n\n#### Fun-ASR-Nano\n\n```python\nfrom funasr import AutoModel\n\nmodel_dir = \"FunAudioLLM/Fun-ASR-Nano-2512\"\n\nmodel = AutoModel(\n    model=model_dir,\n    vad_model=\"fsmn-vad\",\n    vad_kwargs={\"max_single_segment_time\": 30000},\n    device=\"cuda:0\",\n)\nres = model.generate(input=[wav_path], cache={}, batch_size_s=0)\ntext = res[0][\"text\"]\nprint(text)\n```\n\nParameter Description:\n\n- `model_dir`: The name of the model, or the path to the model on the local disk.\n- `vad_model`: This indicates the activation of VAD (Voice Activity Detection). The purpose of VAD is to split long audio into shorter clips. In this case, the inference time includes both VAD and SenseVoice total consumption, and represents the end-to-end latency. If you wish to test the SenseVoice model's inference time separately, the VAD model can be disabled.\n- `vad_kwargs`: Specifies the configurations for the VAD model. `max_single_segment_time`: denotes the maximum duration for audio segmentation by the `vad_model`, with the unit being milliseconds (ms).\n- `use_itn`: Whether the output result includes punctuation and inverse text normalization.\n- `batch_size_s`: Indicates the use of dynamic batching, where the total duration of audio in the batch is measured in seconds (s).\n\n#### SenseVoice\n\n```python\nfrom funasr import AutoModel\nfrom funasr.utils.postprocess_utils import rich_transcription_postprocess\n\nmodel_dir = \"iic/SenseVoiceSmall\"\n\nmodel = AutoModel(\n    model=model_dir,\n    vad_model=\"fsmn-vad\",\n    vad_kwargs={\"max_single_segment_time\": 30000},\n    device=\"cuda:0\",\n)\n\n# en\nres = model.generate(\n    input=f\"{model.model_path}/example/en.mp3\",\n    cache={},\n    language=\"auto\",  # \"zn\", \"en\", \"yue\", \"ja\", \"ko\", \"nospeech\"\n    use_itn=True,\n    batch_size_s=60,\n    merge_vad=True,  #\n    merge_length_s=15,\n)\ntext = rich_transcription_postprocess(res[0][\"text\"])\nprint(text)\n```\n\nParameter Description:\n\n- `model_dir`: The name of the model, or the path to the model on the local disk.\n- `vad_model`: This indicates the activation of VAD (Voice Activity Detection). The purpose of VAD is to split long audio into shorter clips. In this case, the inference time includes both VAD and SenseVoice total consumption, and represents the end-to-end latency. If you wish to test the SenseVoice model's inference time separately, the VAD model can be disabled.\n- `vad_kwargs`: Specifies the configurations for the VAD model. `max_single_segment_time`: denotes the maximum duration for audio segmentation by the `vad_model`, with the unit being milliseconds (ms).\n- `use_itn`: Whether the output result includes punctuation and inverse text normalization.\n- `batch_size_s`: Indicates the use of dynamic batching, where the total duration of audio in the batch is measured in seconds (s).\n- `merge_vad`: Whether to merge short audio fragments segmented by the VAD model, with the merged length being `merge_length_s`, in seconds (s).\n- `ban_emo_unk`: Whether to ban the output of the `emo_unk` token.\n\n#### Paraformer\n\n```python\nfrom funasr import AutoModel\n# paraformer-zh is a multi-functional asr model\n# use vad, punc, spk or not as you need\nmodel = AutoModel(model=\"paraformer-zh\",  vad_model=\"fsmn-vad\",  punc_model=\"ct-punc\",\n                  # spk_model=\"cam++\",\n                  )\nres = model.generate(input=f\"{model.model_path}/example/asr_example.wav\",\n                     batch_size_s=300,\n                     hotword='È≠îÊê≠')\nprint(res)\n```\n\nNote: `hub`: represents the model repository, `ms` stands for selecting ModelScope download, `hf` stands for selecting Huggingface download.\n\n### Speech Recognition (Streaming)\n\n```python\nfrom funasr import AutoModel\n\nchunk_size = [0, 10, 5] #[0, 10, 5] 600ms, [0, 8, 4] 480ms\nencoder_chunk_look_back = 4 #number of chunks to lookback for encoder self-attention\ndecoder_chunk_look_back = 1 #number of encoder chunks to lookback for decoder cross-attention\n\nmodel = AutoModel(model=\"paraformer-zh-streaming\")\n\nimport soundfile\nimport os\n\nwav_file = os.path.join(model.model_path, \"example/asr_example.wav\")\nspeech, sample_rate = soundfile.read(wav_file)\nchunk_stride = chunk_size[1] * 960 # 600ms\n\ncache = {}\ntotal_chunk_num = int(len((speech)-1)/chunk_stride+1)\nfor i in range(total_chunk_num):\n    speech_chunk = speech[i*chunk_stride:(i+1)*chunk_stride]\n    is_final = i == total_chunk_num - 1\n    res = model.generate(input=speech_chunk, cache=cache, is_final=is_final, chunk_size=chunk_size, encoder_chunk_look_back=encoder_chunk_look_back, decoder_chunk_look_back=decoder_chunk_look_back)\n    print(res)\n```\n\nNote: `chunk_size` is the configuration for streaming latency.` [0,10,5]` indicates that the real-time display granularity is `10*60=600ms`, and the lookahead information is `5*60=300ms`. Each inference input is `600ms` (sample points are `16000*0.6=960`), and the output is the corresponding text. For the last speech segment input, `is_final=True` needs to be set to force the output of the last word.\n\n<details><summary>More Examples</summary>\n\n### Voice Activity Detection (Non-Streaming)\n\n```python\nfrom funasr import AutoModel\n\nmodel = AutoModel(model=\"fsmn-vad\")\nwav_file = f\"{model.model_path}/example/vad_example.wav\"\nres = model.generate(input=wav_file)\nprint(res)\n```\n\nNote: The output format of the VAD model is: `[[beg1, end1], [beg2, end2], ..., [begN, endN]]`, where `begN/endN` indicates the starting/ending point of the `N-th` valid audio segment, measured in milliseconds.\n\n### Voice Activity Detection (Streaming)\n\n```python\nfrom funasr import AutoModel\n\nchunk_size = 200 # ms\nmodel = AutoModel(model=\"fsmn-vad\")\n\nimport soundfile\n\nwav_file = f\"{model.model_path}/example/vad_example.wav\"\nspeech, sample_rate = soundfile.read(wav_file)\nchunk_stride = int(chunk_size * sample_rate / 1000)\n\ncache = {}\ntotal_chunk_num = int(len((speech)-1)/chunk_stride+1)\nfor i in range(total_chunk_num):\n    speech_chunk = speech[i*chunk_stride:(i+1)*chunk_stride]\n    is_final = i == total_chunk_num - 1\n    res = model.generate(input=speech_chunk, cache=cache, is_final=is_final, chunk_size=chunk_size)\n    if len(res[0][\"value\"]):\n        print(res)\n```\n\nNote: The output format for the streaming VAD model can be one of four scenarios:\n\n- `[[beg1, end1], [beg2, end2], .., [begN, endN]]`ÔºöThe same as the offline VAD output result mentioned above.\n- `[[beg, -1]]`ÔºöIndicates that only a starting point has been detected.\n- `[[-1, end]]`ÔºöIndicates that only an ending point has been detected.\n- `[]`ÔºöIndicates that neither a starting point nor an ending point has been detected.\n\nThe output is measured in milliseconds and represents the absolute time from the starting point.\n\n### Punctuation Restoration\n\n```python\nfrom funasr import AutoModel\n\nmodel = AutoModel(model=\"ct-punc\")\nres = model.generate(input=\"ÈÇ£‰ªäÂ§©ÁöÑ‰ºöÂ∞±Âà∞ËøôÈáåÂêß happy new year ÊòéÂπ¥ËßÅ\")\nprint(res)\n```\n\n### Timestamp Prediction\n\n```python\nfrom funasr import AutoModel\n\nmodel = AutoModel(model=\"fa-zh\")\nwav_file = f\"{model.model_path}/example/asr_example.wav\"\ntext_file = f\"{model.model_path}/example/text.txt\"\nres = model.generate(input=(wav_file, text_file), data_type=(\"sound\", \"text\"))\nprint(res)\n```\n\n### Speech Emotion Recognition\n\n```python\nfrom funasr import AutoModel\n\nmodel = AutoModel(model=\"emotion2vec_plus_large\")\n\nwav_file = f\"{model.model_path}/example/test.wav\"\n\nres = model.generate(wav_file, output_dir=\"./outputs\", granularity=\"utterance\", extract_embedding=False)\nprint(res)\n```\n\nMore usages ref to [docs](docs/tutorial/README_zh.md),\nmore examples ref to [demo](https://github.com/alibaba-damo-academy/FunASR/tree/main/examples/industrial_data_pretraining)\n\n</details>\n\n## Export ONNX\n\n### Command-line usage\n\n```shell\nfunasr-export ++model=paraformer ++quantize=false ++device=cpu\n```\n\n### Python\n\n```python\nfrom funasr import AutoModel\n\nmodel = AutoModel(model=\"paraformer\", device=\"cpu\")\n\nres = model.export(quantize=False)\n```\n\n### Test ONNX\n\n```python\n# pip3 install -U funasr-onnx\nfrom pathlib import Path\nfrom runtime.python.onnxruntime.funasr_onnx.paraformer_bin import Paraformer\n\n\nhome_dir = Path.home()\n\nmodel_dir = \"damo/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch\"\nmodel = Paraformer(model_dir, batch_size=1, quantize=True)\n\nwav_path = [f\"{home_dir}/.cache/modelscope/hub/models/damo/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch/example/asr_example.wav\"]\n\nresult = model(wav_path)\nprint(result)\n```\n\nMore examples ref to [demo](runtime/python/onnxruntime)\n\n## Deployment Service\n\nFunASR supports deploying pre-trained or further fine-tuned models for service. Currently, it supports the following types of service deployment:\n\n- File transcription service, Mandarin, CPU version, done\n- The real-time transcription service, Mandarin (CPU), done\n- File transcription service, English, CPU version, done\n- File transcription service, Mandarin, GPU version, in progress\n- and more.\n\nFor more detailed information, please refer to the [service deployment documentation](runtime/readme.md).\n\n<a name=\"contact\"></a>\n\n## Community Communication\n\nIf you encounter problems in use, you can directly raise Issues on the github page.\n\nYou can also scan the following DingTalk group to join the community group for communication and discussion.\n\n|                           DingTalk group                            |\n| :-----------------------------------------------------------------: |\n| <div align=\"left\"><img src=\"docs/images/dingding.png\" width=\"250\"/> |\n\n## Contributors\n\n| <div align=\"left\"><img src=\"docs/images/alibaba.png\" width=\"260\"/> | <div align=\"left\"><img src=\"docs/images/nwpu.png\" width=\"260\"/> | <img src=\"docs/images/China_Telecom.png\" width=\"200\"/> </div> | <img src=\"docs/images/RapidAI.png\" width=\"200\"/> </div> | <img src=\"docs/images/aihealthx.png\" width=\"200\"/> </div> | <img src=\"docs/images/XVERSE.png\" width=\"250\"/> </div> |\n| :----------------------------------------------------------------: | :-------------------------------------------------------------: | :-----------------------------------------------------------: | :-----------------------------------------------------: | :-------------------------------------------------------: | :----------------------------------------------------: |\n\nThe contributors can be found in [contributors list](./Acknowledge.md)\n\n## License\n\nThis project is licensed under [The MIT License](https://opensource.org/licenses/MIT). FunASR also contains various third-party components and some code modified from other repos under other open source licenses.\nThe use of pretraining model is subject to [model license](./MODEL_LICENSE)\n\n## Citations\n\n```bibtex\n@inproceedings{gao2023funasr,\n  author={Zhifu Gao and Zerui Li and Jiaming Wang and Haoneng Luo and Xian Shi and Mengzhe Chen and Yabin Li and Lingyun Zuo and Zhihao Du and Zhangyu Xiao and Shiliang Zhang},\n  title={FunASR: A Fundamental End-to-End Speech Recognition Toolkit},\n  year={2023},\n  booktitle={INTERSPEECH},\n}\n@inproceedings{An2023bat,\n  author={Keyu An and Xian Shi and Shiliang Zhang},\n  title={BAT: Boundary aware transducer for memory-efficient and low-latency ASR},\n  year={2023},\n  booktitle={INTERSPEECH},\n}\n@inproceedings{gao22b_interspeech,\n  author={Zhifu Gao and ShiLiang Zhang and Ian McLoughlin and Zhijie Yan},\n  title={Paraformer: Fast and Accurate Parallel Transformer for Non-autoregressive End-to-End Speech Recognition},\n  year=2022,\n  booktitle={Proc. Interspeech 2022},\n  pages={2063--2067},\n  doi={10.21437/Interspeech.2022-9996}\n}\n@inproceedings{shi2023seaco,\n  author={Xian Shi and Yexin Yang and Zerui Li and Yanni Chen and Zhifu Gao and Shiliang Zhang},\n  title={SeACo-Paraformer: A Non-Autoregressive ASR System with Flexible and Effective Hotword Customization Ability},\n  year={2023},\n  booktitle={ICASSP2024}\n}\n```\n"
      }
    },
    {
      "download_info": {
        "url": "https://pypi.tuna.tsinghua.edu.cn/packages/ed/0d/abdbc8e394a9461cf2ae27c16564fadaa65f52bd242dd1582ae5e7736dc3/editdistance-0.8.1-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl",
        "archive_info": {
          "hash": "sha256=0247e7a1e9c66ea75211a97e725366bff19a52aac2c838ed5f90025630e976dd",
          "hashes": {
            "sha256": "0247e7a1e9c66ea75211a97e725366bff19a52aac2c838ed5f90025630e976dd"
          }
        }
      },
      "is_direct": false,
      "is_yanked": false,
      "requested": false,
      "metadata": {
        "metadata_version": "2.1",
        "name": "editdistance",
        "version": "0.8.1",
        "summary": "Fast implementation of the edit distance (Levenshtein distance)",
        "description_content_type": "text/markdown",
        "author_email": "Hiroyuki Tanaka <aflc0x@gmail.com>",
        "license": "MIT",
        "requires_python": ">=3.8",
        "project_url": [
          "Homepage, https://github.com/roy-ht/editdistance",
          "Documentation, https://github.com/roy-ht/editdistance",
          "Repository, https://github.com/roy-ht/editdistance"
        ],
        "description": "# editdistance\n\nFast implementation of the edit distance (Levenshtein distance).\n\nThis library simply implements [Levenshtein distance](http://en.wikipedia.org/wiki/Levenshtein_distance) with C++ and Cython.\n\nThe algorithm used in this library is proposed by\n[Heikki Hyyr√∂, \"Explaining and extending the bit-parallel approximate string matching algorithm of Myers\", (2001)](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.19.7158&rep=rep1&type=pdf)\n\n## Binary wheels\n\nThanks to [pypa/cibuildwheel](https://github.com/pypa/cibuildwheel)\nThere are binary wheels on Linux, Mac OS, and Windows.\n\n## Install\n\nYou can install via pip:\n\n```bash\npip install editdistance\n```\n\n\n## Usage\n\nIt's quite simple:\n\n```python\nimport editdistance\neditdistance.eval('banana', 'bahama')\n# 2L\n```\n\n\n# Simple Benchmark\n\nWith IPython, I tried several libraries:\n\n* [pyxDamerauLevenshtein](https://pypi.python.org/pypi/pyxDamerauLevenshtein)\n* [pylev](https://pypi.python.org/pypi/pylev)\n* [python-Levenshtein](https://pypi.python.org/pypi/python-Levenshtein)\n\nOn Python 2.7.5:\n\n```python\na = 'fsffvfdsbbdfvvdavavavavavava'\nb = 'fvdaabavvvvvadvdvavavadfsfsdafvvav'\nimport pylev\ntimeit pylev.levenshtein(a, b)\n# 100 loops, best of 3: 7.48 ms per loop\n\nfrom pyxdameraulevenshtein import damerau_levenshtein_distance\ntimeit damerau_levenshtein_distance(a, b)\n# 100000 loops, best of 3: 11.4 ¬µs per loop\n\ntimeit editdistance.eval(a, b)  # my library\n# 100000 loops, best of 3: 3.5 ¬µs per loop\n\nimport Levenshtein\n\ntimeit Levenshtein.distance(a, b)\n# 100000 loops, best of 3: 3.21 ¬µs per loop\n```\n\n## Distance with Any Object\n\nAbove libraries only support strings.\nBut Sometimes other type of objects such as list of strings(words).\nI support any iterable, only requires hashable object of it:\n\n```python\nLevenshtein.distance(['spam', 'egg'], ['spam', 'ham'])\n# ---------------------------------------------------------------------------\n# TypeError                                 Traceback (most recent call last)\n# <ipython-input-22-3e0b30d145ac> in <module>()\n# ----> 1 Levenshtein.distance(['spam', 'egg'], ['spam', 'ham'])\n#\n# TypeError: distance expected two Strings or two Unicodes\n\neditdistance.eval(['spam', 'egg'], ['spam', 'ham'])\n# 1L\n```\n\nSo if object's hash is same, it's same.\nYou can provide `__hash__` method to your object instances.\n\nEnjoy!\n\n## License\n\nIt is released under the MIT license.\n\n```\nCopyright (c) 2013 Hiroyuki Tanaka\n\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n```\n"
      }
    },
    {
      "download_info": {
        "url": "https://pypi.tuna.tsinghua.edu.cn/packages/c6/50/e0edd38dcd63fb26a8547f13d28f7a008bc4a3fd4eb4ff030673f22ad41a/hydra_core-1.3.2-py3-none-any.whl",
        "archive_info": {
          "hash": "sha256=fa0238a9e31df3373b35b0bfb672c34cc92718d21f81311d8996a16de1141d8b",
          "hashes": {
            "sha256": "fa0238a9e31df3373b35b0bfb672c34cc92718d21f81311d8996a16de1141d8b"
          }
        }
      },
      "is_direct": false,
      "is_yanked": false,
      "requested": false,
      "metadata": {
        "metadata_version": "2.1",
        "name": "hydra-core",
        "version": "1.3.2",
        "summary": "A framework for elegantly configuring complex applications",
        "description_content_type": "text/markdown",
        "keywords": [
          "command-line",
          "configuration",
          "yaml",
          "tab-completion"
        ],
        "home_page": "https://github.com/facebookresearch/hydra",
        "author": "Omry Yadan",
        "author_email": "omry@fb.com",
        "license": "MIT",
        "license_file": [
          "LICENSE"
        ],
        "classifier": [
          "License :: OSI Approved :: MIT License",
          "Development Status :: 4 - Beta",
          "Programming Language :: Python :: 3.7",
          "Programming Language :: Python :: 3.8",
          "Programming Language :: Python :: 3.9",
          "Programming Language :: Python :: 3.10",
          "Programming Language :: Python :: 3.11",
          "Operating System :: POSIX :: Linux",
          "Operating System :: MacOS",
          "Operating System :: Microsoft :: Windows"
        ],
        "requires_dist": [
          "omegaconf (<2.4,>=2.2)",
          "antlr4-python3-runtime (==4.9.*)",
          "packaging",
          "importlib-resources ; python_version < \"3.9\""
        ],
        "description": "<p align=\"center\"><img src=\"https://raw.githubusercontent.com/facebookresearch/hydra/master/website/static/img/Hydra-Readme-logo2.svg\" alt=\"logo\" width=\"70%\" /></p>\n\n<p align=\"center\">\n  <a href=\"https://pypi.org/project/hydra-core/\">\n    <img src=\"https://img.shields.io/pypi/v/hydra-core\" alt=\"PyPI\" />\n  </a>\n  <a href=\"https://circleci.com/gh/facebookresearch/hydra\">\n    <img src=\"https://img.shields.io/circleci/build/github/facebookresearch/hydra?token=af199cd2deca9e70e53776f9ded96284b10687e9\" alt=\"CircleCI\" />\n  </a>\n  <a href=\"#\">\n    <img src=\"https://img.shields.io/pypi/l/hydra-core\" alt=\"PyPI - License\" />\n  </a>\n  <a href=\"#\">\n    <img src=\"https://img.shields.io/pypi/pyversions/hydra-core\" alt=\"PyPI - Python Version\" />\n  </a>\n  <a href=\"https://pepy.tech/project/hydra-core?versions=0.11.*&versions=1.0.*&versions=1.1.*\">\n    <img src=\"https://pepy.tech/badge/hydra-core/month\" alt=\"Downloads\" />\n  </a>\n  <a href=\"https://github.com/psf/black\">\n    <img src=\"https://img.shields.io/badge/code%20style-black-000000.svg\" alt=\"Code style: black\" />\n  </a>\n  <a href=\"https://lgtm.com/projects/g/facebookresearch/hydra/alerts/\">\n    <img src=\"https://img.shields.io/lgtm/alerts/g/facebookresearch/hydra.svg?logo=lgtm&logoWidth=18\" alt=\"Total alerts\" />\n  </a>\n  <a href=\"https://lgtm.com/projects/g/facebookresearch/hydra/context:python\">\n    <img src=\"https://img.shields.io/lgtm/grade/python/g/facebookresearch/hydra.svg?logo=lgtm&logoWidth=18\" alt=\"Language grade: Python\" />\n  </a>\n  <p align=\"center\">\n    <i>A framework for elegantly configuring complex applications.</i>\n  </p>\n  <p align=\"center\">\n    <i>Check the <a href=\"https://hydra.cc/\">website</a> for more information,<br>\n    or click the thumbnail below for a one-minute video introduction to Hydra.</i>\n  </p>\n  <p align=\"center\">\n   <a href=\"http://www.youtube.com/watch?feature=player_embedded&v=Slc3gRQpnBI\" target=\"_blank\">\n     <img src=\"http://img.youtube.com/vi/Slc3gRQpnBI/hqdefault.jpg\" alt=\"1 minute overview\" width=\"240\" height=\"180\" border=\"10\" />\n   </a>\n  </p>\n</p>\n\n----------------------\n\n\n### Releases\n\n#### Stable\n\n**Hydra 1.3** is the stable version of Hydra.\n- [Documentation](https://hydra.cc/docs/1.3/intro/)\n- Installation : `pip install hydra-core --upgrade`\n\nSee the [NEWS.md](NEWS.md) file for a summary of recent changes to Hydra.\n\n### License\nHydra is licensed under [MIT License](LICENSE).\n\n## Hydra Ecosystem\n\n#### Check out these third-party libraries that build on Hydra's functionality:\n* [hydra-zen](https://github.com/mit-ll-responsible-ai/hydra-zen): Pythonic utilities for working with Hydra. Dynamic config generation capabilities, enhanced config store features, a Python API for launching Hydra jobs, and more.\n* [lightning-hydra-template](https://github.com/ashleve/lightning-hydra-template): user-friendly template combining Hydra with [Pytorch-Lightning](https://github.com/Lightning-AI/lightning) for ML experimentation.\n* [hydra-torch](https://github.com/pytorch/hydra-torch): [configen](https://github.com/facebookresearch/hydra/tree/main/tools/configen)-generated configuration classes enabling type-safe PyTorch configuration for Hydra apps.\n* NVIDIA's DeepLearningExamples repository contains a Hydra Launcher plugin, the [distributed_launcher](https://github.com/NVIDIA/DeepLearningExamples/tree/9c34e35c218514b8607d7cf381d8a982a01175e9/Tools/PyTorch/TimeSeriesPredictionPlatform/distributed_launcher), which makes use of the pytorch [distributed.launch](https://pytorch.org/docs/stable/distributed.html#launch-utility) API.\n\n#### Ask questions in Github Discussions or StackOverflow (Use the tag #fb-hydra or #omegaconf):\n* [Github Discussions](https://github.com/facebookresearch/hydra/discussions)\n* [StackOverflow](https://stackexchange.com/filters/391828/hydra-questions)\n* [Twitter](https://twitter.com/Hydra_Framework)\n\nCheck out the Meta AI [blog post](https://ai.facebook.com/blog/reengineering-facebook-ais-deep-learning-platforms-for-interoperability/) to learn about how Hydra fits into Meta's efforts to reengineer deep learning platforms for interoperability.\n\n### Citing Hydra\nIf you use Hydra in your research please use the following BibTeX entry:\n```BibTeX\n@Misc{Yadan2019Hydra,\n  author =       {Omry Yadan},\n  title =        {Hydra - A framework for elegantly configuring complex applications},\n  howpublished = {Github},\n  year =         {2019},\n  url =          {https://github.com/facebookresearch/hydra}\n}\n```\n\n"
      }
    },
    {
      "download_info": {
        "url": "https://pypi.tuna.tsinghua.edu.cn/packages/3e/38/7859ff46355f76f8d19459005ca000b6e7012f2f1ca597746cbcd1fbfe5e/antlr4-python3-runtime-4.9.3.tar.gz",
        "archive_info": {
          "hash": "sha256=f224469b4168294902bb1efa80a8bf7855f24c99aef99cbefc1bcd3cce77881b",
          "hashes": {
            "sha256": "f224469b4168294902bb1efa80a8bf7855f24c99aef99cbefc1bcd3cce77881b"
          }
        }
      },
      "is_direct": false,
      "is_yanked": false,
      "requested": false,
      "metadata": {
        "metadata_version": "2.4",
        "name": "antlr4-python3-runtime",
        "version": "4.9.3",
        "dynamic": [
          "author",
          "author-email",
          "home-page",
          "license",
          "requires-dist",
          "summary"
        ],
        "summary": "ANTLR 4.9.3 runtime for Python 3.7",
        "home_page": "http://www.antlr.org",
        "author": "Eric Vergnaud, Terence Parr, Sam Harwell",
        "author_email": "eric.vergnaud@wanadoo.fr",
        "license": "BSD",
        "requires_dist": [
          "typing; python_version < \"3.5\""
        ]
      }
    },
    {
      "download_info": {
        "url": "https://pypi.tuna.tsinghua.edu.cn/packages/e3/94/1843518e420fa3ed6919835845df698c7e27e183cb997394e4a670973a65/omegaconf-2.3.0-py3-none-any.whl",
        "archive_info": {
          "hash": "sha256=7b4df175cdb08ba400f45cae3bdcae7ba8365db4d165fc65fd04b050ab63b46b",
          "hashes": {
            "sha256": "7b4df175cdb08ba400f45cae3bdcae7ba8365db4d165fc65fd04b050ab63b46b"
          }
        }
      },
      "is_direct": false,
      "is_yanked": false,
      "requested": false,
      "metadata": {
        "metadata_version": "2.1",
        "name": "omegaconf",
        "version": "2.3.0",
        "summary": "A flexible configuration library",
        "description_content_type": "text/markdown",
        "keywords": [
          "yaml",
          "configuration",
          "config"
        ],
        "home_page": "https://github.com/omry/omegaconf",
        "author": "Omry Yadan",
        "author_email": "omry@yadan.net",
        "license_file": [
          "LICENSE"
        ],
        "classifier": [
          "Programming Language :: Python :: 3.6",
          "Programming Language :: Python :: 3.7",
          "Programming Language :: Python :: 3.8",
          "Programming Language :: Python :: 3.9",
          "Programming Language :: Python :: 3.10",
          "Programming Language :: Python :: 3.11",
          "License :: OSI Approved :: BSD License",
          "Operating System :: OS Independent"
        ],
        "requires_dist": [
          "antlr4-python3-runtime (==4.9.*)",
          "PyYAML (>=5.1.0)",
          "dataclasses ; python_version == \"3.6\""
        ],
        "requires_python": ">=3.6",
        "description": "# OmegaConf\n|  | Description |\n| --- | --- |\n| Project | [![PyPI version](https://badge.fury.io/py/omegaconf.svg)](https://badge.fury.io/py/omegaconf)[![Downloads](https://pepy.tech/badge/omegaconf/month)](https://pepy.tech/project/omegaconf?versions=1.4.*&versions=2.0.*&versions=2.1.*)![PyPI - Python Version](https://img.shields.io/pypi/pyversions/omegaconf.svg) |\n| Code quality| [![CircleCI](https://img.shields.io/circleci/build/github/omry/omegaconf?logo=s&token=5de2f8dc2a0dd78438520575431aa533150806e3)](https://circleci.com/gh/omry/omegaconf)[![Coverage Status](https://coveralls.io/repos/github/omry/omegaconf/badge.svg)](https://coveralls.io/github/omry/omegaconf)[![Total alerts](https://img.shields.io/lgtm/alerts/g/omry/omegaconf.svg?logo=lgtm&logoWidth=18)](https://lgtm.com/projects/g/omry/omegaconf/alerts/)[![Language grade: Python](https://img.shields.io/lgtm/grade/python/g/omry/omegaconf.svg?logo=lgtm&logoWidth=18)](https://lgtm.com/projects/g/omry/omegaconf/context:python)|\n| Docs and support |[![Documentation Status](https://readthedocs.org/projects/omegaconf/badge/?version=2.0_branch)](https://omegaconf.readthedocs.io/en/2.1_branch/)[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/omry/omegaconf/master?filepath=docs%2Fnotebook%2FTutorial.ipynb)|\n\n\nOmegaConf is a hierarchical configuration system, with support for merging configurations from multiple sources (YAML config files, dataclasses/objects and CLI arguments)\nproviding a consistent API regardless of how the configuration was created.\n\n## Releases\n\n### Stable (2.2)\nOmegaConf 2.2 is the current stable version.\n* [What's new](https://github.com/omry/omegaconf/releases/tag/v2.2.1)\n* [Documentation](https://omegaconf.readthedocs.io/en/2.2_branch/)\n* [Source code](https://github.com/omry/omegaconf/tree/2.2_branch)\n\nInstall with `pip install --upgrade omegaconf`\n\n### Previous release (2.1)\nOmegaConf 2.1 is the current stable version.\n* [What's new](https://github.com/omry/omegaconf/releases/tag/v2.1.1)\n* [Documentation](https://omegaconf.readthedocs.io/en/2.1_branch/)\n* [Slides](https://docs.google.com/presentation/d/e/2PACX-1vT_UIV7hCnquIbLUm4NnkUpXvPEh33IKiUEvPRF850WKA8opOlZOszjKdZ3tPmf8u7hGNP6HpqS-NT5/pub?start=false&loop=false&delayms=3000)\n* [Source code](https://github.com/omry/omegaconf/tree/2.1_branch)\n\nInstall with `pip install omegaconf==2.1`\n\n### Previous release (2.0)\n\n* [What's new](https://github.com/omry/omegaconf/releases/tag/v2.0.0)\n* [Documentation](https://omegaconf.readthedocs.io/en/2.0_branch/)\n* [Slides](https://docs.google.com/presentation/d/e/2PACX-1vT_UIV7hCnquIbLUm4NnkUpXvPEh33IKiUEvPRF850WKA8opOlZOszjKdZ3tPmf8u7hGNP6HpqS-NT5/pub?start=false&loop=false&delayms=3000)\n* [Source code](https://github.com/omry/omegaconf/tree/2.0_branch)\n\nInstall with `pip install omegaconf==2.0.6`\n\n## Live tutorial\nRun the live tutorial: [![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/omry/omegaconf/master?filepath=docs%2Fnotebook%2FTutorial.ipynb)\n"
      }
    },
    {
      "download_info": {
        "url": "https://pypi.tuna.tsinghua.edu.cn/packages/ae/e3/6c3b42233225f398f7a72988b524f654ae818cca0d441db847a2761203e9/kaldiio-2.18.1-py3-none-any.whl",
        "archive_info": {
          "hash": "sha256=397a4cd18977acaae7acabfba6807ee0a6978c620064381a266eac15b3c1a0a0",
          "hashes": {
            "sha256": "397a4cd18977acaae7acabfba6807ee0a6978c620064381a266eac15b3c1a0a0"
          }
        }
      },
      "is_direct": false,
      "is_yanked": false,
      "requested": false,
      "metadata": {
        "metadata_version": "2.2",
        "name": "kaldiio",
        "version": "2.18.1",
        "dynamic": [
          "author",
          "classifier",
          "description",
          "description-content-type",
          "home-page",
          "requires-dist",
          "summary"
        ],
        "summary": "Kaldi-ark loading and writing module",
        "description_content_type": "text/markdown",
        "home_page": "https://github.com/nttcslab-sp/kaldiio",
        "author": "nttcslab-sp",
        "license_file": [
          "LICENSE"
        ],
        "classifier": [
          "Development Status :: 5 - Production/Stable",
          "Intended Audience :: Science/Research",
          "Programming Language :: Python :: 3.8",
          "Programming Language :: Python :: 3.9",
          "Programming Language :: Python :: 3.10",
          "Programming Language :: Python :: 3.11",
          "Programming Language :: Python :: 3.12",
          "Programming Language :: Python :: 3.13",
          "Topic :: Multimedia :: Sound/Audio :: Analysis"
        ],
        "requires_dist": [
          "numpy"
        ],
        "description": "# Kaldiio\n[![pypi](https://img.shields.io/pypi/v/kaldiio.svg)](https://pypi.python.org/pypi/kaldiio)\n[![Supported Python versions](https://img.shields.io/pypi/pyversions/kaldiio.svg)](https://pypi.python.org/pypi/kaldiio)\n[![codecov](https://codecov.io/gh/nttcslab-sp/kaldiio/branch/master/graph/badge.svg)](https://codecov.io/gh/nttcslab-sp/kaldiio)\n\nA pure python module for reading and writing kaldi ark files\n\n- [Introduction](#introduction)\n    - [What is this? What are `ark` and `scp`?](#what-is-this-what-are-ark-and-scp)\n    - [Features](#features)\n    - [Similar projects](#similar-projects)\n- [Install](#install)\n- [Usage](#usage)\n    - [ReadHelper](#readhelper)\n    - [WriteHelper](#writehelper)\n- [More low level API](#more-low-level-api)\n\n## Introduction\n### What are `ark` and `scp`?\n`kaldiio` is an IO utility  implemented in pure Python language for several file formats used in [kaldi](https://github.com/kaldi-asr/kaldi), which are named as`ark` and `scp`.  `ark` and `scp` are used in  in order to archive some objects defined in Kaldi, typically it is Matrix object of Kaldi.\n\nIn this section, we describe the basic concept of `ark` and `scp`. More detail about the File-IO in `Kaldi-asr`: http://kaldi-asr.org/doc/io.html\n\n\n#### Basic of File IO in kaldi: Ark and copy-feats\n`ark` is an archive format to save any `Kaldi objects`. This library mainly support `KaldiMatrix/KaldiVector`.\nThis ia an example of ark file of KaldiMatrix: [ark file](tests/arks/test.ark)\n\nIf you have `Kaldi`, you can convert it to text format as following\n\n```bash\n# copy-feats <read-specifier> <write-specifier>\ncopy-feats ark:test.ark ark,t:text.ark\n```\n\n\n`copy-feats` is designed to have high affinity with unix command line:\n\n1. `ark` can be flushed to and from unix pipe.\n\n        cat test.ark | copy-feats ark:- ark,t:- | less # Show the contents in the ark\n    `-` indicates standard input stream or output stream.\n1. Unix command can be used as `read-specifier` and `wspecifier`\n\n        copy-feats ark:'gunzip -c some.ark.gz |' ark:some.ark\n\n#### Scp file\n`scp` is a text file such as,\n\n```\nuttid1 /some/where/feats.ark:123\nuttid2 /some/where/feats.ark:156\nuttid3 /some/where/feats.ark:245\n```\nThe first column, `uttid1`, indicates the utterance id and the second, `/some/where/feats.ark:123`, is the file path of matrix/vector of kaldi formats.  The number after colon is a starting addressof the object of the file.\n\n`scp` looks very simple format, but has several powerful features.\n\n1. Mutual conversion between`ark` and `scp`\n\n        copy-feats scp:foo.scp ark:foo.ark  # scp -> ark\n        copy-feats ark:foo.ark ark,scp:bar.ark,bar.scp  # ark -> ark,scp\n\n1. Unix command can be used insead of direct file path\n\n    For example, the following scp file can be also used.\n\n        uttid1 cat /some/where/feats1.mat |\n        uttid2 cat /some/where/feats2.mat |\n        uttid3 cat /some/where/feats3.mat |\n\n#### wav.scp\n`wav.scp` is a `scp` to describe wave file paths.\n\n```\nuttid1 /some/path/a.wav\nuttid2 /some/path/b.wav\nuttid3 /some/path/c.wav\n```\n\n`wav.scp` is also can be embeded unix command as normal scp file. This is often used for converting file format in kaldi recipes.\n\n```\nuttid1 sph2pipe -f wav /some/path/a.wv1 |\nuttid2 sph2pipe -f wav /some/path/b.wv1 |\nuttid3 sph2pipe -f wav /some/path/c.wv1 |\n```\n\n### Features\nKaldiio supports:\n\n- Read/Write for archive formats: ark, scp\n  - Binary/Text - Float/Double Matrix: DM, FM\n  - Binary/Text - Float/Double Vector: DV, FV\n  - Compressed Matrix for loading: CM, CM2, CM3\n  - Compressed Matrix for writing: All compressoin_method are supported: 1,2,3,4,5,6,7\n  - Binary/Text for Int-vector, typically used for `ali` files.\n- Read/Write via a pipe: e.g. \"ark: cat feats.ark |\"\n- Read wav.scp / wav.ark\n- (New!) Some extended ark format **not supported** in Kaldi originally.\n  - The ark file for numpy, pickle, wav, flac files.\n\nThe followings are **not supported**\n\n- Write in existing scp file\n- NNet2/NNet3 egs\n- Lattice file\n\n### Similar projects\n- Python-C++ binding\n   - https://github.com/pykaldi/pykaldi\n      - Looks great. I recommend pykaldi if you aren't particular about pure python.\n   - https://github.com/janchorowski/kaldi-python/\n      - Maybe not enough maintained now.\n   - https://github.com/t13m/kaldi-readers-for-tensorflow\n      - Ark reader for tensorflow\n   - https://github.com/csukuangfj/kaldi_native_io\n      - Implemented in C++\n      - Have interface for Python\n      - Support all types of `rspecifier` and `wspecifier`\n      - Have a uniform interface for writing, sequential reading, and random access reading\n      - `pip install kaldi_native_io`\n- Pure Python\n   - https://github.com/vesis84/kaldi-io-for-python\n      - `kaldiio` is based on this module, but `kaldiio` supports more features than it.\n   - https://github.com/funcwj/kaldi-python-io\n      - Python>=3.6. `nnet3-egs`is also supported.\n\n## Install\n\n```bash\npip install kaldiio\n```\n\n## Usage\n`kaldiio` doesn't distinguish the API for each kaldi-objects, i.e.\n`Kaldi-Matrix`, `Kaldi-Vector`, not depending on whether it is binary or text, or compressed or not,\ncan be handled by the same API.\n\n### ReadHelper\n`ReadHelper` supports sequential accessing for `scp` or `ark`. If you need to access randomly, then use `kaldiio.load_scp`.\n\n\n- Read matrix-scp\n\n```python\nfrom kaldiio import ReadHelper\nwith ReadHelper('scp:file.scp') as reader:\n    for key, numpy_array in reader:\n        ...\n```\n\n\n- Read gziped ark\n\n```python\nfrom kaldiio import ReadHelper\nwith ReadHelper('ark: gunzip -c file.ark.gz |') as reader:\n    for key, numpy_array in reader:\n        ...\n\n# Ali file\nwith ReadHelper('ark: gunzip -c exp/tri3_ali/ali.*.gz |') as reader:\n    for key, numpy_array in reader:\n        ...\n```\n\n\n- Read wav.scp\n\n```python\nfrom kaldiio import ReadHelper\nwith ReadHelper('scp:wav.scp') as reader:\n    for key, (rate, numpy_array) in reader:\n        ...\n```\n\n„ÄÄ„ÄÄ„ÄÄ„ÄÄ- v2.11.0: Removed `wav` option. You can load `wav.scp` without any addtional argument.\n\n- Read wav.scp with segments\n\n```python\nfrom kaldiio import ReadHelper\nwith ReadHelper('scp:wav.scp', segments='segments') as reader\n    for key, (rate, numpy_array) in reader:\n        ...\n```\n\n- Read from stdin\n\n```python\nfrom kaldiio import ReadHelper\nwith ReadHelper('ark:-') as reader:\n    for key, numpy_array in reader:\n        ...\n```\n\n### WriteHelper\n- Write matrices and vectors in a ark with scp\n\n```python\nimport numpy\nfrom kaldiio import WriteHelper\nwith WriteHelper('ark,scp:file.ark,file.scp') as writer:\n    for i in range(10):\n        writer(str(i), numpy.random.randn(10, 10))\n        # The following is equivalent\n        # writer[str(i)] = numpy.random.randn(10, 10)\n```\n\n- Write in compressed matrix\n\n```python\nimport numpy\nfrom kaldiio import WriteHelper\nwith WriteHelper('ark:file.ark', compression_method=2) as writer:\n    for i in range(10):\n        writer(str(i), numpy.random.randn(10, 10))\n```\n\n- Write matrices in text\n\n```python\nimport numpy\nfrom kaldiio import WriteHelper\nwith WriteHelper('ark,t:file.ark') as writer:\n    for i in range(10):\n        writer(str(i), numpy.random.randn(10, 10))\n```\n\n- Write in gziped ark\n\n```python\nimport numpy\nfrom kaldiio import WriteHelper\nwith WriteHelper('ark:| gzip -c > file.ark.gz') as writer:\n    for i in range(10):\n        writer(str(i), numpy.random.randn(10, 10))\n```\n- Write matrice to stdout\n\n```python\nimport numpy\nfrom kaldiio import WriteHelper\nwith WriteHelper('ark:-') as writer:\n    for i in range(10):\n        writer(str(i), numpy.random.randn(10, 10))\n```\n\n\n- (New!) Extended ark format using numpy, pickle, soundfile\n\n```python\nimport numpy\nfrom kaldiio import WriteHelper\n\n# NPY ARK\nwith WriteHelper('ark:-', write_function=\"numpy\") as writer:\n    writer(\"foo\", numpy.random.randn(10, 10))\n\n# PICKLE ARK\nwith WriteHelper('ark:-', write_function=\"pickle\") as writer:\n    writer(\"foo\", numpy.random.randn(10, 10))\n    \n# FLAC ARK\nwith WriteHelper('ark:-', write_function=\"soundfile_flac\") as writer:\n    writer(\"foo\", numpy.random.randn(1000))\n```\n\nNote that `soundfile` is an optional module and you need to install it to use this feature.\n\n```sh\npip install soundfile\n```\n\n## More low level API\n`WriteHelper` and `ReadHelper` are high level wrapper of the following API to support kaldi style arguments.\n\n### load_ark\n\n```python\nimport kaldiio\n\nd = kaldiio.load_ark('a.ark')  # d is a generator object\nfor key, numpy_array in d:\n    ...\n\n# === load_ark can accepts file descriptor, too\nwith open('a.ark') as fd:\n    for key, numpy_array in kaldiio.load_ark(fd):\n        ...\n\n# === Use with open_like_kaldi\nfrom kaldiio import open_like_kaldi\nwith open_like_kaldi('gunzip -c file.ark.gz |', 'r') as f:\n    for key, numpy_array in kaldiio.load_ark(fd):\n        ...\n```\n\n- `load_ark` can load both matrices of ark and vectors of ark and also, it can be both text and binary.\n\n### load_scp\n`load_scp` creates \"lazy dict\", i.e.\nThe data are loaded in memory when accessing the element.\n\n```python\nimport kaldiio\n\nd = kaldiio.load_scp('a.scp')\nfor key in d:\n    numpy_array = d[key]\n\n\nwith open('a.scp') as fd:\n    kaldiio.load_scp(fd)\n\nd = kaldiio.load_scp('data/train/wav.scp', segments='data/train/segments')\nfor key in d:\n    rate, numpy_array = d[key]\n```\n\nThe object created by `load_scp` is a dict-like object, thus it has methods of `dict`.\n\n```python\nimport kaldiio\nd = kaldiio.load_scp('a.scp')\nd.keys()\nd.items()\nd.values()\n'uttid' in d\nd.get('uttid')\n```\n\n### load_scp_sequential (from v2.13.0)\n\n`load_scp_sequential` creates \"generator\" as same as `load_ark`.\nIf you don't need random-accessing for each elements\nand use it just to iterate for whole data,\nthen this method possibly performs faster than `load_scp`.\n\n```python\nimport kaldiio\nd = kaldiio.load_scp_sequential('a.scp')\nfor key, numpy_array in d:\n    ...\n```\n\n### load_wav_scp\n```python\nd = kaldiio.load_scp('wav.scp')\nfor key in d:\n    rate, numpy_array = d[key]\n\n# Supporting \"segments\"\nd = kaldiio.load_scp('data/train/wav.scp', segments='data/train/segments')\nfor key in d:\n    rate, numpy_array = d[key]\n```\n\n- v2.11.0: `load_wav_scp` is deprecated now. Use `load_scp`.\n\n### load_mat\n```python\nnumpy_array = kaldiio.load_mat('a.mat')\nnumpy_array = kaldiio.load_mat('a.ark:1134')  # Seek and load\n\n# If the file is wav, gets Tuple[int, numpy.ndarray]\nrate, numpy_array = kaldiio.load_mat('a.wav')\n```\n- `load_mat` can load kaldi-matrix, kaldi-vector, and wave\n\n### save_ark\n```python\n\n# === Create ark file from numpy\nkaldiio.save_ark('b.ark', {'key': numpy_array, 'key2': numpy_array2})\n# Create ark with scp _file, too\nkaldiio.save_ark('b.ark', {'key': numpy_array, 'key2': numpy_array2},\n                 scp='b.scp')\n\n# === Writes arrays to sys.stdout\nimport sys\nkaldiio.save_ark(sys.stdout, {'key': numpy_array})\n\n# === Writes arrays for each keys\n# generate a.ark\nkaldiio.save_ark('a.ark', {'key': numpy_array, 'key2': numpy_array2})\n# After here, a.ark is opened with 'a' (append) mode.\nkaldiio.save_ark('a.ark', {'key3': numpy_array3}, append=True)\n\n\n# === Use with open_like_kaldi\nfrom kaldiio import open_like_kaldi\nwith open_like_kaldi('| gzip a.ark.gz', 'w') as f:\n    kaldiio.save_ark(f, {'key': numpy_array})\n    kaldiio.save_ark(f, {'key2': numpy_array2})\n```\n### save_mat\n```python\n# array.ndim must be 1 or 2\nkaldiio.save_mat('a.mat', numpy_array)\n```\n- `save_mat` can save both kaldi-matrix and kaldi-vector\n\n\n### open_like_kaldi\n\n``kaldiio.open_like_kaldi`` is a useful tool if you are familiar with Kaldi. This function can performs as following,\n\n```python\nfrom kaldiio import open_like_kaldi\nwith open_like_kaldi('echo -n hello |', 'r') as f:\n    assert f.read() == 'hello'\nwith open_like_kaldi('| cat > out.txt', 'w') as f:\n    f.write('hello')\nwith open('out.txt', 'r') as f:\n    assert f.read() == 'hello'\n\nimport sys\nwith open_like_kaldi('-', 'r') as f:\n    assert f is sys.stdin\nwith open_like_kaldi('-', 'w') as f:\n    assert f is sys.stdout\n```\n\nFor example, if there are gziped alignment file, then you can load it as:\n\n```python\nfrom kaldiio import open_like_kaldi, load_ark\nwith open_like_kaldi('gunzip -c exp/tri3_ali/ali.*.gz |', 'rb') as f:\n    # Alignment format equals ark of IntVector\n    g = load_ark(f)\n    for k, numpy_array in g:\n        ...\n```\n\n### parse_specifier\n\n```python\nfrom kaldiio import parse_specifier, open_like_kaldi, load_ark\nrspecifier = 'ark:gunzip -c file.ark.gz |'\nspec_dict = parse_specifier(rspecifier)\n# spec_dict = {'ark': 'gunzip -c file.ark.gz |'}\n\nwith open_like_kaldi(spec_dict['ark'], 'rb') as fark:\n    for key, numpy_array in load_ark(fark):\n        ...\n```\n"
      }
    },
    {
      "download_info": {
        "url": "https://pypi.tuna.tsinghua.edu.cn/packages/43/f7/0e6a5ae5599c838c696adb4e6330a59f463265bfa1e116cfd1fbb0abaaae/pyyaml-6.0.3-cp310-cp310-manylinux2014_aarch64.manylinux_2_17_aarch64.manylinux_2_28_aarch64.whl",
        "archive_info": {
          "hash": "sha256=b30236e45cf30d2b8e7b3e85881719e98507abed1011bf463a8fa23e9c3e98a8",
          "hashes": {
            "sha256": "b30236e45cf30d2b8e7b3e85881719e98507abed1011bf463a8fa23e9c3e98a8"
          }
        }
      },
      "is_direct": false,
      "is_yanked": false,
      "requested": false,
      "metadata": {
        "metadata_version": "2.4",
        "name": "PyYAML",
        "version": "6.0.3",
        "dynamic": [
          "author",
          "author-email",
          "classifier",
          "description",
          "download-url",
          "home-page",
          "license",
          "license-file",
          "platform",
          "project-url",
          "requires-python",
          "summary"
        ],
        "platform": [
          "Any"
        ],
        "summary": "YAML parser and emitter for Python",
        "home_page": "https://pyyaml.org/",
        "download_url": "https://pypi.org/project/PyYAML/",
        "author": "Kirill Simonov",
        "author_email": "xi@resolvent.net",
        "license": "MIT",
        "license_file": [
          "LICENSE"
        ],
        "classifier": [
          "Development Status :: 5 - Production/Stable",
          "Intended Audience :: Developers",
          "License :: OSI Approved :: MIT License",
          "Operating System :: OS Independent",
          "Programming Language :: Cython",
          "Programming Language :: Python",
          "Programming Language :: Python :: 3",
          "Programming Language :: Python :: 3.8",
          "Programming Language :: Python :: 3.9",
          "Programming Language :: Python :: 3.10",
          "Programming Language :: Python :: 3.11",
          "Programming Language :: Python :: 3.12",
          "Programming Language :: Python :: 3.13",
          "Programming Language :: Python :: 3.14",
          "Programming Language :: Python :: Implementation :: CPython",
          "Programming Language :: Python :: Implementation :: PyPy",
          "Topic :: Software Development :: Libraries :: Python Modules",
          "Topic :: Text Processing :: Markup"
        ],
        "requires_python": ">=3.8",
        "project_url": [
          "Bug Tracker, https://github.com/yaml/pyyaml/issues",
          "CI, https://github.com/yaml/pyyaml/actions",
          "Documentation, https://pyyaml.org/wiki/PyYAMLDocumentation",
          "Mailing lists, http://lists.sourceforge.net/lists/listinfo/yaml-core",
          "Source Code, https://github.com/yaml/pyyaml"
        ],
        "description": "YAML is a data serialization format designed for human readability\nand interaction with scripting languages.  PyYAML is a YAML parser\nand emitter for Python.\n\nPyYAML features a complete YAML 1.1 parser, Unicode support, pickle\nsupport, capable extension API, and sensible error messages.  PyYAML\nsupports standard YAML tags and provides Python-specific tags that\nallow to represent an arbitrary Python object.\n\nPyYAML is applicable for a broad range of tasks from complex\nconfiguration files to object serialization and persistence.\n"
      }
    },
    {
      "download_info": {
        "url": "https://pypi.tuna.tsinghua.edu.cn/packages/db/0a/92b1de4a7adc7a15dcf5bddc6e191f6f29ee663b30511ce20467ef9b82e4/scipy-1.15.3-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl",
        "archive_info": {
          "hash": "sha256=263961f658ce2165bbd7b99fa5135195c3a12d9bef045345016b8b50c315cb82",
          "hashes": {
            "sha256": "263961f658ce2165bbd7b99fa5135195c3a12d9bef045345016b8b50c315cb82"
          }
        }
      },
      "is_direct": false,
      "is_yanked": false,
      "requested": false,
      "metadata": {
        "metadata_version": "2.1",
        "name": "scipy",
        "version": "1.15.3",
        "summary": "Fundamental algorithms for scientific computing in Python",
        "description_content_type": "text/x-rst",
        "maintainer_email": "SciPy Developers <scipy-dev@python.org>",
        "license": "Copyright (c) 2001-2002 Enthought, Inc. 2003-2024, SciPy Developers.\n         All rights reserved.\n         \n         Redistribution and use in source and binary forms, with or without\n         modification, are permitted provided that the following conditions\n         are met:\n         \n         1. Redistributions of source code must retain the above copyright\n            notice, this list of conditions and the following disclaimer.\n         \n         2. Redistributions in binary form must reproduce the above\n            copyright notice, this list of conditions and the following\n            disclaimer in the documentation and/or other materials provided\n            with the distribution.\n         \n         3. Neither the name of the copyright holder nor the names of its\n            contributors may be used to endorse or promote products derived\n            from this software without specific prior written permission.\n         \n         THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS\n         \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT\n         LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR\n         A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT\n         OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,\n         SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT\n         LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,\n         DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY\n         THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n         (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n         OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n         \n         ----\n         \n         This binary distribution of SciPy can also bundle the following software\n         (depending on the build):\n         \n         \n         Name: OpenBLAS\n         Files: scipy.libs/libscipy_openblas*.so\n         Description: bundled as a dynamically linked library\n         Availability: https://github.com/OpenMathLib/OpenBLAS/\n         License: BSD-3-Clause-Attribution\n           Copyright (c) 2011-2014, The OpenBLAS Project\n           All rights reserved.\n           \n           Redistribution and use in source and binary forms, with or without\n           modification, are permitted provided that the following conditions are\n           met:\n           \n              1. Redistributions of source code must retain the above copyright\n                 notice, this list of conditions and the following disclaimer.\n           \n              2. Redistributions in binary form must reproduce the above copyright\n                 notice, this list of conditions and the following disclaimer in\n                 the documentation and/or other materials provided with the\n                 distribution.\n              3. Neither the name of the OpenBLAS project nor the names of \n                 its contributors may be used to endorse or promote products \n                 derived from this software without specific prior written \n                 permission.\n           \n           THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\n           AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n           IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n           ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE\n           LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\n           DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\n           SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\n           CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\n           OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE\n           USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n         \n         \n         Name: LAPACK\n         Files: scipy.libs/libscipy_openblas*.so\n         Description: bundled in OpenBLAS\n         Availability: https://github.com/OpenMathLib/OpenBLAS/\n         License: BSD-3-Clause-Attribution\n           Copyright (c) 1992-2013 The University of Tennessee and The University\n                                   of Tennessee Research Foundation.  All rights\n                                   reserved.\n           Copyright (c) 2000-2013 The University of California Berkeley. All\n                                   rights reserved.\n           Copyright (c) 2006-2013 The University of Colorado Denver.  All rights\n                                   reserved.\n           \n           $COPYRIGHT$\n           \n           Additional copyrights may follow\n           \n           $HEADER$\n           \n           Redistribution and use in source and binary forms, with or without\n           modification, are permitted provided that the following conditions are\n           met:\n           \n           - Redistributions of source code must retain the above copyright\n             notice, this list of conditions and the following disclaimer.\n           \n           - Redistributions in binary form must reproduce the above copyright\n             notice, this list of conditions and the following disclaimer listed\n             in this license in the documentation and/or other materials\n             provided with the distribution.\n           \n           - Neither the name of the copyright holders nor the names of its\n             contributors may be used to endorse or promote products derived from\n             this software without specific prior written permission.\n           \n           The copyright holders provide no reassurances that the source code\n           provided does not infringe any patent, copyright, or any other\n           intellectual property rights of third parties.  The copyright holders\n           disclaim any liability to any recipient for claims brought against\n           recipient by any third party for infringement of that parties\n           intellectual property rights.\n           \n           THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS\n           \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT\n           LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR\n           A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT\n           OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,\n           SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT\n           LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,\n           DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY\n           THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n           (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n           OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n         \n         \n         Name: GCC runtime library\n         Files: scipy.libs/libgfortran*.so\n         Description: dynamically linked to files compiled with gcc\n         Availability: https://gcc.gnu.org/git/?p=gcc.git;a=tree;f=libgfortran\n         License: GPL-3.0-with-GCC-exception\n           Copyright (C) 2002-2017 Free Software Foundation, Inc.\n           \n           Libgfortran is free software; you can redistribute it and/or modify\n           it under the terms of the GNU General Public License as published by\n           the Free Software Foundation; either version 3, or (at your option)\n           any later version.\n           \n           Libgfortran is distributed in the hope that it will be useful,\n           but WITHOUT ANY WARRANTY; without even the implied warranty of\n           MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n           GNU General Public License for more details.\n           \n           Under Section 7 of GPL version 3, you are granted additional\n           permissions described in the GCC Runtime Library Exception, version\n           3.1, as published by the Free Software Foundation.\n           \n           You should have received a copy of the GNU General Public License and\n           a copy of the GCC Runtime Library Exception along with this program;\n           see the files COPYING3 and COPYING.RUNTIME respectively.  If not, see\n           <http://www.gnu.org/licenses/>.\n         \n         ----\n         \n         Full text of license texts referred to above follows (that they are\n         listed below does not necessarily imply the conditions apply to the\n         present binary release):\n         \n         ----\n         \n         GCC RUNTIME LIBRARY EXCEPTION\n         \n         Version 3.1, 31 March 2009\n         \n         Copyright (C) 2009 Free Software Foundation, Inc. <http://fsf.org/>\n         \n         Everyone is permitted to copy and distribute verbatim copies of this\n         license document, but changing it is not allowed.\n         \n         This GCC Runtime Library Exception (\"Exception\") is an additional\n         permission under section 7 of the GNU General Public License, version\n         3 (\"GPLv3\"). It applies to a given file (the \"Runtime Library\") that\n         bears a notice placed by the copyright holder of the file stating that\n         the file is governed by GPLv3 along with this Exception.\n         \n         When you use GCC to compile a program, GCC may combine portions of\n         certain GCC header files and runtime libraries with the compiled\n         program. The purpose of this Exception is to allow compilation of\n         non-GPL (including proprietary) programs to use, in this way, the\n         header files and runtime libraries covered by this Exception.\n         \n         0. Definitions.\n         \n         A file is an \"Independent Module\" if it either requires the Runtime\n         Library for execution after a Compilation Process, or makes use of an\n         interface provided by the Runtime Library, but is not otherwise based\n         on the Runtime Library.\n         \n         \"GCC\" means a version of the GNU Compiler Collection, with or without\n         modifications, governed by version 3 (or a specified later version) of\n         the GNU General Public License (GPL) with the option of using any\n         subsequent versions published by the FSF.\n         \n         \"GPL-compatible Software\" is software whose conditions of propagation,\n         modification and use would permit combination with GCC in accord with\n         the license of GCC.\n         \n         \"Target Code\" refers to output from any compiler for a real or virtual\n         target processor architecture, in executable form or suitable for\n         input to an assembler, loader, linker and/or execution\n         phase. Notwithstanding that, Target Code does not include data in any\n         format that is used as a compiler intermediate representation, or used\n         for producing a compiler intermediate representation.\n         \n         The \"Compilation Process\" transforms code entirely represented in\n         non-intermediate languages designed for human-written code, and/or in\n         Java Virtual Machine byte code, into Target Code. Thus, for example,\n         use of source code generators and preprocessors need not be considered\n         part of the Compilation Process, since the Compilation Process can be\n         understood as starting with the output of the generators or\n         preprocessors.\n         \n         A Compilation Process is \"Eligible\" if it is done using GCC, alone or\n         with other GPL-compatible software, or if it is done without using any\n         work based on GCC. For example, using non-GPL-compatible Software to\n         optimize any GCC intermediate representations would not qualify as an\n         Eligible Compilation Process.\n         \n         1. Grant of Additional Permission.\n         \n         You have permission to propagate a work of Target Code formed by\n         combining the Runtime Library with Independent Modules, even if such\n         propagation would otherwise violate the terms of GPLv3, provided that\n         all Target Code was generated by Eligible Compilation Processes. You\n         may then convey such a combination under terms of your choice,\n         consistent with the licensing of the Independent Modules.\n         \n         2. No Weakening of GCC Copyleft.\n         \n         The availability of this Exception does not imply any general\n         presumption that third-party software is unaffected by the copyleft\n         requirements of the license of GCC.\n         \n         ----\n         \n                             GNU GENERAL PUBLIC LICENSE\n                                Version 3, 29 June 2007\n         \n          Copyright (C) 2007 Free Software Foundation, Inc. <http://fsf.org/>\n          Everyone is permitted to copy and distribute verbatim copies\n          of this license document, but changing it is not allowed.\n         \n                                     Preamble\n         \n           The GNU General Public License is a free, copyleft license for\n         software and other kinds of works.\n         \n           The licenses for most software and other practical works are designed\n         to take away your freedom to share and change the works.  By contrast,\n         the GNU General Public License is intended to guarantee your freedom to\n         share and change all versions of a program--to make sure it remains free\n         software for all its users.  We, the Free Software Foundation, use the\n         GNU General Public License for most of our software; it applies also to\n         any other work released this way by its authors.  You can apply it to\n         your programs, too.\n         \n           When we speak of free software, we are referring to freedom, not\n         price.  Our General Public Licenses are designed to make sure that you\n         have the freedom to distribute copies of free software (and charge for\n         them if you wish), that you receive source code or can get it if you\n         want it, that you can change the software or use pieces of it in new\n         free programs, and that you know you can do these things.\n         \n           To protect your rights, we need to prevent others from denying you\n         these rights or asking you to surrender the rights.  Therefore, you have\n         certain responsibilities if you distribute copies of the software, or if\n         you modify it: responsibilities to respect the freedom of others.\n         \n           For example, if you distribute copies of such a program, whether\n         gratis or for a fee, you must pass on to the recipients the same\n         freedoms that you received.  You must make sure that they, too, receive\n         or can get the source code.  And you must show them these terms so they\n         know their rights.\n         \n           Developers that use the GNU GPL protect your rights with two steps:\n         (1) assert copyright on the software, and (2) offer you this License\n         giving you legal permission to copy, distribute and/or modify it.\n         \n           For the developers' and authors' protection, the GPL clearly explains\n         that there is no warranty for this free software.  For both users' and\n         authors' sake, the GPL requires that modified versions be marked as\n         changed, so that their problems will not be attributed erroneously to\n         authors of previous versions.\n         \n           Some devices are designed to deny users access to install or run\n         modified versions of the software inside them, although the manufacturer\n         can do so.  This is fundamentally incompatible with the aim of\n         protecting users' freedom to change the software.  The systematic\n         pattern of such abuse occurs in the area of products for individuals to\n         use, which is precisely where it is most unacceptable.  Therefore, we\n         have designed this version of the GPL to prohibit the practice for those\n         products.  If such problems arise substantially in other domains, we\n         stand ready to extend this provision to those domains in future versions\n         of the GPL, as needed to protect the freedom of users.\n         \n           Finally, every program is threatened constantly by software patents.\n         States should not allow patents to restrict development and use of\n         software on general-purpose computers, but in those that do, we wish to\n         avoid the special danger that patents applied to a free program could\n         make it effectively proprietary.  To prevent this, the GPL assures that\n         patents cannot be used to render the program non-free.\n         \n           The precise terms and conditions for copying, distribution and\n         modification follow.\n         \n                                TERMS AND CONDITIONS\n         \n           0. Definitions.\n         \n           \"This License\" refers to version 3 of the GNU General Public License.\n         \n           \"Copyright\" also means copyright-like laws that apply to other kinds of\n         works, such as semiconductor masks.\n         \n           \"The Program\" refers to any copyrightable work licensed under this\n         License.  Each licensee is addressed as \"you\".  \"Licensees\" and\n         \"recipients\" may be individuals or organizations.\n         \n           To \"modify\" a work means to copy from or adapt all or part of the work\n         in a fashion requiring copyright permission, other than the making of an\n         exact copy.  The resulting work is called a \"modified version\" of the\n         earlier work or a work \"based on\" the earlier work.\n         \n           A \"covered work\" means either the unmodified Program or a work based\n         on the Program.\n         \n           To \"propagate\" a work means to do anything with it that, without\n         permission, would make you directly or secondarily liable for\n         infringement under applicable copyright law, except executing it on a\n         computer or modifying a private copy.  Propagation includes copying,\n         distribution (with or without modification), making available to the\n         public, and in some countries other activities as well.\n         \n           To \"convey\" a work means any kind of propagation that enables other\n         parties to make or receive copies.  Mere interaction with a user through\n         a computer network, with no transfer of a copy, is not conveying.\n         \n           An interactive user interface displays \"Appropriate Legal Notices\"\n         to the extent that it includes a convenient and prominently visible\n         feature that (1) displays an appropriate copyright notice, and (2)\n         tells the user that there is no warranty for the work (except to the\n         extent that warranties are provided), that licensees may convey the\n         work under this License, and how to view a copy of this License.  If\n         the interface presents a list of user commands or options, such as a\n         menu, a prominent item in the list meets this criterion.\n         \n           1. Source Code.\n         \n           The \"source code\" for a work means the preferred form of the work\n         for making modifications to it.  \"Object code\" means any non-source\n         form of a work.\n         \n           A \"Standard Interface\" means an interface that either is an official\n         standard defined by a recognized standards body, or, in the case of\n         interfaces specified for a particular programming language, one that\n         is widely used among developers working in that language.\n         \n           The \"System Libraries\" of an executable work include anything, other\n         than the work as a whole, that (a) is included in the normal form of\n         packaging a Major Component, but which is not part of that Major\n         Component, and (b) serves only to enable use of the work with that\n         Major Component, or to implement a Standard Interface for which an\n         implementation is available to the public in source code form.  A\n         \"Major Component\", in this context, means a major essential component\n         (kernel, window system, and so on) of the specific operating system\n         (if any) on which the executable work runs, or a compiler used to\n         produce the work, or an object code interpreter used to run it.\n         \n           The \"Corresponding Source\" for a work in object code form means all\n         the source code needed to generate, install, and (for an executable\n         work) run the object code and to modify the work, including scripts to\n         control those activities.  However, it does not include the work's\n         System Libraries, or general-purpose tools or generally available free\n         programs which are used unmodified in performing those activities but\n         which are not part of the work.  For example, Corresponding Source\n         includes interface definition files associated with source files for\n         the work, and the source code for shared libraries and dynamically\n         linked subprograms that the work is specifically designed to require,\n         such as by intimate data communication or control flow between those\n         subprograms and other parts of the work.\n         \n           The Corresponding Source need not include anything that users\n         can regenerate automatically from other parts of the Corresponding\n         Source.\n         \n           The Corresponding Source for a work in source code form is that\n         same work.\n         \n           2. Basic Permissions.\n         \n           All rights granted under this License are granted for the term of\n         copyright on the Program, and are irrevocable provided the stated\n         conditions are met.  This License explicitly affirms your unlimited\n         permission to run the unmodified Program.  The output from running a\n         covered work is covered by this License only if the output, given its\n         content, constitutes a covered work.  This License acknowledges your\n         rights of fair use or other equivalent, as provided by copyright law.\n         \n           You may make, run and propagate covered works that you do not\n         convey, without conditions so long as your license otherwise remains\n         in force.  You may convey covered works to others for the sole purpose\n         of having them make modifications exclusively for you, or provide you\n         with facilities for running those works, provided that you comply with\n         the terms of this License in conveying all material for which you do\n         not control copyright.  Those thus making or running the covered works\n         for you must do so exclusively on your behalf, under your direction\n         and control, on terms that prohibit them from making any copies of\n         your copyrighted material outside their relationship with you.\n         \n           Conveying under any other circumstances is permitted solely under\n         the conditions stated below.  Sublicensing is not allowed; section 10\n         makes it unnecessary.\n         \n           3. Protecting Users' Legal Rights From Anti-Circumvention Law.\n         \n           No covered work shall be deemed part of an effective technological\n         measure under any applicable law fulfilling obligations under article\n         11 of the WIPO copyright treaty adopted on 20 December 1996, or\n         similar laws prohibiting or restricting circumvention of such\n         measures.\n         \n           When you convey a covered work, you waive any legal power to forbid\n         circumvention of technological measures to the extent such circumvention\n         is effected by exercising rights under this License with respect to\n         the covered work, and you disclaim any intention to limit operation or\n         modification of the work as a means of enforcing, against the work's\n         users, your or third parties' legal rights to forbid circumvention of\n         technological measures.\n         \n           4. Conveying Verbatim Copies.\n         \n           You may convey verbatim copies of the Program's source code as you\n         receive it, in any medium, provided that you conspicuously and\n         appropriately publish on each copy an appropriate copyright notice;\n         keep intact all notices stating that this License and any\n         non-permissive terms added in accord with section 7 apply to the code;\n         keep intact all notices of the absence of any warranty; and give all\n         recipients a copy of this License along with the Program.\n         \n           You may charge any price or no price for each copy that you convey,\n         and you may offer support or warranty protection for a fee.\n         \n           5. Conveying Modified Source Versions.\n         \n           You may convey a work based on the Program, or the modifications to\n         produce it from the Program, in the form of source code under the\n         terms of section 4, provided that you also meet all of these conditions:\n         \n             a) The work must carry prominent notices stating that you modified\n             it, and giving a relevant date.\n         \n             b) The work must carry prominent notices stating that it is\n             released under this License and any conditions added under section\n             7.  This requirement modifies the requirement in section 4 to\n             \"keep intact all notices\".\n         \n             c) You must license the entire work, as a whole, under this\n             License to anyone who comes into possession of a copy.  This\n             License will therefore apply, along with any applicable section 7\n             additional terms, to the whole of the work, and all its parts,\n             regardless of how they are packaged.  This License gives no\n             permission to license the work in any other way, but it does not\n             invalidate such permission if you have separately received it.\n         \n             d) If the work has interactive user interfaces, each must display\n             Appropriate Legal Notices; however, if the Program has interactive\n             interfaces that do not display Appropriate Legal Notices, your\n             work need not make them do so.\n         \n           A compilation of a covered work with other separate and independent\n         works, which are not by their nature extensions of the covered work,\n         and which are not combined with it such as to form a larger program,\n         in or on a volume of a storage or distribution medium, is called an\n         \"aggregate\" if the compilation and its resulting copyright are not\n         used to limit the access or legal rights of the compilation's users\n         beyond what the individual works permit.  Inclusion of a covered work\n         in an aggregate does not cause this License to apply to the other\n         parts of the aggregate.\n         \n           6. Conveying Non-Source Forms.\n         \n           You may convey a covered work in object code form under the terms\n         of sections 4 and 5, provided that you also convey the\n         machine-readable Corresponding Source under the terms of this License,\n         in one of these ways:\n         \n             a) Convey the object code in, or embodied in, a physical product\n             (including a physical distribution medium), accompanied by the\n             Corresponding Source fixed on a durable physical medium\n             customarily used for software interchange.\n         \n             b) Convey the object code in, or embodied in, a physical product\n             (including a physical distribution medium), accompanied by a\n             written offer, valid for at least three years and valid for as\n             long as you offer spare parts or customer support for that product\n             model, to give anyone who possesses the object code either (1) a\n             copy of the Corresponding Source for all the software in the\n             product that is covered by this License, on a durable physical\n             medium customarily used for software interchange, for a price no\n             more than your reasonable cost of physically performing this\n             conveying of source, or (2) access to copy the\n             Corresponding Source from a network server at no charge.\n         \n             c) Convey individual copies of the object code with a copy of the\n             written offer to provide the Corresponding Source.  This\n             alternative is allowed only occasionally and noncommercially, and\n             only if you received the object code with such an offer, in accord\n             with subsection 6b.\n         \n             d) Convey the object code by offering access from a designated\n             place (gratis or for a charge), and offer equivalent access to the\n             Corresponding Source in the same way through the same place at no\n             further charge.  You need not require recipients to copy the\n             Corresponding Source along with the object code.  If the place to\n             copy the object code is a network server, the Corresponding Source\n             may be on a different server (operated by you or a third party)\n             that supports equivalent copying facilities, provided you maintain\n             clear directions next to the object code saying where to find the\n             Corresponding Source.  Regardless of what server hosts the\n             Corresponding Source, you remain obligated to ensure that it is\n             available for as long as needed to satisfy these requirements.\n         \n             e) Convey the object code using peer-to-peer transmission, provided\n             you inform other peers where the object code and Corresponding\n             Source of the work are being offered to the general public at no\n             charge under subsection 6d.\n         \n           A separable portion of the object code, whose source code is excluded\n         from the Corresponding Source as a System Library, need not be\n         included in conveying the object code work.\n         \n           A \"User Product\" is either (1) a \"consumer product\", which means any\n         tangible personal property which is normally used for personal, family,\n         or household purposes, or (2) anything designed or sold for incorporation\n         into a dwelling.  In determining whether a product is a consumer product,\n         doubtful cases shall be resolved in favor of coverage.  For a particular\n         product received by a particular user, \"normally used\" refers to a\n         typical or common use of that class of product, regardless of the status\n         of the particular user or of the way in which the particular user\n         actually uses, or expects or is expected to use, the product.  A product\n         is a consumer product regardless of whether the product has substantial\n         commercial, industrial or non-consumer uses, unless such uses represent\n         the only significant mode of use of the product.\n         \n           \"Installation Information\" for a User Product means any methods,\n         procedures, authorization keys, or other information required to install\n         and execute modified versions of a covered work in that User Product from\n         a modified version of its Corresponding Source.  The information must\n         suffice to ensure that the continued functioning of the modified object\n         code is in no case prevented or interfered with solely because\n         modification has been made.\n         \n           If you convey an object code work under this section in, or with, or\n         specifically for use in, a User Product, and the conveying occurs as\n         part of a transaction in which the right of possession and use of the\n         User Product is transferred to the recipient in perpetuity or for a\n         fixed term (regardless of how the transaction is characterized), the\n         Corresponding Source conveyed under this section must be accompanied\n         by the Installation Information.  But this requirement does not apply\n         if neither you nor any third party retains the ability to install\n         modified object code on the User Product (for example, the work has\n         been installed in ROM).\n         \n           The requirement to provide Installation Information does not include a\n         requirement to continue to provide support service, warranty, or updates\n         for a work that has been modified or installed by the recipient, or for\n         the User Product in which it has been modified or installed.  Access to a\n         network may be denied when the modification itself materially and\n         adversely affects the operation of the network or violates the rules and\n         protocols for communication across the network.\n         \n           Corresponding Source conveyed, and Installation Information provided,\n         in accord with this section must be in a format that is publicly\n         documented (and with an implementation available to the public in\n         source code form), and must require no special password or key for\n         unpacking, reading or copying.\n         \n           7. Additional Terms.\n         \n           \"Additional permissions\" are terms that supplement the terms of this\n         License by making exceptions from one or more of its conditions.\n         Additional permissions that are applicable to the entire Program shall\n         be treated as though they were included in this License, to the extent\n         that they are valid under applicable law.  If additional permissions\n         apply only to part of the Program, that part may be used separately\n         under those permissions, but the entire Program remains governed by\n         this License without regard to the additional permissions.\n         \n           When you convey a copy of a covered work, you may at your option\n         remove any additional permissions from that copy, or from any part of\n         it.  (Additional permissions may be written to require their own\n         removal in certain cases when you modify the work.)  You may place\n         additional permissions on material, added by you to a covered work,\n         for which you have or can give appropriate copyright permission.\n         \n           Notwithstanding any other provision of this License, for material you\n         add to a covered work, you may (if authorized by the copyright holders of\n         that material) supplement the terms of this License with terms:\n         \n             a) Disclaiming warranty or limiting liability differently from the\n             terms of sections 15 and 16 of this License; or\n         \n             b) Requiring preservation of specified reasonable legal notices or\n             author attributions in that material or in the Appropriate Legal\n             Notices displayed by works containing it; or\n         \n             c) Prohibiting misrepresentation of the origin of that material, or\n             requiring that modified versions of such material be marked in\n             reasonable ways as different from the original version; or\n         \n             d) Limiting the use for publicity purposes of names of licensors or\n             authors of the material; or\n         \n             e) Declining to grant rights under trademark law for use of some\n             trade names, trademarks, or service marks; or\n         \n             f) Requiring indemnification of licensors and authors of that\n             material by anyone who conveys the material (or modified versions of\n             it) with contractual assumptions of liability to the recipient, for\n             any liability that these contractual assumptions directly impose on\n             those licensors and authors.\n         \n           All other non-permissive additional terms are considered \"further\n         restrictions\" within the meaning of section 10.  If the Program as you\n         received it, or any part of it, contains a notice stating that it is\n         governed by this License along with a term that is a further\n         restriction, you may remove that term.  If a license document contains\n         a further restriction but permits relicensing or conveying under this\n         License, you may add to a covered work material governed by the terms\n         of that license document, provided that the further restriction does\n         not survive such relicensing or conveying.\n         \n           If you add terms to a covered work in accord with this section, you\n         must place, in the relevant source files, a statement of the\n         additional terms that apply to those files, or a notice indicating\n         where to find the applicable terms.\n         \n           Additional terms, permissive or non-permissive, may be stated in the\n         form of a separately written license, or stated as exceptions;\n         the above requirements apply either way.\n         \n           8. Termination.\n         \n           You may not propagate or modify a covered work except as expressly\n         provided under this License.  Any attempt otherwise to propagate or\n         modify it is void, and will automatically terminate your rights under\n         this License (including any patent licenses granted under the third\n         paragraph of section 11).\n         \n           However, if you cease all violation of this License, then your\n         license from a particular copyright holder is reinstated (a)\n         provisionally, unless and until the copyright holder explicitly and\n         finally terminates your license, and (b) permanently, if the copyright\n         holder fails to notify you of the violation by some reasonable means\n         prior to 60 days after the cessation.\n         \n           Moreover, your license from a particular copyright holder is\n         reinstated permanently if the copyright holder notifies you of the\n         violation by some reasonable means, this is the first time you have\n         received notice of violation of this License (for any work) from that\n         copyright holder, and you cure the violation prior to 30 days after\n         your receipt of the notice.\n         \n           Termination of your rights under this section does not terminate the\n         licenses of parties who have received copies or rights from you under\n         this License.  If your rights have been terminated and not permanently\n         reinstated, you do not qualify to receive new licenses for the same\n         material under section 10.\n         \n           9. Acceptance Not Required for Having Copies.\n         \n           You are not required to accept this License in order to receive or\n         run a copy of the Program.  Ancillary propagation of a covered work\n         occurring solely as a consequence of using peer-to-peer transmission\n         to receive a copy likewise does not require acceptance.  However,\n         nothing other than this License grants you permission to propagate or\n         modify any covered work.  These actions infringe copyright if you do\n         not accept this License.  Therefore, by modifying or propagating a\n         covered work, you indicate your acceptance of this License to do so.\n         \n           10. Automatic Licensing of Downstream Recipients.\n         \n           Each time you convey a covered work, the recipient automatically\n         receives a license from the original licensors, to run, modify and\n         propagate that work, subject to this License.  You are not responsible\n         for enforcing compliance by third parties with this License.\n         \n           An \"entity transaction\" is a transaction transferring control of an\n         organization, or substantially all assets of one, or subdividing an\n         organization, or merging organizations.  If propagation of a covered\n         work results from an entity transaction, each party to that\n         transaction who receives a copy of the work also receives whatever\n         licenses to the work the party's predecessor in interest had or could\n         give under the previous paragraph, plus a right to possession of the\n         Corresponding Source of the work from the predecessor in interest, if\n         the predecessor has it or can get it with reasonable efforts.\n         \n           You may not impose any further restrictions on the exercise of the\n         rights granted or affirmed under this License.  For example, you may\n         not impose a license fee, royalty, or other charge for exercise of\n         rights granted under this License, and you may not initiate litigation\n         (including a cross-claim or counterclaim in a lawsuit) alleging that\n         any patent claim is infringed by making, using, selling, offering for\n         sale, or importing the Program or any portion of it.\n         \n           11. Patents.\n         \n           A \"contributor\" is a copyright holder who authorizes use under this\n         License of the Program or a work on which the Program is based.  The\n         work thus licensed is called the contributor's \"contributor version\".\n         \n           A contributor's \"essential patent claims\" are all patent claims\n         owned or controlled by the contributor, whether already acquired or\n         hereafter acquired, that would be infringed by some manner, permitted\n         by this License, of making, using, or selling its contributor version,\n         but do not include claims that would be infringed only as a\n         consequence of further modification of the contributor version.  For\n         purposes of this definition, \"control\" includes the right to grant\n         patent sublicenses in a manner consistent with the requirements of\n         this License.\n         \n           Each contributor grants you a non-exclusive, worldwide, royalty-free\n         patent license under the contributor's essential patent claims, to\n         make, use, sell, offer for sale, import and otherwise run, modify and\n         propagate the contents of its contributor version.\n         \n           In the following three paragraphs, a \"patent license\" is any express\n         agreement or commitment, however denominated, not to enforce a patent\n         (such as an express permission to practice a patent or covenant not to\n         sue for patent infringement).  To \"grant\" such a patent license to a\n         party means to make such an agreement or commitment not to enforce a\n         patent against the party.\n         \n           If you convey a covered work, knowingly relying on a patent license,\n         and the Corresponding Source of the work is not available for anyone\n         to copy, free of charge and under the terms of this License, through a\n         publicly available network server or other readily accessible means,\n         then you must either (1) cause the Corresponding Source to be so\n         available, or (2) arrange to deprive yourself of the benefit of the\n         patent license for this particular work, or (3) arrange, in a manner\n         consistent with the requirements of this License, to extend the patent\n         license to downstream recipients.  \"Knowingly relying\" means you have\n         actual knowledge that, but for the patent license, your conveying the\n         covered work in a country, or your recipient's use of the covered work\n         in a country, would infringe one or more identifiable patents in that\n         country that you have reason to believe are valid.\n         \n           If, pursuant to or in connection with a single transaction or\n         arrangement, you convey, or propagate by procuring conveyance of, a\n         covered work, and grant a patent license to some of the parties\n         receiving the covered work authorizing them to use, propagate, modify\n         or convey a specific copy of the covered work, then the patent license\n         you grant is automatically extended to all recipients of the covered\n         work and works based on it.\n         \n           A patent license is \"discriminatory\" if it does not include within\n         the scope of its coverage, prohibits the exercise of, or is\n         conditioned on the non-exercise of one or more of the rights that are\n         specifically granted under this License.  You may not convey a covered\n         work if you are a party to an arrangement with a third party that is\n         in the business of distributing software, under which you make payment\n         to the third party based on the extent of your activity of conveying\n         the work, and under which the third party grants, to any of the\n         parties who would receive the covered work from you, a discriminatory\n         patent license (a) in connection with copies of the covered work\n         conveyed by you (or copies made from those copies), or (b) primarily\n         for and in connection with specific products or compilations that\n         contain the covered work, unless you entered into that arrangement,\n         or that patent license was granted, prior to 28 March 2007.\n         \n           Nothing in this License shall be construed as excluding or limiting\n         any implied license or other defenses to infringement that may\n         otherwise be available to you under applicable patent law.\n         \n           12. No Surrender of Others' Freedom.\n         \n           If conditions are imposed on you (whether by court order, agreement or\n         otherwise) that contradict the conditions of this License, they do not\n         excuse you from the conditions of this License.  If you cannot convey a\n         covered work so as to satisfy simultaneously your obligations under this\n         License and any other pertinent obligations, then as a consequence you may\n         not convey it at all.  For example, if you agree to terms that obligate you\n         to collect a royalty for further conveying from those to whom you convey\n         the Program, the only way you could satisfy both those terms and this\n         License would be to refrain entirely from conveying the Program.\n         \n           13. Use with the GNU Affero General Public License.\n         \n           Notwithstanding any other provision of this License, you have\n         permission to link or combine any covered work with a work licensed\n         under version 3 of the GNU Affero General Public License into a single\n         combined work, and to convey the resulting work.  The terms of this\n         License will continue to apply to the part which is the covered work,\n         but the special requirements of the GNU Affero General Public License,\n         section 13, concerning interaction through a network will apply to the\n         combination as such.\n         \n           14. Revised Versions of this License.\n         \n           The Free Software Foundation may publish revised and/or new versions of\n         the GNU General Public License from time to time.  Such new versions will\n         be similar in spirit to the present version, but may differ in detail to\n         address new problems or concerns.\n         \n           Each version is given a distinguishing version number.  If the\n         Program specifies that a certain numbered version of the GNU General\n         Public License \"or any later version\" applies to it, you have the\n         option of following the terms and conditions either of that numbered\n         version or of any later version published by the Free Software\n         Foundation.  If the Program does not specify a version number of the\n         GNU General Public License, you may choose any version ever published\n         by the Free Software Foundation.\n         \n           If the Program specifies that a proxy can decide which future\n         versions of the GNU General Public License can be used, that proxy's\n         public statement of acceptance of a version permanently authorizes you\n         to choose that version for the Program.\n         \n           Later license versions may give you additional or different\n         permissions.  However, no additional obligations are imposed on any\n         author or copyright holder as a result of your choosing to follow a\n         later version.\n         \n           15. Disclaimer of Warranty.\n         \n           THERE IS NO WARRANTY FOR THE PROGRAM, TO THE EXTENT PERMITTED BY\n         APPLICABLE LAW.  EXCEPT WHEN OTHERWISE STATED IN WRITING THE COPYRIGHT\n         HOLDERS AND/OR OTHER PARTIES PROVIDE THE PROGRAM \"AS IS\" WITHOUT WARRANTY\n         OF ANY KIND, EITHER EXPRESSED OR IMPLIED, INCLUDING, BUT NOT LIMITED TO,\n         THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n         PURPOSE.  THE ENTIRE RISK AS TO THE QUALITY AND PERFORMANCE OF THE PROGRAM\n         IS WITH YOU.  SHOULD THE PROGRAM PROVE DEFECTIVE, YOU ASSUME THE COST OF\n         ALL NECESSARY SERVICING, REPAIR OR CORRECTION.\n         \n           16. Limitation of Liability.\n         \n           IN NO EVENT UNLESS REQUIRED BY APPLICABLE LAW OR AGREED TO IN WRITING\n         WILL ANY COPYRIGHT HOLDER, OR ANY OTHER PARTY WHO MODIFIES AND/OR CONVEYS\n         THE PROGRAM AS PERMITTED ABOVE, BE LIABLE TO YOU FOR DAMAGES, INCLUDING ANY\n         GENERAL, SPECIAL, INCIDENTAL OR CONSEQUENTIAL DAMAGES ARISING OUT OF THE\n         USE OR INABILITY TO USE THE PROGRAM (INCLUDING BUT NOT LIMITED TO LOSS OF\n         DATA OR DATA BEING RENDERED INACCURATE OR LOSSES SUSTAINED BY YOU OR THIRD\n         PARTIES OR A FAILURE OF THE PROGRAM TO OPERATE WITH ANY OTHER PROGRAMS),\n         EVEN IF SUCH HOLDER OR OTHER PARTY HAS BEEN ADVISED OF THE POSSIBILITY OF\n         SUCH DAMAGES.\n         \n           17. Interpretation of Sections 15 and 16.\n         \n           If the disclaimer of warranty and limitation of liability provided\n         above cannot be given local legal effect according to their terms,\n         reviewing courts shall apply local law that most closely approximates\n         an absolute waiver of all civil liability in connection with the\n         Program, unless a warranty or assumption of liability accompanies a\n         copy of the Program in return for a fee.\n         \n                              END OF TERMS AND CONDITIONS\n         \n                     How to Apply These Terms to Your New Programs\n         \n           If you develop a new program, and you want it to be of the greatest\n         possible use to the public, the best way to achieve this is to make it\n         free software which everyone can redistribute and change under these terms.\n         \n           To do so, attach the following notices to the program.  It is safest\n         to attach them to the start of each source file to most effectively\n         state the exclusion of warranty; and each file should have at least\n         the \"copyright\" line and a pointer to where the full notice is found.\n         \n             <one line to give the program's name and a brief idea of what it does.>\n             Copyright (C) <year>  <name of author>\n         \n             This program is free software: you can redistribute it and/or modify\n             it under the terms of the GNU General Public License as published by\n             the Free Software Foundation, either version 3 of the License, or\n             (at your option) any later version.\n         \n             This program is distributed in the hope that it will be useful,\n             but WITHOUT ANY WARRANTY; without even the implied warranty of\n             MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n             GNU General Public License for more details.\n         \n             You should have received a copy of the GNU General Public License\n             along with this program.  If not, see <http://www.gnu.org/licenses/>.\n         \n         Also add information on how to contact you by electronic and paper mail.\n         \n           If the program does terminal interaction, make it output a short\n         notice like this when it starts in an interactive mode:\n         \n             <program>  Copyright (C) <year>  <name of author>\n             This program comes with ABSOLUTELY NO WARRANTY; for details type `show w'.\n             This is free software, and you are welcome to redistribute it\n             under certain conditions; type `show c' for details.\n         \n         The hypothetical commands `show w' and `show c' should show the appropriate\n         parts of the General Public License.  Of course, your program's commands\n         might be different; for a GUI interface, you would use an \"about box\".\n         \n           You should also get your employer (if you work as a programmer) or school,\n         if any, to sign a \"copyright disclaimer\" for the program, if necessary.\n         For more information on this, and how to apply and follow the GNU GPL, see\n         <http://www.gnu.org/licenses/>.\n         \n           The GNU General Public License does not permit incorporating your program\n         into proprietary programs.  If your program is a subroutine library, you\n         may consider it more useful to permit linking proprietary applications with\n         the library.  If this is what you want to do, use the GNU Lesser General\n         Public License instead of this License.  But first, please read\n         <http://www.gnu.org/philosophy/why-not-lgpl.html>.\n         \n         \n         Name: libquadmath\n         Files: scipy.libs/libquadmath*.so\n         Description: dynamically linked to files compiled with gcc\n         Availability: https://gcc.gnu.org/git/?p=gcc.git;a=tree;f=libquadmath\n         License: LGPL-2.1-or-later\n         \n             GCC Quad-Precision Math Library\n             Copyright (C) 2010-2019 Free Software Foundation, Inc.\n             Written by Francois-Xavier Coudert  <fxcoudert@gcc.gnu.org>\n         \n             This file is part of the libquadmath library.\n             Libquadmath is free software; you can redistribute it and/or\n             modify it under the terms of the GNU Library General Public\n             License as published by the Free Software Foundation; either\n             version 2.1 of the License, or (at your option) any later version.\n         \n             Libquadmath is distributed in the hope that it will be useful,\n             but WITHOUT ANY WARRANTY; without even the implied warranty of\n             MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\n             Lesser General Public License for more details.\n             https://www.gnu.org/licenses/old-licenses/lgpl-2.1.html\n         ",
        "classifier": [
          "Development Status :: 5 - Production/Stable",
          "Intended Audience :: Science/Research",
          "Intended Audience :: Developers",
          "License :: OSI Approved :: BSD License",
          "Programming Language :: C",
          "Programming Language :: Python",
          "Programming Language :: Python :: 3",
          "Programming Language :: Python :: 3.10",
          "Programming Language :: Python :: 3.11",
          "Programming Language :: Python :: 3.12",
          "Programming Language :: Python :: 3.13",
          "Topic :: Software Development :: Libraries",
          "Topic :: Scientific/Engineering",
          "Operating System :: Microsoft :: Windows",
          "Operating System :: POSIX :: Linux",
          "Operating System :: POSIX",
          "Operating System :: Unix",
          "Operating System :: MacOS"
        ],
        "requires_dist": [
          "numpy<2.5,>=1.23.5",
          "pytest; extra == \"test\"",
          "pytest-cov; extra == \"test\"",
          "pytest-timeout; extra == \"test\"",
          "pytest-xdist; extra == \"test\"",
          "asv; extra == \"test\"",
          "mpmath; extra == \"test\"",
          "gmpy2; extra == \"test\"",
          "threadpoolctl; extra == \"test\"",
          "scikit-umfpack; extra == \"test\"",
          "pooch; extra == \"test\"",
          "hypothesis>=6.30; extra == \"test\"",
          "array-api-strict<2.1.1,>=2.0; extra == \"test\"",
          "Cython; extra == \"test\"",
          "meson; extra == \"test\"",
          "ninja; sys_platform != \"emscripten\" and extra == \"test\"",
          "sphinx<8.0.0,>=5.0.0; extra == \"doc\"",
          "intersphinx_registry; extra == \"doc\"",
          "pydata-sphinx-theme>=0.15.2; extra == \"doc\"",
          "sphinx-copybutton; extra == \"doc\"",
          "sphinx-design>=0.4.0; extra == \"doc\"",
          "matplotlib>=3.5; extra == \"doc\"",
          "numpydoc; extra == \"doc\"",
          "jupytext; extra == \"doc\"",
          "myst-nb; extra == \"doc\"",
          "pooch; extra == \"doc\"",
          "jupyterlite-sphinx>=0.19.1; extra == \"doc\"",
          "jupyterlite-pyodide-kernel; extra == \"doc\"",
          "mypy==1.10.0; extra == \"dev\"",
          "typing_extensions; extra == \"dev\"",
          "types-psutil; extra == \"dev\"",
          "pycodestyle; extra == \"dev\"",
          "ruff>=0.0.292; extra == \"dev\"",
          "cython-lint>=0.12.2; extra == \"dev\"",
          "rich-click; extra == \"dev\"",
          "doit>=0.36.0; extra == \"dev\"",
          "pydevtool; extra == \"dev\""
        ],
        "requires_python": ">=3.10",
        "project_url": [
          "homepage, https://scipy.org/",
          "documentation, https://docs.scipy.org/doc/scipy/",
          "source, https://github.com/scipy/scipy",
          "download, https://github.com/scipy/scipy/releases",
          "tracker, https://github.com/scipy/scipy/issues"
        ],
        "provides_extra": [
          "test",
          "doc",
          "dev"
        ],
        "description": ".. image:: https://raw.githubusercontent.com/scipy/scipy/main/doc/source/_static/logo.svg\n  :target: https://scipy.org\n  :width: 110\n  :height: 110\n  :align: left \n\n.. image:: https://img.shields.io/badge/powered%20by-NumFOCUS-orange.svg?style=flat&colorA=E1523D&colorB=007D8A\n  :target: https://numfocus.org\n\n.. image:: https://img.shields.io/pypi/dm/scipy.svg?label=Pypi%20downloads\n  :target: https://pypi.org/project/scipy/\n\n.. image:: https://img.shields.io/conda/dn/conda-forge/scipy.svg?label=Conda%20downloads\n  :target: https://anaconda.org/conda-forge/scipy\n\n.. image:: https://img.shields.io/badge/stackoverflow-Ask%20questions-blue.svg\n  :target: https://stackoverflow.com/questions/tagged/scipy\n\n.. image:: https://img.shields.io/badge/DOI-10.1038%2Fs41592--019--0686--2-blue.svg\n  :target: https://www.nature.com/articles/s41592-019-0686-2\n\nSciPy (pronounced \"Sigh Pie\") is an open-source software for mathematics,\nscience, and engineering. It includes modules for statistics, optimization,\nintegration, linear algebra, Fourier transforms, signal and image processing,\nODE solvers, and more.\n\n- **Website:** https://scipy.org\n- **Documentation:** https://docs.scipy.org/doc/scipy/\n- **Development version of the documentation:** https://scipy.github.io/devdocs\n- **SciPy development forum:** https://discuss.scientific-python.org/c/contributor/scipy\n- **Stack Overflow:** https://stackoverflow.com/questions/tagged/scipy\n- **Source code:** https://github.com/scipy/scipy\n- **Contributing:** https://scipy.github.io/devdocs/dev/index.html\n- **Bug reports:** https://github.com/scipy/scipy/issues\n- **Code of Conduct:** https://docs.scipy.org/doc/scipy/dev/conduct/code_of_conduct.html\n- **Report a security vulnerability:** https://tidelift.com/docs/security\n- **Citing in your work:** https://www.scipy.org/citing-scipy/\n\nSciPy is built to work with\nNumPy arrays, and provides many user-friendly and efficient numerical routines,\nsuch as routines for numerical integration and optimization. Together, they\nrun on all popular operating systems, are quick to install, and are free of\ncharge. NumPy and SciPy are easy to use, but powerful enough to be depended\nupon by some of the world's leading scientists and engineers. If you need to\nmanipulate numbers on a computer and display or publish the results, give\nSciPy a try!\n\nFor the installation instructions, see `our install\nguide <https://scipy.org/install/>`__.\n\n\nCall for Contributions\n----------------------\n\nWe appreciate and welcome contributions. Small improvements or fixes are always appreciated; issues labeled as \"good\nfirst issue\" may be a good starting point. Have a look at `our contributing\nguide <https://scipy.github.io/devdocs/dev/index.html>`__.\n\nWriting code isn‚Äôt the only way to contribute to SciPy. You can also:\n\n- review pull requests\n- triage issues\n- develop tutorials, presentations, and other educational materials\n- maintain and improve `our website <https://github.com/scipy/scipy.org>`__\n- develop graphic design for our brand assets and promotional materials\n- help with outreach and onboard new contributors\n- write grant proposals and help with other fundraising efforts\n\nIf you‚Äôre unsure where to start or how your skills fit in, reach out! You can\nask on the `forum <https://discuss.scientific-python.org/c/contributor/scipy>`__\nor here, on GitHub, by leaving a comment on a relevant issue that is already\nopen.\n\nIf you are new to contributing to open source, `this\nguide <https://opensource.guide/how-to-contribute/>`__ helps explain why, what,\nand how to get involved.\n"
      }
    },
    {
      "download_info": {
        "url": "https://pypi.tuna.tsinghua.edu.cn/packages/58/ae/c0e4a53d77cf6e9a04179535766b3321b0b9ced5f70522e4caf9329f0046/soundfile-0.13.1-py2.py3-none-manylinux_2_28_aarch64.whl",
        "archive_info": {
          "hash": "sha256=9c9e855f5a4d06ce4213f31918653ab7de0c5a8d8107cd2427e44b42df547deb",
          "hashes": {
            "sha256": "9c9e855f5a4d06ce4213f31918653ab7de0c5a8d8107cd2427e44b42df547deb"
          }
        }
      },
      "is_direct": false,
      "is_yanked": false,
      "requested": false,
      "metadata": {
        "metadata_version": "2.1",
        "name": "soundfile",
        "version": "0.13.1",
        "platform": [
          "any"
        ],
        "summary": "An audio library based on libsndfile, CFFI and NumPy",
        "description_content_type": "text/x-rst",
        "keywords": [
          "audio",
          "libsndfile"
        ],
        "home_page": "https://github.com/bastibe/python-soundfile",
        "author": "Bastian Bechtold",
        "author_email": "basti@bastibe.de",
        "license": "BSD 3-Clause License",
        "license_file": [
          "LICENSE"
        ],
        "classifier": [
          "Development Status :: 5 - Production/Stable",
          "Intended Audience :: Developers",
          "Intended Audience :: Science/Research",
          "License :: OSI Approved :: BSD License",
          "Natural Language :: English",
          "Operating System :: OS Independent",
          "Programming Language :: Python",
          "Programming Language :: Python :: 3",
          "Programming Language :: Python :: 2",
          "Programming Language :: Python :: Implementation :: PyPy",
          "Programming Language :: Python :: Implementation :: CPython",
          "Topic :: Multimedia :: Sound/Audio"
        ],
        "requires_dist": [
          "cffi >=1.0",
          "numpy"
        ],
        "provides_extra": [
          "numpy"
        ],
        "description": "python-soundfile\n================\n\n|version| |python| |status| |license|\n\n|contributors| |downloads|\n\nThe `soundfile <https://github.com/bastibe/python-soundfile>`__ module is an audio\nlibrary based on libsndfile, CFFI and NumPy. Full documentation is\navailable on https://python-soundfile.readthedocs.io/.\n\nThe ``soundfile`` module can read and write sound files. File reading/writing is\nsupported through `libsndfile <http://www.mega-nerd.com/libsndfile/>`__,\nwhich is a free, cross-platform, open-source (LGPL) library for reading\nand writing many different sampled sound file formats that runs on many\nplatforms including Windows, OS X, and Unix. It is accessed through\n`CFFI <https://cffi.readthedocs.io/>`__, which is a foreign function\ninterface for Python calling C code. CFFI is supported for CPython 2.6+,\n3.x and PyPy 2.0+. The ``soundfile`` module represents audio data as NumPy arrays.\n\n| python-soundfile is BSD licensed (BSD 3-Clause License).\n| (c) 2013, Bastian Bechtold\n\n\n|open-issues| |closed-issues| |open-prs| |closed-prs|\n\n.. |contributors| image:: https://img.shields.io/github/contributors/bastibe/python-soundfile.svg\n.. |version| image:: https://img.shields.io/pypi/v/soundfile.svg\n.. |python| image:: https://img.shields.io/pypi/pyversions/soundfile.svg\n.. |license| image:: https://img.shields.io/github/license/bastibe/python-soundfile.svg\n.. |downloads| image:: https://img.shields.io/pypi/dm/soundfile.svg\n.. |open-issues| image:: https://img.shields.io/github/issues/bastibe/python-soundfile.svg\n.. |closed-issues| image:: https://img.shields.io/github/issues-closed/bastibe/python-soundfile.svg\n.. |open-prs| image:: https://img.shields.io/github/issues-pr/bastibe/python-soundfile.svg\n.. |closed-prs| image:: https://img.shields.io/github/issues-pr-closed/bastibe/python-soundfile.svg\n.. |status| image:: https://img.shields.io/pypi/status/soundfile.svg\n\nBreaking Changes\n----------------\n\nThe ``soundfile`` module has evolved rapidly in the past. Most\nnotably, we changed the import name from ``import pysoundfile`` to\n``import soundfile`` in 0.7. In 0.6, we cleaned up many small\ninconsistencies, particularly in the the ordering and naming of\nfunction arguments and the removal of the indexing interface.\n\nIn 0.8.0, we changed the default value of ``always_2d`` from ``True``\nto ``False``. Also, the order of arguments of the ``write`` function\nchanged from ``write(data, file, ...)`` to ``write(file, data, ...)``.\n\nIn 0.9.0, we changed the ``ctype`` arguments of the ``buffer_*``\nmethods to ``dtype``, using the Numpy ``dtype`` notation. The old\n``ctype`` arguments still work, but are now officially deprecated.\n\nIn 0.12.0, we changed the load order of the libsndfile library. Now,\nthe packaged libsndfile in the platform-specific wheels is tried\nbefore falling back to any system-provided libsndfile. If you would\nprefer using the system-provided libsndfile, install the source\npackage or source wheel instead of the platform-specific wheels.\n\nInstallation\n------------\n\nThe ``soundfile`` module depends on the Python packages CFFI and NumPy, and the\nlibrary libsndfile.\n\nIn a modern Python, you can use ``pip install soundfile`` to download\nand install the latest release of the ``soundfile`` module and its\ndependencies. On Windows (64/32) and OS X (Intel/ARM) and Linux 64,\nthis will also install a current version of the library libsndfile. If\nyou install the source module, you need to install libsndfile using\nyour distribution's package manager, for example ``sudo apt install\nlibsndfile1``.\n\nIf you are running on an unusual platform or if you are using an older\nversion of Python, you might need to install NumPy and CFFI separately,\nfor example using the Anaconda_ package manager.\n\n.. _Anaconda: https://www.continuum.io/downloads\n\nBuilding\n--------\n\n``Soundfile`` itself does not contain any compiled code and can be\nbundled into a wheel with the usual ``python setup.py bdist_wheel``.\nHowever, ``soundfile`` relies on libsndfile, and optionally ships its\nown copy of libsndfile in the wheel.\n\nTo build a binary wheel that contains libsndfile, make sure to\ncheckout and update the ``_soundfile_data`` submodule, then run\n``python setup.py bdist_wheel`` as usual. If the resulting file size\nof the wheel is around one megabyte, a matching libsndfile has been\nbundled (without libsndfile, it's around 25 KB).\n\nTo build binary wheels for all supported platforms, run ``python\nbuild_wheels.py``, which will ``python setup.py bdist_wheel`` for each\nof the platforms we have precompiled libsndfiles for.\n\nError Reporting\n---------------\n\nIn case of API usage errors the ``soundfile`` module raises the usual `ValueError` or `TypeError`.\n\nFor other errors `SoundFileError` is raised (used to be `RuntimeError`).\nParticularly, a `LibsndfileError` subclass of this exception is raised on\nerrors reported by the libsndfile library. In that case the exception object\nprovides the libsndfile internal error code in the `LibsndfileError.code` attribute and the raw\nlibsndfile error message in the `LibsndfileError.error_string` attribute.\n\nRead/Write Functions\n--------------------\n\nData can be written to the file using `soundfile.write()`, or read from\nthe file using `soundfile.read()`. The ``soundfile`` module can open all file formats\nthat `libsndfile supports\n<http://www.mega-nerd.com/libsndfile/#Features>`__, for example WAV,\nFLAC, OGG and MAT files (see `Known Issues <https://github.com/bastibe/python-soundfile#known-issues>`__ below about writing OGG files).\n\nHere is an example for a program that reads a wave file and copies it\ninto an FLAC file:\n\n.. code:: python\n\n    import soundfile as sf\n\n    data, samplerate = sf.read('existing_file.wav')\n    sf.write('new_file.flac', data, samplerate)\n\nBlock Processing\n----------------\n\nSound files can also be read in short, optionally overlapping blocks\nwith `soundfile.blocks()`.\nFor example, this calculates the signal level for each block of a long\nfile:\n\n.. code:: python\n\n   import numpy as np\n   import soundfile as sf\n\n   rms = [np.sqrt(np.mean(block**2)) for block in\n          sf.blocks('myfile.wav', blocksize=1024, overlap=512)]\n\n``SoundFile`` Objects\n---------------------\n\nSound files can also be opened as `SoundFile` objects. Every\n`SoundFile` has a specific sample rate, data format and a set number of\nchannels.\n\nIf a file is opened, it is kept open for as long as the `SoundFile`\nobject exists. The file closes when the object is garbage collected,\nbut you should use the `SoundFile.close()` method or the\ncontext manager to close the file explicitly:\n\n.. code:: python\n\n   import soundfile as sf\n\n   with sf.SoundFile('myfile.wav', 'r+') as f:\n       while f.tell() < f.frames:\n           pos = f.tell()\n           data = f.read(1024)\n           f.seek(pos)\n           f.write(data*2)\n\nAll data access uses frames as index. A frame is one discrete time-step\nin the sound file. Every frame contains as many samples as there are\nchannels in the file.\n\nRAW Files\n---------\n\n`soundfile.read()` can usually auto-detect the file type of sound files. This\nis not possible for RAW files, though:\n\n.. code:: python\n\n   import soundfile as sf\n\n   data, samplerate = sf.read('myfile.raw', channels=1, samplerate=44100,\n                              subtype='FLOAT')\n\nNote that on x86, this defaults to ``endian='LITTLE'``. If you are\nreading big endian data (mostly old PowerPC/6800-based files), you\nhave to set ``endian='BIG'`` accordingly.\n\nYou can write RAW files in a similar way, but be advised that in most\ncases, a more expressive format is better and should be used instead.\n\nVirtual IO\n----------\n\nIf you have an open file-like object, `soundfile.read()` can open it just like\nregular files:\n\n.. code:: python\n\n    import soundfile as sf\n    with open('filename.flac', 'rb') as f:\n        data, samplerate = sf.read(f)\n\nHere is an example using an HTTP request:\n\n.. code:: python\n\n    import io\n    import soundfile as sf\n    from urllib.request import urlopen\n\n    url = \"http://tinyurl.com/shepard-risset\"\n    data, samplerate = sf.read(io.BytesIO(urlopen(url).read()))\n\nNote that the above example only works with Python 3.x.\nFor Python 2.x support, replace the third line with:\n\n.. code:: python\n\n    from urllib2 import urlopen\n\nIn-memory files\n^^^^^^^^^^^^^^^\n\nChunks of audio, i.e. `bytes`, can also be read and written without touching the filesystem.\nIn the following example OGG is converted to WAV entirely in memory (without writing files to the disk):\n\n.. code:: python\n\n    import io\n    import soundfile as sf\n\n    def ogg2wav(ogg: bytes):\n        ogg_buf = io.BytesIO(ogg)\n        ogg_buf.name = 'file.ogg'\n        data, samplerate = sf.read(ogg_buf)\n        wav_buf = io.BytesIO()\n        wav_buf.name = 'file.wav'\n        sf.write(wav_buf, data, samplerate)\n        wav_buf.seek(0)  # Necessary for `.read()` to return all bytes\n        return wav_buf.read()\n\nControlling bitrate mode and compression level\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nFor some audio formats, you can control the bitrate and compression level. \n\n`compression_level` is a float between 0 and 1, with 1 being the highest compression, \nand `bitrate_mode` is 'VARIABLE', 'CONSTANT', or 'AVERAGE'.\n\n.. code:: python\n\n    import soundfile as sf\n    \n    # for example, this uncompressed 5 minute wav file with 32 kHz sample rate is 18 Mb\n    data, samplerate = sf.read('5min_32kHz.wav') \n    \n    # maximum mp3 compression results in 1.1 Mb file, with either CONSTANT or VARIABLE bit rate\n    sf.write('max_compression_vbr.mp3', data, samplerate, bitrate_mode='VARIABLE', compression_level=.99) \n    sf.write('max_compression_cbr.mp3', data, samplerate, bitrate_mode='CONSTANT', compression_level=.99)\n    \n    # minimum mp3 compression results in 3.5 Mb file\n    sf.write('min_compression_vbr.mp3', data, samplerate, bitrate_mode='VARIABLE', compression_level=0)\n\nKnown Issues\n------------\n\nWriting to OGG files can result in empty files with certain versions of libsndfile. See `#130 <https://github.com/bastibe/python-soundfile/issues/130>`__ for news on this issue.\n\nIf using a Buildroot style system, Python has trouble locating ``libsndfile.so`` file, which causes python-soundfile to not be loaded. This is apparently a bug in `python <https://bugs.python.org/issue13508>`__. For the time being, in ``soundfile.py``, you can remove the call to ``_find_library`` and hardcode the location of the ``libsndfile.so`` in ``_ffi.dlopen``. See `#258 <https://github.com/bastibe/python-soundfile/issues/258>`__ for discussion on this issue.\n\nNews\n----\n\n2013-08-27 V0.1.0 Bastian Bechtold:\n    Initial prototype. A simple wrapper for libsndfile in Python\n\n2013-08-30 V0.2.0 Bastian Bechtold:\n    Bugfixes and more consistency with PySoundCard\n\n2013-08-30 V0.2.1 Bastian Bechtold:\n    Bugfixes\n\n2013-09-27 V0.3.0 Bastian Bechtold:\n    Added binary installer for Windows, and context manager\n\n2013-11-06 V0.3.1 Bastian Bechtold:\n    Switched from distutils to setuptools for easier installation\n\n2013-11-29 V0.4.0 Bastian Bechtold:\n    Thanks to David Blewett, now with Virtual IO!\n\n2013-12-08 V0.4.1 Bastian Bechtold:\n    Thanks to Xidorn Quan, FLAC files are not float32 any more.\n\n2014-02-26 V0.5.0 Bastian Bechtold:\n    Thanks to Matthias Geier, improved seeking and a flush() method.\n\n2015-01-19 V0.6.0 Bastian Bechtold:\n    A big, big thank you to Matthias Geier, who did most of the work!\n\n    - Switched to ``float64`` as default data type.\n    - Function arguments changed for consistency.\n    - Added unit tests.\n    - Added global `read()`, `write()`, `blocks()` convenience\n      functions.\n    - Documentation overhaul and hosting on readthedocs.\n    - Added ``'x'`` open mode.\n    - Added `tell()` method.\n    - Added ``__repr__()`` method.\n\n2015-04-12 V0.7.0 Bastian Bechtold:\n    Again, thanks to Matthias Geier for all of his hard work, but also\n    Nils Werner and Whistler7 for their many suggestions and help.\n\n    - Renamed ``import pysoundfile`` to ``import soundfile``.\n    - Installation through pip wheels that contain the necessary\n      libraries for OS X and Windows.\n    - Removed ``exclusive_creation`` argument to `write()`.\n    - Added `truncate()` method.\n\n2015-10-20 V0.8.0 Bastian Bechtold:\n    Again, Matthias Geier contributed a whole lot of hard work to this\n    release.\n\n    - Changed the default value of ``always_2d`` from ``True`` to\n      ``False``.\n    - Numpy is now optional, and only loaded for ``read`` and\n      ``write``.\n    - Added `SoundFile.buffer_read()` and\n      `SoundFile.buffer_read_into()` and `SoundFile.buffer_write()`,\n      which read/write raw data without involving Numpy.\n    - Added `info()` function that returns metadata of a sound file.\n    - Changed the argument order of the `write()` function from\n      ``write(data, file, ...)`` to ``write(file, data, ...)``\n\n    And many more minor bug fixes.\n\n2017-02-02 V0.9.0 Bastian Bechtold:\n    Thank you, Matthias Geier, Tomas Garcia, and Todd, for contributions\n    for this release.\n\n    - Adds support for ALAC files.\n    - Adds new member ``__libsndfile_version__``\n    - Adds number of frames to ``info`` class\n    - Adds ``dtype`` argument to ``buffer_*`` methods\n    - Deprecates ``ctype`` argument to ``buffer_*`` methods\n    - Adds official support for Python 3.6\n\n    And some minor bug fixes.\n\n2017-11-12 V0.10.0 Bastian Bechtold:\n    Thank you, Matthias Geier, Toni Barth, Jon Peirce, Till Hoffmann,\n    and Tomas Garcia, for contributions to this release.\n\n    - Should now work with cx_freeze.\n    - Several documentation fixes in the README.\n    - Removes deprecated ``ctype`` argument in favor of ``dtype`` in ``buffer_*()``.\n    - Adds `SoundFile.frames` in favor of now-deprecated ``__len__()``.\n    - Improves performance of `blocks()` and `SoundFile.blocks()`.\n    - Improves import time by using CFFI's out of line mode.\n    - Adds a build script for building distributions.\n\n2022-06-02 V0.11.0 Bastian Bechtold:\n    Thank you, tennies, Hannes Helmholz, Christoph Boeddeker, Matt\n    Vollrath, Matthias Geier, Jacek Konieczny, Boris Verkhovskiy,\n    Jonas Haag, Eduardo Moguillansky, Panos Laganakos, Jarvy Jarvison,\n    Domingo Ramirez, Tim Chagnon, Kyle Benesch, Fabian-Robert St√∂ter,\n    Joe Todd\n\n    - MP3 support\n    - Adds binary wheels for macOS M1\n    - Improves compatibility with macOS, specifically for M1 machines\n    - Fixes file descriptor open for binary wheels on Windows and Python 3.5+\n    - Updates libsndfile to v1.1.0\n    - Adds get_strings method for retrieving all metadata at once\n    - Improves documentation, error messages and tests\n    - Displays length of very short files in samples\n    - Supports the file system path protocol (pathlib et al)\n\n2023-02-02 V0.12.0 Bastian Bechtold\n    Thank you, Barabazs, Andrew Murray, Jon Peirce, for contributions\n    to this release.\n\n    - Updated libsndfile to v1.2.0\n    - Improves precompiled library location, especially with py2app or cx-freeze.\n    - Now provide binary wheels for Linux x86_64\n    - Now prefers packaged libsndfile over system-installed libsndfile\n\n2023-02-15 V0.12.1 Bastian Bechtold\n    Thank you, funnypig, for the bug report\n\n    - Fixed typo on library location detection if no packaged lib and\n      no system lib was found\n\n2025-01-02 V0.13.0 Bastian Bechtold\n    Thank you, Zhong Jianxin, mcclure, jneuendorf-i4h, aoirint, endolith, Guy Illes, ytya, Sam Lapp, Benjamin Moody\n\n    - Updated libsndfile to v1.2.2\n    - Linux arm64 builds added\n    - Numpy is now a dependency\n    - Fixed error in blocks, if file is very short\n    - Compression level and bitrate controls added for compressed files\n    - Various README improvements\n    - Various build system improvements\n    - Various improvements to error messages\n\n2025-01-25 V0.13.1 Bastian Bechtold\n    Thank you, Brian McFee and Guy Illes\n\n    - Fixed regression in blocks\n"
      }
    },
    {
      "download_info": {
        "url": "https://pypi.tuna.tsinghua.edu.cn/packages/4f/27/6933a8b2562d7bd1fb595074cf99cc81fc3789f6a6c05cdabb46284a3188/cffi-2.0.0-cp310-cp310-manylinux2014_aarch64.manylinux_2_17_aarch64.whl",
        "archive_info": {
          "hash": "sha256=3e837e369566884707ddaf85fc1744b47575005c0a229de3327f8f9a20f4efeb",
          "hashes": {
            "sha256": "3e837e369566884707ddaf85fc1744b47575005c0a229de3327f8f9a20f4efeb"
          }
        }
      },
      "is_direct": false,
      "is_yanked": false,
      "requested": false,
      "metadata": {
        "metadata_version": "2.4",
        "name": "cffi",
        "version": "2.0.0",
        "dynamic": [
          "license-file"
        ],
        "summary": "Foreign Function Interface for Python calling C code.",
        "description_content_type": "text/markdown",
        "author": "Armin Rigo, Maciej Fijalkowski",
        "maintainer": "Matt Davis, Matt Clay, Matti Picus",
        "license_expression": "MIT",
        "license_file": [
          "LICENSE",
          "AUTHORS"
        ],
        "classifier": [
          "Programming Language :: Python",
          "Programming Language :: Python :: 3",
          "Programming Language :: Python :: 3.9",
          "Programming Language :: Python :: 3.10",
          "Programming Language :: Python :: 3.11",
          "Programming Language :: Python :: 3.12",
          "Programming Language :: Python :: 3.13",
          "Programming Language :: Python :: 3.14",
          "Programming Language :: Python :: Free Threading :: 2 - Beta",
          "Programming Language :: Python :: Implementation :: CPython"
        ],
        "requires_dist": [
          "pycparser; implementation_name != \"PyPy\""
        ],
        "requires_python": ">=3.9",
        "project_url": [
          "Documentation, https://cffi.readthedocs.io/",
          "Changelog, https://cffi.readthedocs.io/en/latest/whatsnew.html",
          "Downloads, https://github.com/python-cffi/cffi/releases",
          "Contact, https://groups.google.com/forum/#!forum/python-cffi",
          "Source Code, https://github.com/python-cffi/cffi",
          "Issue Tracker, https://github.com/python-cffi/cffi/issues"
        ],
        "description": "[![GitHub Actions Status](https://github.com/python-cffi/cffi/actions/workflows/ci.yaml/badge.svg?branch=main)](https://github.com/python-cffi/cffi/actions/workflows/ci.yaml?query=branch%3Amain++)\n[![PyPI version](https://img.shields.io/pypi/v/cffi.svg)](https://pypi.org/project/cffi)\n[![Read the Docs](https://img.shields.io/badge/docs-latest-blue.svg)][Documentation]\n\n\nCFFI\n====\n\nForeign Function Interface for Python calling C code.\n\nPlease see the [Documentation] or uncompiled in the `doc/` subdirectory.\n\nDownload\n--------\n\n[Download page](https://github.com/python-cffi/cffi/releases)\n\nSource Code\n-----------\n\nSource code is publicly available on\n[GitHub](https://github.com/python-cffi/cffi).\n\nContact\n-------\n\n[Mailing list](https://groups.google.com/forum/#!forum/python-cffi)\n\nTesting/development tips\n------------------------\n\nAfter `git clone` or `wget && tar`, we will get a directory called `cffi` or `cffi-x.x.x`. we call it `repo-directory`. To run tests under CPython, run the following in the `repo-directory`:\n\n    pip install pytest\n    pip install -e .  # editable install of CFFI for local development\n    pytest src/c/ testing/\n\n[Documentation]: http://cffi.readthedocs.org/\n"
      }
    },
    {
      "download_info": {
        "url": "https://pypi.tuna.tsinghua.edu.cn/packages/07/30/dcf9c45aca696c44d37f69f167c6ceae3e58cca853031836d5a0a7c58f3b/jaconv-0.4.1-py3-none-any.whl",
        "archive_info": {
          "hash": "sha256=98d42263225f0c1b780c6428f3ac65b96d652248929d1757520afef90687c2d7",
          "hashes": {
            "sha256": "98d42263225f0c1b780c6428f3ac65b96d652248929d1757520afef90687c2d7"
          }
        }
      },
      "is_direct": false,
      "is_yanked": false,
      "requested": false,
      "metadata": {
        "metadata_version": "2.4",
        "name": "jaconv",
        "version": "0.4.1",
        "dynamic": [
          "description",
          "license-file",
          "summary"
        ],
        "platform": [
          "POSIX",
          "Windows",
          "Unix",
          "MacOS"
        ],
        "summary": "Pure-Python Japanese character interconverter for Hiragana, Katakana, Hankaku, Zenkaku and more",
        "keywords": [
          "Japanese converter",
          "Japanese",
          "text preprocessing",
          "half-width kana",
          "Hiragana",
          "Katakana",
          "Hankaku",
          "Zenkaku",
          "transliteration",
          "Julius"
        ],
        "home_page": "https://github.com/ikegami-yukino/jaconv",
        "author": "Yukino Ikegami",
        "author_email": "yknikgm@gmail.com",
        "license": "MIT License",
        "license_file": [
          "LICENSE"
        ],
        "classifier": [
          "Development Status :: 5 - Production/Stable",
          "Intended Audience :: Developers",
          "Intended Audience :: Information Technology",
          "Natural Language :: Japanese",
          "Operating System :: MacOS",
          "Operating System :: Microsoft",
          "Operating System :: POSIX",
          "Programming Language :: Python :: 2.7",
          "Programming Language :: Python :: 3",
          "Programming Language :: Python :: 3.4",
          "Programming Language :: Python :: 3.5",
          "Programming Language :: Python :: 3.6",
          "Programming Language :: Python :: 3.7",
          "Programming Language :: Python :: 3.8",
          "Programming Language :: Python :: 3.9",
          "Programming Language :: Python :: 3.10",
          "Programming Language :: Python :: 3.11",
          "Programming Language :: Python :: 3.12",
          "Programming Language :: Python :: 3.13",
          "Programming Language :: Python :: 3.14",
          "Topic :: Text Processing",
          "Topic :: Text Processing :: General"
        ],
        "description": "jaconv\n==========\n|pyversion| |version| |license| |download| |nowar| |nonuke|\n\njaconv (Japanese Converter) is interconverter for Hiragana, Katakana, Hankaku (half-width character) and Zenkaku (full-width character)\n\n`Japanese README <https://github.com/ikegami-yukino/jaconv/blob/master/README_JP.rst>`_ is available.\n\nINSTALLATION\n==============\n\n::\n\n $ pip install jaconv\n\n\nUSAGE\n============\n\nSee also `document <http://ikegami-yukino.github.io/jaconv/jaconv.html>`_\n\n.. code:: python\n\n  import jaconv\n\n  # Hiragana to Katakana\n  jaconv.hira2kata('„Å®„ÇÇ„Åà„Åæ„Åø')\n  # => '„Éà„É¢„Ç®„Éû„Éü'\n\n  # Hiragana to half-width Katakana\n  jaconv.hira2hkata('„Å®„ÇÇ„Åà„Åæ„Åø')\n  # => 'ÔæÑÔæìÔΩ¥ÔæèÔæê'\n\n  # Katakana to Hiragana\n  jaconv.kata2hira('Â∑¥„Éû„Éü')\n  # => 'Â∑¥„Åæ„Åø'\n\n  # half-width character to full-width character\n  # default parameters are followings: kana=True, ascii=False, digit=False\n  jaconv.h2z('ÔæÉÔΩ®ÔæõÔΩ•ÔæåÔΩ®ÔæÖÔΩ∞Ôæö')\n  # => '„ÉÜ„Ç£„É≠„Éª„Éï„Ç£„Éä„Éº„É¨'\n\n  # half-width character to full-width character\n  # but only ascii characters\n  jaconv.h2z('abc', kana=False, ascii=True, digit=False)\n  # => 'ÔΩÅÔΩÇÔΩÉ'\n\n  # half-width character to full-width character\n  # but only digit characters\n  jaconv.h2z('123', kana=False, ascii=False, digit=True)\n  # => 'ÔºëÔºíÔºì'\n\n  # half-width character to full-width character\n  # except half-width Katakana\n  jaconv.h2z('ÔΩ±abc123', kana=False, digit=True, ascii=True)\n  # => 'ÔΩ±ÔΩÅÔΩÇÔΩÉÔºëÔºíÔºì'\n\n  # an alias of h2z\n  jaconv.hankaku2zenkaku('ÔæÉÔΩ®ÔæõÔΩ•ÔæåÔΩ®ÔæÖÔΩ∞Ôæöabc123')\n  # => '„ÉÜ„Ç£„É≠„Éª„Éï„Ç£„Éä„Éº„É¨abc123'\n\n  # full-width character to half-width character\n  # default parameters are followings: kana=True, ascii=False, digit=False\n  jaconv.z2h('„ÉÜ„Ç£„É≠„Éª„Éï„Ç£„Éä„Éº„É¨')\n  # => 'ÔæÉÔΩ®Ôæõ„ÉªÔæåÔΩ®ÔæÖÔΩ∞Ôæö'\n\n  # full-width character to half-width character\n  # but only ascii characters\n  jaconv.z2h('ÔΩÅÔΩÇÔΩÉ', kana=False, ascii=True, digit=False)\n  # => 'abc'\n\n  # full-width character to half-width character\n  # but only digit characters\n  jaconv.z2h('ÔºëÔºíÔºì', kana=False, ascii=False, digit=True)\n  # => '123'\n\n  # full-width character to half-width character\n  # except full-width Katakana\n  jaconv.z2h('„Ç¢ÔΩÅÔΩÇÔΩÉÔºëÔºíÔºì', kana=False, digit=True, ascii=True)\n  # => '„Ç¢abc123'\n\n  # an alias of z2h\n  jaconv.zenkaku2hankaku('„ÉÜ„Ç£„É≠„Éª„Éï„Ç£„Éä„Éº„É¨ÔΩÅÔΩÇÔΩÉÔºëÔºíÔºì')\n  # => 'ÔæÉÔΩ®ÔæõÔΩ•ÔæåÔΩ®ÔæÖÔΩ∞ÔæöÔΩÅÔΩÇÔΩÉÔºëÔºíÔºì'\n\n  # normalize\n  jaconv.normalize('„ÉÜ„Ç£„É≠ÔΩ•„Éï„Ç£„Éä„Äú„É¨', 'NFKC')\n  # => '„ÉÜ„Ç£„É≠„Éª„Éï„Ç£„Éä„Éº„É¨'\n\n  # Hiragana to alphabet\n  jaconv.kana2alphabet('„Åò„ÇÉ„Å±„Çì')\n  # => 'japan'\n\n  # Alphabet to Hiragana\n  jaconv.alphabet2kana('japan')\n  # => '„Åò„ÇÉ„Å±„Çì'\n\n  # Katakana to Alphabet\n  jaconv.kata2alphabet('„Ç±„ÉÑ„Ç§')\n  # => 'ketsui'\n\n  # Alphabet to Katakana\n  jaconv.alphabet2kata('namba')\n  # => '„Éä„É≥„Éê'\n\n  # Hiragana to Julius's phoneme format\n  jaconv.hiragana2julius('„Å¶„Çì„Åç„Åô„Åî„Åè„ÅÑ„ÅÑ„ÅÑ„ÅÑ„ÅÑ„ÅÑ')\n  # => 't e N k i s u g o k u i:'\n\n\nNOTE\n============\n\njaconv.normalize method expand unicodedata.normalize for Japanese language processing.\n\n.. code::\n\n    '„Äú' => '„Éº'\n    'ÔΩû' => '„Éº'\n    \"‚Äô\" => \"'\"\n    '‚Äù'=> '\"'\n    '‚Äú' => '``'\n    '‚Äï' => '-'\n    '‚Äê' => '-'\n    'Àó' => '-'\n    '÷ä' => '-'\n    '‚Äê' => '-'\n    '‚Äë' => '-'\n    '‚Äí' => '-'\n    '‚Äì' => '-'\n    '‚ÅÉ' => '-'\n    '‚Åª' => '-'\n    '‚Çã' => '-'\n    '‚àí' => '-'\n    'Ôπ£' => '„Éº'\n    'Ôºç' => '„Éº'\n    '‚Äî' => '„Éº'\n    '‚Äï' => '„Éº'\n    '‚îÅ' => '„Éº'\n    '‚îÄ' => '„Éº'\n\n\n.. |pyversion| image:: https://img.shields.io/pypi/pyversions/jaconv.svg\n    :target: http://pypi.python.org/pypi/jaconv/\n    :alt: Python version\n\n.. |version| image:: https://img.shields.io/pypi/v/jaconv.svg\n    :target: http://pypi.python.org/pypi/jaconv/\n    :alt: latest version\n\n.. |license| image:: https://img.shields.io/pypi/l/jaconv.svg\n    :target: http://pypi.python.org/pypi/jaconv/\n    :alt: license\n\n.. |download| image:: https://static.pepy.tech/personalized-badge/jaconv?period=total&units=international_system&left_color=black&right_color=blue&left_text=Downloads\n    :target: https://pepy.tech/project/jaconv\n    :alt: download\n\n.. |nowar| image:: https://img.shields.io/badge/%F0%9F%A4%9D%20NO%20WAR-FF0000?style=plastic\n    :alt: NO WAR budge\n\n.. |nonuke| image:: https://img.shields.io/badge/%E2%98%A2%20NO%20NUKE-FFFF00?style=plastic\n    :alt: NO NUKE budge\n\n\nCHANGES\n=======\n\n0.4.1 (2025-11-30)\n-------------------\n- port static configs to setup.cfg (thanks @eli-schwartz)\n- migrate testsuite to pytest (thanks @eli-schwartz)\n- Support Python 3.13 and 3.14\n\n0.4.0 (2024-7-26)\n-------------------\n- Support Python 3.12\n- Add stub files according to PEP 561 for mypy (thanks @ernix)\n\n0.3.4 (2023-2-18)\n-------------------\n- Fix to support Python2.7 ~ 3.4 (thanks @manjuu-eater)\n- Support Python 3.11\n\n0.3.3 (2022-12-31)\n-------------------\n- Support Python 3.10\n- Re-support Python2.7 ~ 3.4 (thanks @manjuu-eater)\n- Fix z2h, h2z all flag off bug (thanks @manjuu-eater)\n\n0.3.1 (2022-12-14)\n-------------------\n- Fix alpha2kana infinite loop bug (thanks @frog42)\n\n0.3 (2021-03-29)\n-------------------\n- Fix bug (alphabet2kana) thanks @Cuddlemuffin007\n- Support Python 3.8 and 3.9\n- Add handy functions: alphabet2kata and kata2alphabet. thanks @kokimame\n- Add function for julius: hiragana2julius\n\n0.2.4 (2018-02-04)\n-------------------\n- Fix bug (kana2alphabet)\n- Support Python 3.7\n- No longer support Python 2.6\n- Add aliases of z2h -> zenkaku2hankaku and h2z -> hankaku2zenkaku\n\n0.2.3 (2018-02-03)\n-------------------\n- Fix bugs (alphabet2kana, kana2alphabet) thanks @letuananh\n\n0.2.2 (2018-01-22)\n-------------------\n- Fix bug (kana2alphabet) thanks @kokimame\n- Support Python 3.6\n\n0.2.1 (2017-09-14)\n-------------------\n- Fix bugs (alphabet2kana, kana2alphabet)\n\n0.2 (2015-04-02)\n------------------\n\n- Change module name jctconv -> jaconv\n- Add alphabet and hiragana interconvert (alphabet2kana, kana2alphabet)\n\n0.1.1 (2015-03-12)\n------------------\n\n- Support Windows\n- Support Python 3.5\n\n\n0.1 (2014-11-24)\n------------------\n\n- Add some Japanese characters to convert table („Çù„Çû„Éª„Äå„Äç„ÄÇ„ÄÅ)\n- Decresing memory usage\n- Some function names are deprecated (hankaku2zenkaku, zenkaku2hankaku, H2K, H2hK, K2H)\n\n\n0.0.7 (2014-03-22)\n------------------\n\nz2h and h2z allow mojimoji-like target character type determination.\nBug fix about Half Kana conversion.\n\n"
      }
    },
    {
      "download_info": {
        "url": "https://pypi.tuna.tsinghua.edu.cn/packages/ac/cc/49812faae67f9a24be6ddaf58a2cf7e8c3cbfcf5b762d9414f7103d2ea2c/jamo-0.4.1-py3-none-any.whl",
        "archive_info": {
          "hash": "sha256=d4b94fd23324c606ed2fbc4037c603e2c3a7ae9390c05d3473aea1ccb6b1c3fb",
          "hashes": {
            "sha256": "d4b94fd23324c606ed2fbc4037c603e2c3a7ae9390c05d3473aea1ccb6b1c3fb"
          }
        }
      },
      "is_direct": false,
      "is_yanked": false,
      "requested": false,
      "metadata": {
        "metadata_version": "2.0",
        "name": "jamo",
        "version": "0.4.1",
        "platform": [
          "UNKNOWN"
        ],
        "summary": "A Hangul syllable and jamo analyzer.",
        "description_content_type": "UNKNOWN",
        "keywords": [
          "Korean",
          "Hangul",
          "jamo",
          "syllable",
          "nlp"
        ],
        "home_page": "https://github.com/jdongian/python-jamo",
        "author": "Joshua Dong",
        "author_email": "jdong42@gmail.com",
        "license": "http://www.apache.org/licenses/LICENSE-2.0",
        "classifier": [
          "License :: OSI Approved :: Apache Software License",
          "Programming Language :: Python :: 3",
          "Programming Language :: Python :: 3.2",
          "Programming Language :: Python :: 3.3",
          "Programming Language :: Python :: 3.4"
        ],
        "description": "Jamo: Hangul Character Analysis\n===============================\n\n.. image:: https://travis-ci.org/JDongian/python-jamo.svg?branch=master\n    :target: https://travis-ci.org/JDongian/python-jamo\n\n.. image:: https://readthedocs.org/projects/python-jamo/badge/?version=latest\n    :target: https://readthedocs.org/projects/python-jamo/?badge=latest\n\nPython-jamo is a Python Hangul syllable decomposition and synthesis library\nfor working with Hangul characters and jamo.\n\nCurrently in beta release, function names are subject to change, but there is\ncoverage for nearly all Hangul-related codepoints under Unicode 7.0.\n\nOriginally designed to help students identify difficult-to-spell words\ncontaining („Öî,„Öê) or („Öó,„Öú), this project hopes to fill the niche of Korean\nphonetic and spelling analysis.\n\n\nInstallation\n------------\n\nTo install Jamo from `pypi`_, simply:\n\n.. code-block:: bash\n\n   $ pip install jamo\n\nThe jamo module is Python 3 only. Viva the bleeding edge!\n\n\nDocumentation\n-------------\n\nDocumentation is available at ReadTheDocs in `English`_.\n\n\nContributing\n------------\n\nLike this project or want to help? Take a look at the issues! I'm active on\ngithub, and will review pulls. I'm open to email as well, so please contact\nme if you have any ideas for this project.\n\n\nLicense\n-------\n\nApache 2.0 licensed.\n\nAnyone is free to use the software for any purpose, to distribute it, to\nmodify it, and to distribute modified versions of the software, under the\nterms of the license, without concern for royalties.\n\n\n.. _pypi: https://pypi.python.org/pypi/jamo\n.. _English: http://python-jamo.readthedocs.org/en/latest/\n.. _Korean: http://python-jamo.readthedocs.org/ko/latest/\n\n\n"
      }
    },
    {
      "download_info": {
        "url": "https://pypi.tuna.tsinghua.edu.cn/packages/c6/cb/18eeb235f833b726522d7ebed54f2278ce28ba9438e3135ab0278d9792a2/jieba-0.42.1.tar.gz",
        "archive_info": {
          "hash": "sha256=055ca12f62674fafed09427f176506079bc135638a14e23e25be909131928db2",
          "hashes": {
            "sha256": "055ca12f62674fafed09427f176506079bc135638a14e23e25be909131928db2"
          }
        }
      },
      "is_direct": false,
      "is_yanked": false,
      "requested": false,
      "metadata": {
        "metadata_version": "2.4",
        "name": "jieba",
        "version": "0.42.1",
        "dynamic": [
          "author",
          "author-email",
          "classifier",
          "description",
          "home-page",
          "keywords",
          "license",
          "summary"
        ],
        "summary": "Chinese Words Segmentation Utilities",
        "keywords": [
          "NLP",
          "tokenizing",
          "Chinese word segementation"
        ],
        "home_page": "https://github.com/fxsjy/jieba",
        "author": "Sun, Junyi",
        "author_email": "ccnusjy@gmail.com",
        "license": "MIT",
        "classifier": [
          "Intended Audience :: Developers",
          "License :: OSI Approved :: MIT License",
          "Operating System :: OS Independent",
          "Natural Language :: Chinese (Simplified)",
          "Natural Language :: Chinese (Traditional)",
          "Programming Language :: Python",
          "Programming Language :: Python :: 2",
          "Programming Language :: Python :: 2.6",
          "Programming Language :: Python :: 2.7",
          "Programming Language :: Python :: 3",
          "Programming Language :: Python :: 3.2",
          "Programming Language :: Python :: 3.3",
          "Programming Language :: Python :: 3.4",
          "Topic :: Text Processing",
          "Topic :: Text Processing :: Indexing",
          "Topic :: Text Processing :: Linguistic"
        ],
        "description": "\njieba\n=====\n\n‚ÄúÁªìÂ∑¥‚Äù‰∏≠ÊñáÂàÜËØçÔºöÂÅöÊúÄÂ•ΩÁöÑ Python ‰∏≠ÊñáÂàÜËØçÁªÑ‰ª∂\n\n\"Jieba\" (Chinese for \"to stutter\") Chinese text segmentation: built to\nbe the best Python Chinese word segmentation module.\n\nÂÆåÊï¥ÊñáÊ°£ËßÅ ``README.md``\n\nGitHub: https://github.com/fxsjy/jieba\n\nÁâπÁÇπ\n====\n\n-  ÊîØÊåÅ‰∏âÁßçÂàÜËØçÊ®°ÂºèÔºö\n\n   -  Á≤æÁ°ÆÊ®°ÂºèÔºåËØïÂõæÂ∞ÜÂè•Â≠êÊúÄÁ≤æÁ°ÆÂú∞ÂàáÂºÄÔºåÈÄÇÂêàÊñáÊú¨ÂàÜÊûêÔºõ\n   -  ÂÖ®Ê®°ÂºèÔºåÊääÂè•Â≠ê‰∏≠ÊâÄÊúâÁöÑÂèØ‰ª•ÊàêËØçÁöÑËØçËØ≠ÈÉΩÊâ´ÊèèÂá∫Êù•,\n      ÈÄüÂ∫¶ÈùûÂ∏∏Âø´Ôºå‰ΩÜÊòØ‰∏çËÉΩËß£ÂÜ≥Ê≠ß‰πâÔºõ\n   -  ÊêúÁ¥¢ÂºïÊìéÊ®°ÂºèÔºåÂú®Á≤æÁ°ÆÊ®°ÂºèÁöÑÂü∫Á°Ä‰∏äÔºåÂØπÈïøËØçÂÜçÊ¨°ÂàáÂàÜÔºåÊèêÈ´òÂè¨ÂõûÁéáÔºåÈÄÇÂêàÁî®‰∫éÊêúÁ¥¢ÂºïÊìéÂàÜËØç„ÄÇ\n\n-  ÊîØÊåÅÁπÅ‰ΩìÂàÜËØç\n-  ÊîØÊåÅËá™ÂÆö‰πâËØçÂÖ∏\n-  MIT ÊéàÊùÉÂçèËÆÆ\n\nÂú®Á∫øÊºîÁ§∫Ôºö http://jiebademo.ap01.aws.af.cm/\n\nÂÆâË£ÖËØ¥Êòé\n========\n\n‰ª£Á†ÅÂØπ Python 2/3 ÂùáÂÖºÂÆπ\n\n-  ÂÖ®Ëá™Âä®ÂÆâË£ÖÔºö ``easy_install jieba`` ÊàñËÄÖ ``pip install jieba`` / ``pip3 install jieba``\n-  ÂçäËá™Âä®ÂÆâË£ÖÔºöÂÖà‰∏ãËΩΩ https://pypi.python.org/pypi/jieba/ ÔºåËß£ÂéãÂêéËøêË°å\n   python setup.py install\n-  ÊâãÂä®ÂÆâË£ÖÔºöÂ∞Ü jieba ÁõÆÂΩïÊîæÁΩÆ‰∫éÂΩìÂâçÁõÆÂΩïÊàñËÄÖ site-packages ÁõÆÂΩï\n-  ÈÄöËøá ``import jieba`` Êù•ÂºïÁî®\n\n"
      }
    },
    {
      "download_info": {
        "url": "https://pypi.tuna.tsinghua.edu.cn/packages/b5/ba/c63c5786dfee4c3417094c4b00966e61e4a63efecee22cb7b4c0387dda83/librosa-0.11.0-py3-none-any.whl",
        "archive_info": {
          "hash": "sha256=0b6415c4fd68bff4c29288abe67c6d80b587e0e1e2cfb0aad23e4559504a7fa1",
          "hashes": {
            "sha256": "0b6415c4fd68bff4c29288abe67c6d80b587e0e1e2cfb0aad23e4559504a7fa1"
          }
        }
      },
      "is_direct": false,
      "is_yanked": false,
      "requested": false,
      "metadata": {
        "metadata_version": "2.2",
        "name": "librosa",
        "version": "0.11.0",
        "summary": "Python module for audio and music processing",
        "description_content_type": "text/markdown; charset=UTF-8",
        "home_page": "https://librosa.org",
        "author": "Brian McFee, librosa development team",
        "author_email": "brian.mcfee@nyu.edu",
        "license": "ISC",
        "license_file": [
          "LICENSE.md"
        ],
        "classifier": [
          "License :: OSI Approved :: ISC License (ISCL)",
          "Programming Language :: Python",
          "Development Status :: 3 - Alpha",
          "Intended Audience :: Developers",
          "Topic :: Multimedia :: Sound/Audio :: Analysis",
          "Framework :: Matplotlib",
          "Programming Language :: Python :: 3",
          "Programming Language :: Python :: 3.8",
          "Programming Language :: Python :: 3.9",
          "Programming Language :: Python :: 3.10",
          "Programming Language :: Python :: 3.11",
          "Programming Language :: Python :: 3.12",
          "Programming Language :: Python :: 3.13"
        ],
        "requires_dist": [
          "audioread>=2.1.9",
          "numba>=0.51.0",
          "numpy>=1.22.3",
          "scipy>=1.6.0",
          "scikit-learn>=1.1.0",
          "joblib>=1.0",
          "decorator>=4.3.0",
          "soundfile>=0.12.1",
          "pooch>=1.1",
          "soxr>=0.3.2",
          "typing_extensions>=4.1.1",
          "lazy_loader>=0.1",
          "msgpack>=1.0",
          "standard-aifc; python_version >= \"3.13\"",
          "standard-sunau; python_version >= \"3.13\"",
          "numpydoc; extra == \"docs\"",
          "sphinx!=1.3.1; extra == \"docs\"",
          "sphinx_rtd_theme>=1.2.0; extra == \"docs\"",
          "numba>=0.51; extra == \"docs\"",
          "matplotlib>=3.5.0; extra == \"docs\"",
          "sphinx-multiversion>=0.2.3; extra == \"docs\"",
          "sphinx-gallery>=0.7; extra == \"docs\"",
          "mir_eval>=0.5; extra == \"docs\"",
          "ipython>=7.0; extra == \"docs\"",
          "sphinxcontrib-svg2pdfconverter; extra == \"docs\"",
          "sphinxcontrib-googleanalytics>=0.4; extra == \"docs\"",
          "presets; extra == \"docs\"",
          "sphinx-copybutton>=0.5.2; extra == \"docs\"",
          "matplotlib>=3.5.0; extra == \"tests\"",
          "packaging>=20.0; extra == \"tests\"",
          "pytest-mpl; extra == \"tests\"",
          "pytest-cov; extra == \"tests\"",
          "pytest; extra == \"tests\"",
          "samplerate; extra == \"tests\"",
          "resampy>=0.2.2; extra == \"tests\"",
          "types-decorator; extra == \"tests\"",
          "matplotlib>=3.5.0; extra == \"display\""
        ],
        "requires_python": ">=3.8",
        "project_url": [
          "Documentation, https://librosa.org/doc",
          "Download, https://github.com/librosa/librosa/releases",
          "Source, https://github.com/librosa/librosa",
          "Tracker, https://github.com/librosa/librosa/issues",
          "Discussion forum, https://groups.google.com/g/librosa"
        ],
        "provides_extra": [
          "docs",
          "tests",
          "display"
        ],
        "description": "[![librosa logo](docs/img/librosa_logo_text.svg)](https://librosa.org/)\n\n# librosa\n\n\nA python package for music and audio analysis.  \n\n[![PyPI](https://img.shields.io/pypi/v/librosa.svg)](https://pypi.python.org/pypi/librosa)\n[![Anaconda-Server Badge](https://anaconda.org/conda-forge/librosa/badges/version.svg)](https://anaconda.org/conda-forge/librosa)\n[![License](https://img.shields.io/pypi/l/librosa.svg)](https://github.com/librosa/librosa/blob/main/LICENSE.md)\n[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.591533.svg)](https://doi.org/10.5281/zenodo.591533)\n\n[![CI](https://github.com/librosa/librosa/actions/workflows/ci.yml/badge.svg)](https://github.com/librosa/librosa/actions/workflows/ci.yml)\n[![codecov](https://codecov.io/gh/librosa/librosa/branch/main/graph/badge.svg?token=ULWnUHaIJC)](https://codecov.io/gh/librosa/librosa)\n[![Docs](https://github.com/librosa/librosa/actions/workflows/docs.yml/badge.svg)](https://librosa.org/doc/latest/index.html)\n\n#  Table of Contents\n\n- [Documentation](#Documentation)\n- [Installation](#Installation)\n  - [Using PyPI](#using-pypi)\n  - [Using Anaconda](#using-anaconda)\n  - [Building From Source](#building-from-source)\n  - [Hints for Installation](#hints-for-the-installation)\n    - [`soundfile`](#soundfile)\n    - [`audioread`](#audioread-and-mp3-support)\n      - [Linux (`apt get`)](#linux-apt-get)\n      - [Linux (`yum`)](#linux-yum)\n      - [Mac](#mac)\n      - [Windows](#windows)\n- [Discussion](#discussion)\n- [Citing](#citing)\n\n---\n\n## Documentation\n\n\nSee https://librosa.org/doc/ for a complete reference manual and introductory tutorials.\n\nThe [advanced example gallery](https://librosa.org/doc/latest/advanced.html) should give you a quick sense of the kinds\nof things that librosa can do.\n\n---\n\n[Back To Top ‚Ü•](#librosa)\n\n\n## Installation\n\n\n### Using PyPI\n\nThe latest stable release is available on PyPI, and you can install it by saying\n```\npython -m pip install librosa\n```\n\n### Using Anaconda\n\nAnaconda users can install using ```conda-forge```:\n```\nconda install -c conda-forge librosa\n```\n\n### Building from source\n\nTo build librosa from source, say \n```\npython setup.py build\n```\nThen, to install librosa, say \n```\npython setup.py install\n```\nIf all went well, you should be able to execute the following commands from a python console:\n```\nimport librosa\nlibrosa.show_versions()\n```\nThis should print out a description of your software environment, along with the installed versions of other packages used by librosa.\n\nüìù OS X users should follow the installation guide given below.\n\nAlternatively, you can download or clone the repository and use `pip` to handle dependencies:\n\n```\nunzip librosa.zip\npython -m pip install -e librosa\n```\nor\n\n```\ngit clone https://github.com/librosa/librosa.git\npython -m pip install -e librosa\n```\n\nBy calling `pip list` you should see `librosa` now as an installed package:\n```\nlibrosa (0.x.x, /path/to/librosa)\n```\n\n---\n\n[Back To Top ‚Ü•](#librosa)\n\n### Hints for the Installation\n\n`librosa` uses `soundfile` and `audioread` to load audio files.\n\nüìù Note that older releases of `soundfile` (prior to 0.11) do not support MP3, which will cause librosa to fall back on the `audioread` library.\n\n### `soundfile`\n\nIf you're using `conda` to install librosa, then audio encoding dependencies will be handled automatically.\n\nIf you're using `pip` on a Linux environment, you may need to install `libsndfile`\nmanually.  Please refer to the [SoundFile installation documentation](https://python-soundfile.readthedocs.io/#installation) for details.\n\n### `audioread` and MP3 support\n\nTo fuel `audioread` with more audio-decoding power (e.g., for reading MP3 files),\nyou may need to install either *ffmpeg* or *GStreamer*.\n\nüìù*Note that on some platforms, `audioread` needs at least one of the programs to work properly.*\n\nIf you are using Anaconda, install *ffmpeg* by calling\n\n```\nconda install -c conda-forge ffmpeg\n```\n\nIf you are not using Anaconda, here are some common commands for different operating systems:\n\n- ####  Linux (`apt-get`): \n\n```\napt-get install ffmpeg\n```\nor\n \n```\napt-get install gstreamer1.0-plugins-base gstreamer1.0-plugins-ugly\n```\n- #### Linux (`yum`):\n```\nyum install ffmpeg\n```\nor\n\n\n```\nyum install gstreamer1.0-plugins-base gstreamer1.0-plugins-ugly\n```\n\n- #### Mac: \n```\nbrew install ffmpeg\n```\nor\n\n```\nbrew install gstreamer\n```\n\n- #### Windows: \n\ndownload ffmpeg binaries from this [website](https://www.gyan.dev/ffmpeg/builds/) or gstreamer binaries from this [website](https://gstreamer.freedesktop.org/)\n\nFor GStreamer, you also need to install the Python bindings with \n\n```\npython -m pip install pygobject\n```\n\n---\n\n[Back To Top ‚Ü•](#librosa)\n\n## Discussion\n\n\nPlease direct non-development questions and discussion topics to our web forum at\nhttps://groups.google.com/forum/#!forum/librosa\n\n---\n\n[Back To Top ‚Ü•](#librosa)\n\n## Citing\n\n\nIf you want to cite librosa in a scholarly work, there are two ways to do it.\n\n- If you are using the library for your work, for the sake of reproducibility, please cite\n  the version you used as indexed at Zenodo:\n\n    [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.591533.svg)](https://doi.org/10.5281/zenodo.591533)\n\n  From librosa version 0.10.2 or later, you can also use `librosa.cite()`\n  to get the DOI link for any version of librosa.\n\n- If you wish to cite librosa for its design, motivation, etc., please cite the paper\n  published at SciPy 2015:\n\n    McFee, Brian, Colin Raffel, Dawen Liang, Daniel PW Ellis, Matt McVicar, Eric Battenberg, and Oriol Nieto. \"librosa: Audio and music signal analysis in python.\" In Proceedings of the 14th python in science conference, pp. 18-25. 2015.\n\n---\n\n[Back To Top ‚Ü•](#librosa)\n"
      }
    },
    {
      "download_info": {
        "url": "https://pypi.tuna.tsinghua.edu.cn/packages/7e/16/fbe8e1e185a45042f7cd3a282def5bb8d95bb69ab9e9ef6a5368aa17e426/audioread-3.1.0-py3-none-any.whl",
        "archive_info": {
          "hash": "sha256=b30d1df6c5d3de5dcef0fb0e256f6ea17bdcf5f979408df0297d8a408e2971b4",
          "hashes": {
            "sha256": "b30d1df6c5d3de5dcef0fb0e256f6ea17bdcf5f979408df0297d8a408e2971b4"
          }
        }
      },
      "is_direct": false,
      "is_yanked": false,
      "requested": false,
      "metadata": {
        "metadata_version": "2.4",
        "name": "audioread",
        "version": "3.1.0",
        "summary": "Multi-library, cross-platform audio decoding.",
        "description_content_type": "text/x-rst",
        "author": "Adrian Sampson",
        "author_email": "adrian@radbox.org",
        "license_expression": "MIT",
        "license_file": [
          "LICENSE"
        ],
        "classifier": [
          "Topic :: Multimedia :: Sound/Audio :: Conversion",
          "Intended Audience :: Developers",
          "Programming Language :: Python :: 3",
          "Programming Language :: Python :: 3.9",
          "Programming Language :: Python :: 3.10",
          "Programming Language :: Python :: 3.11",
          "License :: OSI Approved :: MIT License"
        ],
        "requires_dist": [
          "pygobject (>=3.54.2,<4.0.0) ; extra == \"gi\"",
          "pymad[mad] (>=0.11.3,<0.12.0) ; extra == \"mad\"",
          "pytest (>=8.4.2) ; extra == \"test\"",
          "pytest-cov (>=7.0.0) ; extra == \"test\"",
          "standard-aifc ; python_version >= \"3.13\"",
          "standard-sunau ; python_version >= \"3.13\""
        ],
        "requires_python": ">=3.9",
        "project_url": [
          "Bug Tracker, https://github.com/beetbox/audioread/issues",
          "Homepage, https://github.com/beetbox/audioread",
          "Repository, https://github.com/beetbox/audioread"
        ],
        "provides_extra": [
          "gi",
          "mad",
          "test"
        ],
        "description": "audioread\n=========\n\nDecode audio files using whichever backend is available. The library\ncurrently supports:\n\n- `Gstreamer`_ via `PyGObject`_.\n- `Core Audio`_ on Mac OS X via `ctypes`_. (PyObjC not required.)\n- `MAD`_ via the `pymad`_ bindings.\n- `FFmpeg`_ or `Libav`_ via its command-line interface.\n- The standard library `wave`_, `aifc`_, and `sunau`_ modules (for\n  uncompressed audio formats).\n\n.. _Gstreamer: http://gstreamer.freedesktop.org/\n.. _gst-python: http://gstreamer.freedesktop.org/modules/gst-python.html\n.. _Core Audio: http://developer.apple.com/technologies/mac/audio-and-video.html\n.. _ctypes: http://docs.python.org/library/ctypes.html\n.. _MAD: http://www.underbit.com/products/mad/\n.. _pymad: http://spacepants.org/src/pymad/\n.. _FFmpeg: http://ffmpeg.org/\n.. _Libav: https://www.libav.org/\n.. _wave: http://docs.python.org/library/wave.html\n.. _aifc: http://docs.python.org/library/aifc.html\n.. _sunau: http://docs.python.org/library/sunau.html\n.. _PyGObject: https://pygobject.readthedocs.io/\n\nUse the library like so::\n\n    with audioread.audio_open(filename) as f:\n        print(f.channels, f.samplerate, f.duration)\n        for buf in f:\n            do_something(buf)\n\nBuffers in the file can be accessed by iterating over the object returned from\n``audio_open``. Each buffer is a bytes-like object (``buffer``, ``bytes``, or\n``bytearray``) containing raw **16-bit little-endian signed integer PCM\ndata**. (Currently, these PCM format parameters are not configurable, but this\ncould be added to most of the backends.)\n\nAdditional values are available as fields on the audio file object:\n\n- ``channels`` is the number of audio channels (an integer).\n- ``samplerate`` is given in Hz (an integer).\n- ``duration`` is the length of the audio in seconds (a float).\n\nThe ``audio_open`` function transparently selects a backend that can read the\nfile. (Each backend is implemented in a module inside the ``audioread``\npackage.) If no backends succeed in opening the file, a ``DecodeError``\nexception is raised. This exception is only used when the file type is\nunsupported by the backends; if the file doesn't exist, a standard ``IOError``\nwill be raised.\n\nA second optional parameter to ``audio_open`` specifies which backends to try\n(instead of trying them all, which is the default). You can use the\n``available_backends`` function to get a list backends that are usable on the\ncurrent system.\n\nAudioread supports Python 3 (3.9+).\n\nExample\n-------\n\nThe included ``decode.py`` script demonstrates using this package to\nconvert compressed audio files to WAV files.\n\nTroubleshooting\n---------------\n\nA ``NoBackendError`` exception means that the library could not find one of\nthe libraries or tools it needs to decode audio. This could mean, for example,\nthat you have a broken installation of `FFmpeg`_. To check, try typing\n``ffmpeg -version`` in your shell. If that gives you an error, try installing\nFFmpeg with your OS's package manager (e.g., apt or yum) or `using Conda\n<https://anaconda.org/conda-forge/ffmpeg>`_.\n\nVersion History\n---------------\n\n3.0.2\n  Support path-like objects (not just strings) in the Core Audio backend.\n\n3.0.1\n  Fix a possible deadlock when FFmpeg's version output produces too much data.\n\n3.0.0\n  Drop support for Python 2 and older versions of Python 3. The library now\n  requires Python 3.6+.\n  Increase default block size in FFmpegAudioFile to get slightly faster file reading.\n  Cache backends for faster lookup (thanks to @bmcfee).\n  Audio file classes now inherit from a common base ``AudioFile`` class.\n\n2.1.9\n  Work correctly with GStreamer 1.18 and later (thanks to @ssssam).\n\n2.1.8\n  Fix an unhandled ``OSError`` when FFmpeg is not installed.\n\n2.1.7\n  Properly close some filehandles in the FFmpeg backend (thanks to\n  @RyanMarcus and @ssssam).\n  The maddec backend now always produces bytes objects, like the other\n  backends (thanks to @ssssam).\n  Resolve an audio data memory leak in the GStreamer backend (thanks again to\n  @ssssam).\n  You can now optionally specify which specific backends ``audio_open`` should\n  try (thanks once again to @ssssam).\n  On Windows, avoid opening a console window to run FFmpeg (thanks to @flokX).\n\n2.1.6\n  Fix a \"no such process\" crash in the FFmpeg backend on Windows Subsystem for\n  Linux (thanks to @llamasoft).\n  Avoid suppressing SIGINT in the GStreamer backend on older versions of\n  PyGObject (thanks to @lazka).\n\n2.1.5\n  Properly clean up the file handle when a backend fails to decode a file.\n  Fix parsing of \"N.M\" channel counts in the FFmpeg backend (thanks to @piem).\n  Avoid a crash in the raw backend when a file uses an unsupported number of\n  bits per sample (namely, 24-bit samples in Python < 3.4).\n  Add a ``__version__`` value to the package.\n\n2.1.4\n  Fix a bug in the FFmpeg backend where, after closing a file, the program's\n  standard input stream would be \"broken\" and wouldn't receive any input.\n\n2.1.3\n  Avoid some warnings in the GStreamer backend when using modern versions of\n  GLib. We now require at least GLib 2.32.\n\n2.1.2\n  Fix a file descriptor leak when opening and closing many files using\n  GStreamer.\n\n2.1.1\n  Just fix ReST formatting in the README.\n\n2.1.0\n  The FFmpeg backend can now also use Libav's ``avconv`` command.\n  Fix a warning by requiring GStreamer >= 1.0.\n  Fix some Python 3 crashes with the new GStreamer backend (thanks to\n  @xix-xeaon).\n\n2.0.0\n  The GStreamer backend now uses GStreamer 1.x via the new\n  gobject-introspection API (and is compatible with Python 3).\n\n1.2.2\n  When running FFmpeg on Windows, disable its crash dialog. Thanks to\n  jcsaaddupuy.\n\n1.2.1\n  Fix an unhandled exception when opening non-raw audio files (thanks to\n  aostanin).\n  Fix Python 3 compatibility for the raw-file backend.\n\n1.2.0\n  Add support for FFmpeg on Windows (thanks to Jean-Christophe Saad-Dupuy).\n\n1.1.0\n  Add support for Sun/NeXT `Au files`_ via the standard-library ``sunau``\n  module (thanks to Dan Ellis).\n\n1.0.3\n  Use the rawread (standard-library) backend for .wav files.\n\n1.0.2\n  Send SIGKILL, not SIGTERM, to ffmpeg processes to avoid occasional hangs.\n\n1.0.1\n  When GStreamer fails to report a duration, raise an exception instead of\n  silently setting the duration field to None.\n\n1.0.0\n  Catch GStreamer's exception when necessary components, such as\n  ``uridecodebin``, are missing.\n  The GStreamer backend now accepts relative paths.\n  Fix a hang in GStreamer when the stream finishes before it begins (when\n  reading broken files).\n  Initial support for Python 3.\n\n0.8\n  All decoding errors are now subclasses of ``DecodeError``.\n\n0.7\n  Fix opening WAV and AIFF files via Unicode filenames.\n\n0.6\n  Make FFmpeg timeout more robust.\n  Dump FFmpeg output on timeout.\n  Fix a nondeterministic hang in the Gstreamer backend.\n  Fix a file descriptor leak in the MAD backend.\n\n0.5\n  Fix crash when FFmpeg fails to report a duration.\n  Fix a hang when FFmpeg fills up its stderr output buffer.\n  Add a timeout to ``ffmpeg`` tool execution (currently 10 seconds for each\n  4096-byte read); a ``ReadTimeoutError`` exception is raised if the tool times\n  out.\n\n0.4\n  Fix channel count detection for FFmpeg backend.\n\n0.3\n  Fix a problem with the Gstreamer backend where audio files could be left open\n  even after the ``GstAudioFile`` was \"closed\".\n\n0.2\n  Fix a hang in the GStreamer backend that occurs occasionally on some\n  platforms.\n\n0.1\n  Initial release.\n\n.. _Au files: http://en.wikipedia.org/wiki/Au_file_format\n\nEt Cetera\n---------\n\n``audioread`` is by Adrian Sampson. It is made available under `the MIT\nlicense`_. An alternative to this module is `decoder.py`_.\n\n.. _the MIT license: http://www.opensource.org/licenses/mit-license.php\n.. _decoder.py: http://www.brailleweb.com/cgi-bin/python.py\n\n"
      }
    },
    {
      "download_info": {
        "url": "https://pypi.tuna.tsinghua.edu.cn/packages/4e/8c/f3147f5c4b73e7550fe5f9352eaa956ae838d5c51eb58e7a25b9f3e2643b/decorator-5.2.1-py3-none-any.whl",
        "archive_info": {
          "hash": "sha256=d316bb415a2d9e2d2b3abcc4084c6502fc09240e292cd76a76afc106a1c8e04a",
          "hashes": {
            "sha256": "d316bb415a2d9e2d2b3abcc4084c6502fc09240e292cd76a76afc106a1c8e04a"
          }
        }
      },
      "is_direct": false,
      "is_yanked": false,
      "requested": false,
      "metadata": {
        "metadata_version": "2.2",
        "name": "decorator",
        "version": "5.2.1",
        "summary": "Decorators for Humans",
        "description_content_type": "text/x-rst",
        "keywords": [
          "decorators"
        ],
        "author_email": "Michele Simionato <michele.simionato@gmail.com>",
        "license": "BSD-2-Clause",
        "license_file": [
          "LICENSE.txt"
        ],
        "classifier": [
          "Development Status :: 5 - Production/Stable",
          "Intended Audience :: Developers",
          "License :: OSI Approved :: BSD License",
          "Natural Language :: English",
          "Operating System :: OS Independent",
          "Programming Language :: Python",
          "Programming Language :: Python :: 3.8",
          "Programming Language :: Python :: 3.9",
          "Programming Language :: Python :: 3.10",
          "Programming Language :: Python :: 3.11",
          "Programming Language :: Python :: 3.12",
          "Programming Language :: Python :: 3.13",
          "Programming Language :: Python :: Implementation :: CPython",
          "Topic :: Software Development :: Libraries",
          "Topic :: Utilities"
        ],
        "requires_python": ">=3.8",
        "description": "Decorators for Humans\n=====================\n\nThe goal of the decorator module is to make it easy to define\nsignature-preserving function decorators and decorator factories.\nIt also includes an implementation of multiple dispatch and other niceties\n(please check the docs). It is released under a two-clauses\nBSD license, i.e. basically you can do whatever you want with it but I am not\nresponsible.\n\nInstallation\n-------------\n\nIf you are lazy, just perform\n\n ``$ pip install decorator``\n\nwhich will install just the module on your system.\n\nIf you prefer to install the full distribution from source, including\nthe documentation, clone the `GitHub repo`_ or download the tarball_, unpack it and run\n\n ``$ pip install .``\n\nin the main directory, possibly as superuser.\n\n.. _tarball: https://pypi.org/project/decorator/#files\n.. _GitHub repo: https://github.com/micheles/decorator\n\nTesting\n--------\n\nIf you have the source code installation you can run the tests with\n\n `$ python src/tests/test.py -v`\n\nor (if you have setuptools installed)\n\n `$ python setup.py test`\n\nNotice that you may run into trouble if in your system there\nis an older version of the decorator module; in such a case remove the\nold version. It is safe even to copy the module `decorator.py` over\nan existing one, since we kept backward-compatibility for a long time.\n\nRepository\n---------------\n\nThe project is hosted on GitHub. You can look at the source here:\n\n https://github.com/micheles/decorator\n\nDocumentation\n---------------\n\nThe documentation has been moved to https://github.com/micheles/decorator/blob/master/docs/documentation.md\n\nFrom there you can get a PDF version by simply using the print\nfunctionality of your browser.\n\nHere is the documentation for previous versions of the module:\n\nhttps://github.com/micheles/decorator/blob/4.3.2/docs/tests.documentation.rst\nhttps://github.com/micheles/decorator/blob/4.2.1/docs/tests.documentation.rst\nhttps://github.com/micheles/decorator/blob/4.1.2/docs/tests.documentation.rst\nhttps://github.com/micheles/decorator/blob/4.0.0/documentation.rst\nhttps://github.com/micheles/decorator/blob/3.4.2/documentation.rst\n\nFor the impatient\n-----------------\n\nHere is an example of how to define a family of decorators tracing slow\noperations:\n\n.. code-block:: python\n\n   from decorator import decorator\n\n   @decorator\n   def warn_slow(func, timelimit=60, *args, **kw):\n       t0 = time.time()\n       result = func(*args, **kw)\n       dt = time.time() - t0\n       if dt > timelimit:\n           logging.warning('%s took %d seconds', func.__name__, dt)\n       else:\n           logging.info('%s took %d seconds', func.__name__, dt)\n       return result\n\n   @warn_slow  # warn if it takes more than 1 minute\n   def preprocess_input_files(inputdir, tempdir):\n       ...\n\n   @warn_slow(timelimit=600)  # warn if it takes more than 10 minutes\n   def run_calculation(tempdir, outdir):\n       ...\n\nEnjoy!\n"
      }
    },
    {
      "download_info": {
        "url": "https://pypi.tuna.tsinghua.edu.cn/packages/7b/91/984aca2ec129e2757d1e4e3c81c3fcda9d0f85b74670a094cc443d9ee949/joblib-1.5.3-py3-none-any.whl",
        "archive_info": {
          "hash": "sha256=5fc3c5039fc5ca8c0276333a188bbd59d6b7ab37fe6632daa76bc7f9ec18e713",
          "hashes": {
            "sha256": "5fc3c5039fc5ca8c0276333a188bbd59d6b7ab37fe6632daa76bc7f9ec18e713"
          }
        }
      },
      "is_direct": false,
      "is_yanked": false,
      "requested": false,
      "metadata": {
        "metadata_version": "2.4",
        "name": "joblib",
        "version": "1.5.3",
        "dynamic": [
          "license-file"
        ],
        "platform": [
          "any"
        ],
        "summary": "Lightweight pipelining with Python functions",
        "description_content_type": "text/x-rst",
        "author_email": "Gael Varoquaux <gael.varoquaux@normalesup.org>",
        "license_expression": "BSD-3-Clause",
        "license_file": [
          "LICENSE.txt"
        ],
        "classifier": [
          "Development Status :: 5 - Production/Stable",
          "Environment :: Console",
          "Intended Audience :: Developers",
          "Intended Audience :: Science/Research",
          "Intended Audience :: Education",
          "Operating System :: OS Independent",
          "Programming Language :: Python :: 3",
          "Programming Language :: Python :: 3.9",
          "Programming Language :: Python :: 3.10",
          "Programming Language :: Python :: 3.11",
          "Programming Language :: Python :: 3.12",
          "Programming Language :: Python :: 3.13",
          "Topic :: Scientific/Engineering",
          "Topic :: Utilities",
          "Topic :: Software Development :: Libraries"
        ],
        "requires_python": ">=3.9",
        "project_url": [
          "Homepage, https://joblib.readthedocs.io",
          "Source, https://github.com/joblib/joblib"
        ],
        "description": "|PyPi| |CIStatus| |ReadTheDocs| |Codecov|\n\n.. |PyPi| image:: https://badge.fury.io/py/joblib.svg\n   :target: https://badge.fury.io/py/joblib\n   :alt: Joblib version\n\n.. |CIStatus| image:: https://github.com/joblib/joblib/actions/workflows/test.yml/badge.svg\n   :target: https://github.com/joblib/joblib/actions/workflows/test.yml?query=branch%3Amain\n   :alt: CI status\n\n.. |ReadTheDocs| image:: https://readthedocs.org/projects/joblib/badge/?version=latest\n    :target: https://joblib.readthedocs.io/en/latest/?badge=latest\n    :alt: Documentation Status\n\n.. |Codecov| image:: https://codecov.io/gh/joblib/joblib/branch/main/graph/badge.svg\n   :target: https://codecov.io/gh/joblib/joblib\n   :alt: Codecov coverage\n\n\nThe homepage of joblib with user documentation is located on:\n\nhttps://joblib.readthedocs.io\n\nGetting the latest code\n=======================\n\nTo get the latest code using git, simply type::\n\n    git clone https://github.com/joblib/joblib.git\n\nIf you don't have git installed, you can download a zip\nof the latest code: https://github.com/joblib/joblib/archive/refs/heads/main.zip\n\nInstalling\n==========\n\nYou can use `pip` to install joblib from any directory::\n\n    pip install joblib\n\nor install it in editable mode from the source directory::\n\n    pip install -e .\n\nDependencies\n============\n\n- Joblib has no mandatory dependencies besides Python (supported versions are\n  3.9+).\n- Joblib has an optional dependency on Numpy (at least version 1.6.1) for array\n  manipulation.\n- Joblib includes its own vendored copy of\n  `loky <https://github.com/tomMoral/loky>`_ for process management.\n- Joblib can efficiently dump and load numpy arrays but does not require numpy\n  to be installed.\n- Joblib has an optional dependency on\n  `python-lz4 <https://pypi.python.org/pypi/lz4>`_ as a faster alternative to\n  zlib and gzip for compressed serialization.\n- Joblib has an optional dependency on psutil to mitigate memory leaks in\n  parallel worker processes.\n- Some examples require external dependencies such as pandas. See the\n  instructions in the `Building the docs`_ section for details.\n\nWorkflow to contribute\n======================\n\nTo contribute to joblib, first create an account on `github\n<https://github.com/>`_. Once this is done, fork the `joblib repository\n<https://github.com/joblib/joblib>`_ to have your own repository,\nclone it using ``git clone``. Make your changes in a branch of your clone, push\nthem to your github account, test them locally, and when you are happy with\nthem, send a pull request to the main repository.\n\nYou can use `pre-commit <https://pre-commit.com/#install>`_ to run code style checks\nbefore each commit::\n\n    pip install pre-commit\n    pre-commit install\n\npre-commit checks can be disabled for a single commit with::\n\n    git commit -n\n\nRunning the test suite\n======================\n\nTo run the test suite, you need the pytest (version >= 3) and coverage modules.\nRun the test suite using::\n\n    pytest joblib\n\nfrom the root of the project.\n\nBuilding the docs\n=================\n\nTo build the docs you need to have sphinx (>=1.4) and some dependencies\ninstalled::\n\n    pip install -U -r .readthedocs-requirements.txt\n\nThe docs can then be built with the following command::\n\n    make doc\n\nThe html docs are located in the ``doc/_build/html`` directory.\n\n\nMaking a source tarball\n=======================\n\nTo create a source tarball, eg for packaging or distributing, run the\nfollowing command::\n\n    pip install build\n    python -m build --sdist\n\nThe tarball will be created in the `dist` directory. This command will create\nthe resulting tarball that can be installed with no extra dependencies than the\nPython standard library.\n\nMaking a release and uploading it to PyPI\n=========================================\n\nThis command is only run by project manager, to make a release, and\nupload in to PyPI::\n\n    pip install build\n    python -m build --sdist --wheel\n    twine upload dist/*\n\n\nNote that the documentation should automatically get updated at each git\npush. If that is not the case, try building th doc locally and resolve\nany doc build error (in particular when running the examples).\n\nUpdating the changelog\n======================\n\nChanges are listed in the CHANGES.rst file. They must be manually updated\nbut, the following git command may be used to generate the lines::\n\n    git log --abbrev-commit --date=short --no-merges --sparse\n"
      }
    },
    {
      "download_info": {
        "url": "https://pypi.tuna.tsinghua.edu.cn/packages/83/60/d497a310bde3f01cb805196ac61b7ad6dc5dcf8dce66634dc34364b20b4f/lazy_loader-0.4-py3-none-any.whl",
        "archive_info": {
          "hash": "sha256=342aa8e14d543a154047afb4ba8ef17f5563baad3fc610d7b15b213b0f119efc",
          "hashes": {
            "sha256": "342aa8e14d543a154047afb4ba8ef17f5563baad3fc610d7b15b213b0f119efc"
          }
        }
      },
      "is_direct": false,
      "is_yanked": false,
      "requested": false,
      "metadata": {
        "metadata_version": "2.1",
        "name": "lazy_loader",
        "version": "0.4",
        "summary": "Makes it easy to load subpackages and functions on demand.",
        "description_content_type": "text/markdown",
        "author": "Scientific Python Developers",
        "license": "BSD 3-Clause License\n        \n        Copyright (c) 2022--2023, Scientific Python project\n        All rights reserved.\n        \n        Redistribution and use in source and binary forms, with or without\n        modification, are permitted provided that the following conditions are met:\n        \n        1. Redistributions of source code must retain the above copyright notice, this\n           list of conditions and the following disclaimer.\n        \n        2. Redistributions in binary form must reproduce the above copyright notice,\n           this list of conditions and the following disclaimer in the documentation\n           and/or other materials provided with the distribution.\n        \n        3. Neither the name of the copyright holder nor the names of its\n           contributors may be used to endorse or promote products derived from\n           this software without specific prior written permission.\n        \n        THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\n        AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n        IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n        DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\n        FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\n        DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\n        SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\n        CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\n        OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n        OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n        ",
        "license_file": [
          "LICENSE.md"
        ],
        "classifier": [
          "Development Status :: 4 - Beta",
          "License :: OSI Approved :: BSD License",
          "Programming Language :: Python :: 3",
          "Programming Language :: Python :: 3.7",
          "Programming Language :: Python :: 3.8",
          "Programming Language :: Python :: 3.9",
          "Programming Language :: Python :: 3.10",
          "Programming Language :: Python :: 3.11",
          "Programming Language :: Python :: 3.12"
        ],
        "requires_dist": [
          "packaging",
          "importlib-metadata ; python_version < \"3.8\"",
          "changelist ==0.5 ; extra == 'dev'",
          "pre-commit ==3.7.0 ; extra == 'lint'",
          "pytest >=7.4 ; extra == 'test'",
          "pytest-cov >=4.1 ; extra == 'test'"
        ],
        "requires_python": ">=3.7",
        "project_url": [
          "Home, https://scientific-python.org/specs/spec-0001/",
          "Source, https://github.com/scientific-python/lazy_loader"
        ],
        "provides_extra": [
          "dev",
          "lint",
          "test"
        ],
        "description": "[![PyPI](https://img.shields.io/pypi/v/lazy_loader)](https://pypi.org/project/lazy_loader/)\n[![Test status](https://github.com/scientific-python/lazy_loader/workflows/test/badge.svg?branch=main)](https://github.com/scientific-python/lazy_loader/actions?query=workflow%3A%22test%22)\n[![Test coverage](https://codecov.io/gh/scientific-python/lazy_loader/branch/main/graph/badge.svg)](https://app.codecov.io/gh/scientific-python/lazy_loader/branch/main)\n\n`lazy_loader` makes it easy to load subpackages and functions on demand.\n\n## Motivation\n\n1. Allow subpackages to be made visible to users without incurring import costs.\n2. Allow external libraries to be imported only when used, improving import times.\n\nFor a more detailed discussion, see [the SPEC](https://scientific-python.org/specs/spec-0001/).\n\n## Installation\n\n```\npip install -U lazy_loader\n```\n\nWe recommend using `lazy_loader` with Python >= 3.11.\nIf using Python 3.11, please upgrade to 3.11.9 or later.\nIf using Python 3.12, please upgrade to 3.12.3 or later.\nThese versions [avoid](https://github.com/python/cpython/pull/114781) a [known race condition](https://github.com/python/cpython/issues/114763).\n\n## Usage\n\n### Lazily load subpackages\n\nConsider the `__init__.py` from [scikit-image](https://scikit-image.org):\n\n```python\nsubpackages = [\n    ...,\n    'filters',\n    ...\n]\n\nimport lazy_loader as lazy\n__getattr__, __dir__, _ = lazy.attach(__name__, subpackages)\n```\n\nYou can now do:\n\n```python\nimport skimage as ski\nski.filters.gaussian(...)\n```\n\nThe `filters` subpackages will only be loaded once accessed.\n\n### Lazily load subpackages and functions\n\nConsider `skimage/filters/__init__.py`:\n\n```python\nfrom ..util import lazy\n\n__getattr__, __dir__, __all__ = lazy.attach(\n    __name__,\n    submodules=['rank'],\n    submod_attrs={\n        '_gaussian': ['gaussian', 'difference_of_gaussians'],\n        'edges': ['sobel', 'scharr', 'prewitt', 'roberts',\n                  'laplace', 'farid']\n    }\n)\n```\n\nThe above is equivalent to:\n\n```python\nfrom . import rank\nfrom ._gaussian import gaussian, difference_of_gaussians\nfrom .edges import (sobel, scharr, prewitt, roberts,\n                    laplace, farid)\n```\n\nExcept that all subpackages (such as `rank`) and functions (such as `sobel`) are loaded upon access.\n\n### Type checkers\n\nStatic type checkers and IDEs cannot infer type information from\nlazily loaded imports. As a workaround you can load [type\nstubs](https://mypy.readthedocs.io/en/stable/stubs.html) (`.pyi`\nfiles) with `lazy.attach_stub`:\n\n```python\nimport lazy_loader as lazy\n__getattr__, __dir__, _ = lazy.attach_stub(__name__, \"subpackages.pyi\")\n```\n\nNote that, since imports are now defined in `.pyi` files, those\nare not only necessary for type checking but also at runtime.\n\nThe SPEC [describes this workaround in more\ndetail](https://scientific-python.org/specs/spec-0001/#type-checkers).\n\n### Early failure\n\nWith lazy loading, missing imports no longer fail upon loading the\nlibrary. During development and testing, you can set the `EAGER_IMPORT`\nenvironment variable to disable lazy loading.\n\n### External libraries\n\nThe `lazy.attach` function discussed above is used to set up package\ninternal imports.\n\nUse `lazy.load` to lazily import external libraries:\n\n```python\nsp = lazy.load('scipy')  # `sp` will only be loaded when accessed\nsp.linalg.norm(...)\n```\n\n_Note that lazily importing *sub*packages,\ni.e. `load('scipy.linalg')` will cause the package containing the\nsubpackage to be imported immediately; thus, this usage is\ndiscouraged._\n\nYou can ask `lazy.load` to raise import errors as soon as it is called:\n\n```python\nlinalg = lazy.load('scipy.linalg', error_on_import=True)\n```\n\n#### Optional requirements\n\nOne use for lazy loading is for loading optional dependencies, with\n`ImportErrors` only arising when optional functionality is accessed. If optional\nfunctionality depends on a specific version, a version requirement can\nbe set:\n\n```python\nnp = lazy.load(\"numpy\", require=\"numpy >=1.24\")\n```\n\nIn this case, if `numpy` is installed, but the version is less than 1.24,\nthe `np` module returned will raise an error on attribute access. Using\nthis feature is not all-or-nothing: One module may rely on one version of\nnumpy, while another module may not set any requirement.\n\n_Note that the requirement must use the package [distribution name][] instead\nof the module [import name][]. For example, the `pyyaml` distribution provides\nthe `yaml` module for import._\n\n[distribution name]: https://packaging.python.org/en/latest/glossary/#term-Distribution-Package\n[import name]: https://packaging.python.org/en/latest/glossary/#term-Import-Package\n"
      }
    },
    {
      "download_info": {
        "url": "https://pypi.tuna.tsinghua.edu.cn/packages/71/e5/c2241de64bfceac456b140737812a2ab310b10538a7b34a1d393b748e095/msgpack-1.1.2-cp310-cp310-manylinux2014_aarch64.manylinux_2_17_aarch64.manylinux_2_28_aarch64.whl",
        "archive_info": {
          "hash": "sha256=8b696e83c9f1532b4af884045ba7f3aa741a63b2bc22617293a2c6a7c645f251",
          "hashes": {
            "sha256": "8b696e83c9f1532b4af884045ba7f3aa741a63b2bc22617293a2c6a7c645f251"
          }
        }
      },
      "is_direct": false,
      "is_yanked": false,
      "requested": false,
      "metadata": {
        "metadata_version": "2.4",
        "name": "msgpack",
        "version": "1.1.2",
        "dynamic": [
          "license-file"
        ],
        "summary": "MessagePack serializer",
        "description_content_type": "text/markdown",
        "keywords": [
          "msgpack",
          "messagepack",
          "serializer",
          "serialization",
          "binary"
        ],
        "author_email": "Inada Naoki <songofacandy@gmail.com>",
        "license_expression": "Apache-2.0",
        "license_file": [
          "COPYING"
        ],
        "classifier": [
          "Development Status :: 5 - Production/Stable",
          "Operating System :: OS Independent",
          "Topic :: File Formats",
          "Intended Audience :: Developers",
          "Programming Language :: Python :: Implementation :: CPython",
          "Programming Language :: Python :: Implementation :: PyPy"
        ],
        "requires_python": ">=3.9",
        "project_url": [
          "Homepage, https://msgpack.org/",
          "Documentation, https://msgpack-python.readthedocs.io/",
          "Repository, https://github.com/msgpack/msgpack-python/",
          "Tracker, https://github.com/msgpack/msgpack-python/issues",
          "Changelog, https://github.com/msgpack/msgpack-python/blob/main/ChangeLog.rst"
        ],
        "description": "# MessagePack for Python\n\n[![Build Status](https://github.com/msgpack/msgpack-python/actions/workflows/wheel.yml/badge.svg)](https://github.com/msgpack/msgpack-python/actions/workflows/wheel.yml)\n[![Documentation Status](https://readthedocs.org/projects/msgpack-python/badge/?version=latest)](https://msgpack-python.readthedocs.io/en/latest/?badge=latest)\n\n## What is this?\n\n[MessagePack](https://msgpack.org/) is an efficient binary serialization format.\nIt lets you exchange data among multiple languages like JSON.\nBut it's faster and smaller.\nThis package provides CPython bindings for reading and writing MessagePack data.\n\n## Install\n\n```\n$ pip install msgpack\n```\n\n### Pure Python implementation\n\nThe extension module in msgpack (`msgpack._cmsgpack`) does not support PyPy.\n\nBut msgpack provides a pure Python implementation (`msgpack.fallback`) for PyPy.\n\n\n### Windows\n\nIf you can't use a binary distribution, you need to install Visual Studio\nor the Windows SDK on Windows.\nWithout the extension, the pure Python implementation on CPython runs slowly.\n\n\n## How to use\n\n### One-shot pack & unpack\n\nUse `packb` for packing and `unpackb` for unpacking.\nmsgpack provides `dumps` and `loads` as aliases for compatibility with\n`json` and `pickle`.\n\n`pack` and `dump` pack to a file-like object.\n`unpack` and `load` unpack from a file-like object.\n\n```pycon\n>>> import msgpack\n>>> msgpack.packb([1, 2, 3])\n'\\x93\\x01\\x02\\x03'\n>>> msgpack.unpackb(_)\n[1, 2, 3]\n```\n\nRead the docstring for options.\n\n\n### Streaming unpacking\n\n`Unpacker` is a \"streaming unpacker\". It unpacks multiple objects from one\nstream (or from bytes provided through its `feed` method).\n\n```py\nimport msgpack\nfrom io import BytesIO\n\nbuf = BytesIO()\nfor i in range(100):\n   buf.write(msgpack.packb(i))\n\nbuf.seek(0)\n\nunpacker = msgpack.Unpacker(buf)\nfor unpacked in unpacker:\n    print(unpacked)\n```\n\n\n### Packing/unpacking of custom data types\n\nIt is also possible to pack/unpack custom data types. Here is an example for\n`datetime.datetime`.\n\n```py\nimport datetime\nimport msgpack\n\nuseful_dict = {\n    \"id\": 1,\n    \"created\": datetime.datetime.now(),\n}\n\ndef decode_datetime(obj):\n    if '__datetime__' in obj:\n        obj = datetime.datetime.strptime(obj[\"as_str\"], \"%Y%m%dT%H:%M:%S.%f\")\n    return obj\n\ndef encode_datetime(obj):\n    if isinstance(obj, datetime.datetime):\n        return {'__datetime__': True, 'as_str': obj.strftime(\"%Y%m%dT%H:%M:%S.%f\")}\n    return obj\n\n\npacked_dict = msgpack.packb(useful_dict, default=encode_datetime)\nthis_dict_again = msgpack.unpackb(packed_dict, object_hook=decode_datetime)\n```\n\n`Unpacker`'s `object_hook` callback receives a dict; the\n`object_pairs_hook` callback may instead be used to receive a list of\nkey-value pairs.\n\nNOTE: msgpack can encode datetime with tzinfo into standard ext type for now.\nSee `datetime` option in `Packer` docstring.\n\n\n### Extended types\n\nIt is also possible to pack/unpack custom data types using the **ext** type.\n\n```pycon\n>>> import msgpack\n>>> import array\n>>> def default(obj):\n...     if isinstance(obj, array.array) and obj.typecode == 'd':\n...         return msgpack.ExtType(42, obj.tostring())\n...     raise TypeError(\"Unknown type: %r\" % (obj,))\n...\n>>> def ext_hook(code, data):\n...     if code == 42:\n...         a = array.array('d')\n...         a.fromstring(data)\n...         return a\n...     return ExtType(code, data)\n...\n>>> data = array.array('d', [1.2, 3.4])\n>>> packed = msgpack.packb(data, default=default)\n>>> unpacked = msgpack.unpackb(packed, ext_hook=ext_hook)\n>>> data == unpacked\nTrue\n```\n\n\n### Advanced unpacking control\n\nAs an alternative to iteration, `Unpacker` objects provide `unpack`,\n`skip`, `read_array_header`, and `read_map_header` methods. The former two\nread an entire message from the stream, respectively deserializing and returning\nthe result, or ignoring it. The latter two methods return the number of elements\nin the upcoming container, so that each element in an array, or key-value pair\nin a map, can be unpacked or skipped individually.\n\n\n## Notes\n\n### String and binary types in the old MessagePack spec\n\nEarly versions of msgpack didn't distinguish string and binary types.\nThe type for representing both string and binary types was named **raw**.\n\nYou can pack into and unpack from this old spec using `use_bin_type=False`\nand `raw=True` options.\n\n```pycon\n>>> import msgpack\n>>> msgpack.unpackb(msgpack.packb([b'spam', 'eggs'], use_bin_type=False), raw=True)\n[b'spam', b'eggs']\n>>> msgpack.unpackb(msgpack.packb([b'spam', 'eggs'], use_bin_type=True), raw=False)\n[b'spam', 'eggs']\n```\n\n### ext type\n\nTo use the **ext** type, pass a `msgpack.ExtType` object to the packer.\n\n```pycon\n>>> import msgpack\n>>> packed = msgpack.packb(msgpack.ExtType(42, b'xyzzy'))\n>>> msgpack.unpackb(packed)\nExtType(code=42, data='xyzzy')\n```\n\nYou can use it with `default` and `ext_hook`. See below.\n\n\n### Security\n\nWhen unpacking data received from an unreliable source, msgpack provides\ntwo security options.\n\n`max_buffer_size` (default: `100*1024*1024`) limits the internal buffer size.\nIt is also used to limit preallocated list sizes.\n\n`strict_map_key` (default: `True`) limits the type of map keys to bytes and str.\nWhile the MessagePack spec doesn't limit map key types,\nthere is a risk of a hash DoS.\nIf you need to support other types for map keys, use `strict_map_key=False`.\n\n\n### Performance tips\n\nCPython's GC starts when the number of allocated objects grows.\nThis means unpacking may trigger unnecessary GC.\nYou can use `gc.disable()` when unpacking a large message.\n\nA list is the default sequence type in Python.\nHowever, a tuple is lighter than a list.\nYou can use `use_list=False` while unpacking when performance is important.\n\n\n## Major breaking changes in the history\n\n### msgpack 0.5\n\nThe package name on PyPI was changed from `msgpack-python` to `msgpack` in 0.5.\n\nWhen upgrading from msgpack-0.4 or earlier, do `pip uninstall msgpack-python` before\n`pip install -U msgpack`.\n\n\n### msgpack 1.0\n\n* Python 2 support\n\n  * The extension module no longer supports Python 2.\n    The pure Python implementation (`msgpack.fallback`) is used for Python 2.\n  \n  * msgpack 1.0.6 drops official support of Python 2.7, as pip and\n    GitHub Action \"setup-python\" no longer supports Python 2.7.\n\n* Packer\n\n  * Packer uses `use_bin_type=True` by default.\n    Bytes are encoded in the bin type in MessagePack.\n  * The `encoding` option is removed. UTF-8 is always used.\n\n* Unpacker\n\n  * Unpacker uses `raw=False` by default. It assumes str values are valid UTF-8 strings\n    and decodes them to Python str (Unicode) objects.\n  * `encoding` option is removed.  You can use `raw=True` to support old format (e.g. unpack into bytes, not str).\n  * The default value of `max_buffer_size` is changed from 0 to 100 MiB to avoid DoS attacks.\n    You need to pass `max_buffer_size=0` if you have large but safe data.\n  * The default value of `strict_map_key` is changed to True to avoid hash DoS.\n    You need to pass `strict_map_key=False` if you have data that contain map keys\n    whose type is neither bytes nor str.\n"
      }
    },
    {
      "download_info": {
        "url": "https://pypi.tuna.tsinghua.edu.cn/packages/ca/17/1913b7c1173b2db30fb7a9696892a7c4c59aeee777a9af6859e9e01bac51/numba-0.63.1-cp310-cp310-manylinux_2_27_aarch64.manylinux_2_28_aarch64.whl",
        "archive_info": {
          "hash": "sha256=f09eebf5650246ce2a4e9a8d38270e2d4b0b0ae978103bafb38ed7adc5ea906e",
          "hashes": {
            "sha256": "f09eebf5650246ce2a4e9a8d38270e2d4b0b0ae978103bafb38ed7adc5ea906e"
          }
        }
      },
      "is_direct": false,
      "is_yanked": false,
      "requested": false,
      "metadata": {
        "metadata_version": "2.4",
        "name": "numba",
        "version": "0.63.1",
        "dynamic": [
          "classifier",
          "description",
          "home-page",
          "license",
          "license-file",
          "requires-dist",
          "requires-python",
          "summary"
        ],
        "summary": "compiling Python code using LLVM",
        "home_page": "https://numba.pydata.org",
        "license": "BSD",
        "license_file": [
          "LICENSE",
          "LICENSES.third-party"
        ],
        "classifier": [
          "Development Status :: 4 - Beta",
          "Intended Audience :: Developers",
          "License :: OSI Approved :: BSD License",
          "Operating System :: OS Independent",
          "Programming Language :: Python",
          "Programming Language :: Python :: 3",
          "Programming Language :: Python :: 3.10",
          "Programming Language :: Python :: 3.11",
          "Programming Language :: Python :: 3.12",
          "Programming Language :: Python :: 3.13",
          "Programming Language :: Python :: 3.14",
          "Topic :: Software Development :: Compilers"
        ],
        "requires_dist": [
          "llvmlite<0.47,>=0.46.0dev0",
          "numpy<2.4,>=1.22"
        ],
        "requires_python": ">=3.10",
        "description": "*****\nNumba\n*****\n\n.. image:: https://badges.gitter.im/numba/numba.svg\n   :target: https://gitter.im/numba/numba?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge\n   :alt: Gitter\n\n.. image:: https://img.shields.io/badge/discuss-on%20discourse-blue\n   :target: https://numba.discourse.group/\n   :alt: Discourse\n\n.. image:: https://zenodo.org/badge/3659275.svg\n   :target: https://zenodo.org/badge/latestdoi/3659275\n   :alt: Zenodo DOI\n\n.. image:: https://img.shields.io/pypi/v/numba.svg\n   :target: https://pypi.python.org/pypi/numba/\n   :alt: PyPI\n\n.. image:: https://dev.azure.com/numba/numba/_apis/build/status/numba.numba?branchName=main\n    :target: https://dev.azure.com/numba/numba/_build/latest?definitionId=1?branchName=main\n    :alt: Azure Pipelines\n\nA Just-In-Time Compiler for Numerical Functions in Python\n#########################################################\n\nNumba is an open source, NumPy-aware optimizing compiler for Python sponsored\nby Anaconda, Inc.  It uses the LLVM compiler project to generate machine code\nfrom Python syntax.\n\nNumba can compile a large subset of numerically-focused Python, including many\nNumPy functions.  Additionally, Numba has support for automatic\nparallelization of loops, generation of GPU-accelerated code, and creation of\nufuncs and C callbacks.\n\nFor more information about Numba, see the Numba homepage:\nhttps://numba.pydata.org and the online documentation:\nhttps://numba.readthedocs.io/en/stable/index.html\n\nInstallation\n============\n\nPlease follow the instructions:\n\nhttps://numba.readthedocs.io/en/stable/user/installing.html\n\nDemo\n====\n\nPlease have a look and the demo notebooks via the mybinder service:\n\nhttps://mybinder.org/v2/gh/numba/numba-examples/master?filepath=notebooks\n\nContact\n=======\n\nNumba has a discourse forum for discussions:\n\n* https://numba.discourse.group\n\n"
      }
    },
    {
      "download_info": {
        "url": "https://pypi.tuna.tsinghua.edu.cn/packages/79/7f/a7f2028805dac8c1a6fae7bda4e739b7ebbcd45b29e15bf6d21556fcd3d5/llvmlite-0.46.0-cp310-cp310-manylinux_2_27_aarch64.manylinux_2_28_aarch64.whl",
        "archive_info": {
          "hash": "sha256=b1f6595a35b7b39c3518b85a28bf18f45e075264e4b2dce3f0c2a4f232b4a910",
          "hashes": {
            "sha256": "b1f6595a35b7b39c3518b85a28bf18f45e075264e4b2dce3f0c2a4f232b4a910"
          }
        }
      },
      "is_direct": false,
      "is_yanked": false,
      "requested": false,
      "metadata": {
        "metadata_version": "2.4",
        "name": "llvmlite",
        "version": "0.46.0",
        "dynamic": [
          "classifier",
          "description",
          "home-page",
          "license-file",
          "license-expression",
          "project-url",
          "requires-python",
          "summary"
        ],
        "summary": "lightweight wrapper around basic LLVM functionality",
        "home_page": "http://llvmlite.readthedocs.io",
        "license_expression": "BSD-2-Clause AND Apache-2.0 WITH LLVM-exception",
        "license_file": [
          "LICENSE",
          "LICENSE.thirdparty"
        ],
        "classifier": [
          "Development Status :: 4 - Beta",
          "Intended Audience :: Developers",
          "Operating System :: OS Independent",
          "Programming Language :: Python",
          "Programming Language :: Python :: 3",
          "Programming Language :: Python :: 3.10",
          "Programming Language :: Python :: 3.11",
          "Programming Language :: Python :: 3.12",
          "Programming Language :: Python :: 3.13",
          "Programming Language :: Python :: 3.14",
          "Topic :: Software Development :: Code Generators",
          "Topic :: Software Development :: Compilers"
        ],
        "requires_python": ">=3.10",
        "project_url": [
          "Source, https://github.com/numba/llvmlite"
        ],
        "description": "========\nllvmlite\n========\n\n.. image:: https://dev.azure.com/numba/numba/_apis/build/status/numba.llvmlite?branchName=main\n   :target: https://dev.azure.com/numba/numba/_build/latest?definitionId=2&branchName=main\n   :alt: Azure Pipelines\n.. image:: https://coveralls.io/repos/github/numba/llvmlite/badge.svg\n   :target: https://coveralls.io/github/numba/llvmlite\n   :alt: Coveralls.io\n.. image:: https://readthedocs.org/projects/llvmlite/badge/\n   :target: https://llvmlite.readthedocs.io\n   :alt: Readthedocs.io\n\nA Lightweight LLVM Python Binding for Writing JIT Compilers\n-----------------------------------------------------------\n\n.. _llvmpy: https://github.com/llvmpy/llvmpy\n\nllvmlite is a project originally tailored for Numba_'s needs, using the\nfollowing approach:\n\n* A small C wrapper around the parts of the LLVM C++ API we need that are\n  not already exposed by the LLVM C API.\n* A ctypes Python wrapper around the C API.\n* A pure Python implementation of the subset of the LLVM IR builder that we\n  need for Numba.\n\nWhy llvmlite\n============\n\nThe old llvmpy_  binding exposes a lot of LLVM APIs but the mapping of\nC++-style memory management to Python is error prone. Numba_ and many JIT\ncompilers do not need a full LLVM API.  Only the IR builder, optimizer,\nand JIT compiler APIs are necessary.\n\nKey Benefits\n============\n\n* The IR builder is pure Python code and decoupled from LLVM's\n  frequently-changing C++ APIs.\n* Materializing a LLVM module calls LLVM's IR parser which provides\n  better error messages than step-by-step IR building through the C++\n  API (no more segfaults or process aborts).\n* Most of llvmlite uses the LLVM C API which is small but very stable\n  (low maintenance when changing LLVM version).\n* The binding is not a Python C-extension, but a plain DLL accessed using\n  ctypes (no need to wrestle with Python's compiler requirements and C++ 11\n  compatibility).\n* The Python binding layer has sane memory management.\n* llvmlite is faster than llvmpy thanks to a much simpler architecture\n  (the Numba_ test suite is twice faster than it was).\n\nCompatibility\n=============\n\nllvmlite has been tested with Python 3.10 -- 3.13 and is likely to work with\ngreater versions.\n\nAs of version 0.45.0, llvmlite requires LLVM 20.x.x on all architectures\n\nHistorical compatibility table:\n\n=================  ========================\nllvmlite versions  compatible LLVM versions\n=================  ========================\n0.45.0 - ......    20.x.x\n0.44.0             15.x.x and 16.x.x\n0.41.0 - 0.43.0    14.x.x\n0.40.0 - 0.40.1    11.x.x and 14.x.x (12.x.x and 13.x.x untested but may work)\n0.37.0 - 0.39.1    11.x.x\n0.34.0 - 0.36.0    10.0.x (9.0.x for  ``aarch64`` only)\n0.33.0             9.0.x\n0.29.0 - 0.32.0    7.0.x, 7.1.x, 8.0.x\n0.27.0 - 0.28.0    7.0.x\n0.23.0 - 0.26.0    6.0.x\n0.21.0 - 0.22.0    5.0.x\n0.17.0 - 0.20.0    4.0.x\n0.16.0 - 0.17.0    3.9.x\n0.13.0 - 0.15.0    3.8.x\n0.9.0 - 0.12.1     3.7.x\n0.6.0 - 0.8.0      3.6.x\n0.1.0 - 0.5.1      3.5.x\n=================  ========================\n\nDocumentation\n=============\n\nYou'll find the documentation at http://llvmlite.pydata.org\n\n\nPre-built binaries\n==================\n\nWe recommend you use the binaries provided by the Numba_ team for\nthe Conda_ package manager.  You can find them in Numba's `anaconda.org\nchannel <https://anaconda.org/numba>`_.  For example::\n\n   $ conda install --channel=numba llvmlite\n\n(or, simply, the official llvmlite package provided in the Anaconda_\ndistribution)\n\n.. _Numba: http://numba.pydata.org/\n.. _Conda: http://conda.pydata.org/\n.. _Anaconda: http://docs.continuum.io/anaconda/index.html\n\n\nOther build methods\n===================\n\nIf you don't want to use our pre-built packages, you can compile\nand install llvmlite yourself.  The documentation will teach you how:\nhttp://llvmlite.pydata.org/en/latest/install/index.html\n"
      }
    },
    {
      "download_info": {
        "url": "https://pypi.tuna.tsinghua.edu.cn/packages/a8/87/77cc11c7a9ea9fd05503def69e3d18605852cd0d4b0d3b8f15bbeb3ef1d1/pooch-1.8.2-py3-none-any.whl",
        "archive_info": {
          "hash": "sha256=3529a57096f7198778a5ceefd5ac3ef0e4d06a6ddaf9fc2d609b806f25302c47",
          "hashes": {
            "sha256": "3529a57096f7198778a5ceefd5ac3ef0e4d06a6ddaf9fc2d609b806f25302c47"
          }
        }
      },
      "is_direct": false,
      "is_yanked": false,
      "requested": false,
      "metadata": {
        "metadata_version": "2.1",
        "name": "pooch",
        "version": "1.8.2",
        "summary": "A friend to fetch your data files",
        "description_content_type": "text/markdown",
        "keywords": [
          "data",
          "download",
          "caching",
          "http"
        ],
        "author_email": "The Pooch Developers <fatiandoaterra@protonmail.com>",
        "maintainer_email": "Leonardo Uieda <leo@uieda.com>",
        "license": "BSD-3-Clause",
        "license_file": [
          "LICENSE.txt",
          "AUTHORS.md"
        ],
        "classifier": [
          "Development Status :: 5 - Production/Stable",
          "Intended Audience :: Science/Research",
          "Intended Audience :: Developers",
          "Intended Audience :: Education",
          "License :: OSI Approved :: BSD License",
          "Operating System :: OS Independent",
          "Topic :: Scientific/Engineering",
          "Topic :: Software Development :: Libraries",
          "Programming Language :: Python :: 3 :: Only",
          "Programming Language :: Python :: 3.7",
          "Programming Language :: Python :: 3.8",
          "Programming Language :: Python :: 3.9",
          "Programming Language :: Python :: 3.10",
          "Programming Language :: Python :: 3.11"
        ],
        "requires_dist": [
          "platformdirs >=2.5.0",
          "packaging >=20.0",
          "requests >=2.19.0",
          "tqdm <5.0.0,>=4.41.0 ; extra == 'progress'",
          "paramiko >=2.7.0 ; extra == 'sftp'",
          "xxhash >=1.4.3 ; extra == 'xxhash'"
        ],
        "requires_python": ">=3.7",
        "project_url": [
          "Documentation, https://www.fatiando.org/pooch",
          "Changelog, https://www.fatiando.org/pooch/latest/changes.html",
          "Bug Tracker, https://github.com/fatiando/pooch/issues",
          "Source Code, https://github.com/fatiando/pooch"
        ],
        "provides_extra": [
          "progress",
          "sftp",
          "xxhash"
        ],
        "description": "<img src=\"https://github.com/fatiando/pooch/raw/main/doc/_static/readme-banner.png\" alt=\"Pooch: A friend to fetch your data files\">\n\n<p align=\"center\">\n<a href=\"https://www.fatiando.org/pooch\"><strong>Documentation</strong> (latest)</a> ‚Ä¢\n<a href=\"https://www.fatiando.org/pooch/dev\"><strong>Documentation</strong> (main branch)</a> ‚Ä¢\n<a href=\"https://github.com/fatiando/pooch/blob/main/CONTRIBUTING.md\"><strong>Contributing</strong></a> ‚Ä¢\n<a href=\"https://www.fatiando.org/contact/\"><strong>Contact</strong></a>\n</p>\n\n<p align=\"center\">\nPart of the <a href=\"https://www.fatiando.org\"><strong>Fatiando a Terra</strong></a> project\n</p>\n\n<p align=\"center\">\n<a href=\"https://pypi.python.org/pypi/pooch\"><img src=\"http://img.shields.io/pypi/v/pooch.svg?style=flat-square\" alt=\"Latest version on PyPI\"></a>\n<a href=\"https://github.com/conda-forge/pooch-feedstock\"><img src=\"https://img.shields.io/conda/vn/conda-forge/pooch.svg?style=flat-square\" alt=\"Latest version on conda-forge\"></a>\n<a href=\"https://codecov.io/gh/fatiando/pooch\"><img src=\"https://img.shields.io/codecov/c/github/fatiando/pooch/main.svg?style=flat-square\" alt=\"Test coverage status\"></a>\n<a href=\"https://pypi.python.org/pypi/pooch\"><img src=\"https://img.shields.io/pypi/pyversions/pooch.svg?style=flat-square\" alt=\"Compatible Python versions.\"></a>\n<a href=\"https://doi.org/10.21105/joss.01943\"><img src=\"https://img.shields.io/badge/doi-10.21105%2Fjoss.01943-blue?style=flat-square\" alt=\"DOI used to cite Pooch\"></a>\n</p>\n\n## About\n\n> Just want to download a file without messing with `requests` and `urllib`?\n> Trying to add sample datasets to your Python package?\n> **Pooch is here to help!**\n\n*Pooch* is a **Python library** that can manage data by **downloading files**\nfrom a server (only when needed) and storing them locally in a data **cache**\n(a folder on your computer).\n\n* Pure Python and minimal dependencies.\n* Download files over HTTP, FTP, and from data repositories like Zenodo and figshare.\n* Built-in post-processors to unzip/decompress the data after download.\n* Designed to be extended: create custom downloaders and post-processors.\n\nAre you a **scientist** or researcher? Pooch can help you too!\n\n* Host your data on a repository and download using the DOI.\n* Automatically download data using code instead of telling colleagues to do it themselves.\n* Make sure everyone running the code has the same version of the data files.\n\n## Projects using Pooch\n\n[SciPy](https://github.com/scipy/scipy), \n[scikit-image](https://github.com/scikit-image/scikit-image),\n[xarray](https://github.com/pydata/xarray),\n[Ensaio](https://github.com/fatiando/ensaio),\n[GemPy](https://github.com/cgre-aachen/gempy),\n[MetPy](https://github.com/Unidata/MetPy),\n[napari](https://github.com/napari/napari),\n[Satpy](https://github.com/pytroll/satpy),\n[yt](https://github.com/yt-project/yt),\n[PyVista](https://github.com/pyvista/pyvista),\n[icepack](https://github.com/icepack/icepack),\n[histolab](https://github.com/histolab/histolab),\n[seaborn-image](https://github.com/SarthakJariwala/seaborn-image),\n[Open AR-Sandbox](https://github.com/cgre-aachen/open_AR_Sandbox),\n[climlab](https://github.com/climlab/climlab),\n[mne-python](https://github.com/mne-tools/mne-python),\n[GemGIS](https://github.com/cgre-aachen/gemgis),\n[SHTOOLS](https://github.com/SHTOOLS/SHTOOLS),\n[MOABB](https://github.com/NeuroTechX/moabb),\n[GeoViews](https://github.com/holoviz/geoviews),\n[ScopeSim](https://github.com/AstarVienna/ScopeSim),\n[Brainrender](https://github.com/brainglobe/brainrender),\n[pyxem](https://github.com/pyxem/pyxem),\n[cellfinder](https://github.com/brainglobe/cellfinder),\n[PVGeo](https://github.com/OpenGeoVis/PVGeo),\n[geosnap](https://github.com/oturns/geosnap),\n[BioCypher](https://github.com/biocypher/biocypher),\n[cf-xarray](https://github.com/xarray-contrib/cf-xarray),\n[Scirpy](https://github.com/scverse/scirpy),\n[rembg](https://github.com/danielgatis/rembg),\n[DASCore](https://github.com/DASDAE/dascore),\n[scikit-mobility](https://github.com/scikit-mobility/scikit-mobility),\n[Py-ART](https://github.com/ARM-DOE/pyart),\n[HyperSpy](https://github.com/hyperspy/hyperspy),\n[RosettaSciIO](https://github.com/hyperspy/rosettasciio),\n[eXSpy](https://github.com/hyperspy/exspy)\n\n\n> If you're using Pooch, **send us a pull request** adding your project to the list.\n\n## Example\n\nFor a **scientist downloading a data file** for analysis:\n\n```python\nimport pooch\nimport pandas as pd\n\n# Download a file and save it locally, returning the path to it.\n# Running this again will not cause a download. Pooch will check the hash\n# (checksum) of the downloaded file against the given value to make sure\n# it's the right file (not corrupted or outdated).\nfname_bathymetry = pooch.retrieve(\n    url=\"https://github.com/fatiando-data/caribbean-bathymetry/releases/download/v1/caribbean-bathymetry.csv.xz\",\n    known_hash=\"md5:a7332aa6e69c77d49d7fb54b764caa82\",\n)\n\n# Pooch can also download based on a DOI from certain providers.\nfname_gravity = pooch.retrieve(\n    url=\"doi:10.5281/zenodo.5882430/southern-africa-gravity.csv.xz\",\n    known_hash=\"md5:1dee324a14e647855366d6eb01a1ef35\",\n)\n\n# Load the data with Pandas\ndata_bathymetry = pd.read_csv(fname_bathymetry)\ndata_gravity = pd.read_csv(fname_gravity)\n```\n\nFor **package developers** including sample data in their projects:\n\n```python\n\"\"\"\nModule mypackage/datasets.py\n\"\"\"\nimport pkg_resources\nimport pandas\nimport pooch\n\n# Get the version string from your project. You have one of these, right?\nfrom . import version\n\n# Create a new friend to manage your sample data storage\nGOODBOY = pooch.create(\n    # Folder where the data will be stored. For a sensible default, use the\n    # default cache folder for your OS.\n    path=pooch.os_cache(\"mypackage\"),\n    # Base URL of the remote data store. Will call .format on this string\n    # to insert the version (see below).\n    base_url=\"https://github.com/myproject/mypackage/raw/{version}/data/\",\n    # Pooches are versioned so that you can use multiple versions of a\n    # package simultaneously. Use PEP440 compliant version number. The\n    # version will be appended to the path.\n    version=version,\n    # If a version as a \"+XX.XXXXX\" suffix, we'll assume that this is a dev\n    # version and replace the version with this string.\n    version_dev=\"main\",\n    # An environment variable that overwrites the path.\n    env=\"MYPACKAGE_DATA_DIR\",\n    # The cache file registry. A dictionary with all files managed by this\n    # pooch. Keys are the file names (relative to *base_url*) and values\n    # are their respective SHA256 hashes. Files will be downloaded\n    # automatically when needed (see fetch_gravity_data).\n    registry={\"gravity-data.csv\": \"89y10phsdwhs09whljwc09whcowsdhcwodcydw\"}\n)\n# You can also load the registry from a file. Each line contains a file\n# name and it's sha256 hash separated by a space. This makes it easier to\n# manage large numbers of data files. The registry file should be packaged\n# and distributed with your software.\nGOODBOY.load_registry(\n    pkg_resources.resource_stream(\"mypackage\", \"registry.txt\")\n)\n\n# Define functions that your users can call to get back the data in memory\ndef fetch_gravity_data():\n    \"\"\"\n    Load some sample gravity data to use in your docs.\n    \"\"\"\n    # Fetch the path to a file in the local storage. If it's not there,\n    # we'll download it.\n    fname = GOODBOY.fetch(\"gravity-data.csv\")\n    # Load it with numpy/pandas/etc\n    data = pandas.read_csv(fname)\n    return data\n```\n\n## Getting involved\n\nüó®Ô∏è **Contact us:**\nFind out more about how to reach us at\n[fatiando.org/contact](https://www.fatiando.org/contact/).\n\nüë©üèæ‚Äçüíª **Contributing to project development:**\nPlease read our\n[Contributing Guide](https://github.com/fatiando/pooch/blob/main/CONTRIBUTING.md)\nto see how you can help and give feedback.\n\nüßëüèæ‚Äçü§ù‚Äçüßëüèº **Code of conduct:**\nThis project is released with a\n[Code of Conduct](https://github.com/fatiando/community/blob/main/CODE_OF_CONDUCT.md).\nBy participating in this project you agree to abide by its terms.\n\n> **Imposter syndrome disclaimer:**\n> We want your help. **No, really.** There may be a little voice inside your\n> head that is telling you that you're not ready, that you aren't skilled\n> enough to contribute. We assure you that the little voice in your head is\n> wrong. Most importantly, **there are many valuable ways to contribute besides\n> writing code**.\n>\n> *This disclaimer was adapted from the*\n> [MetPy project](https://github.com/Unidata/MetPy).\n\n## License\n\nThis is free software: you can redistribute it and/or modify it under the terms\nof the **BSD 3-clause License**. A copy of this license is provided in\n[`LICENSE.txt`](https://github.com/fatiando/pooch/blob/main/LICENSE.txt).\n"
      }
    },
    {
      "download_info": {
        "url": "https://pypi.tuna.tsinghua.edu.cn/packages/20/12/38679034af332785aac8774540895e234f4d07f7545804097de4b666afd8/packaging-25.0-py3-none-any.whl",
        "archive_info": {
          "hash": "sha256=29572ef2b1f17581046b3a2227d5c611fb25ec70ca1ba8554b24b0e69331a484",
          "hashes": {
            "sha256": "29572ef2b1f17581046b3a2227d5c611fb25ec70ca1ba8554b24b0e69331a484"
          }
        }
      },
      "is_direct": false,
      "is_yanked": false,
      "requested": false,
      "metadata": {
        "metadata_version": "2.4",
        "name": "packaging",
        "version": "25.0",
        "summary": "Core utilities for Python packages",
        "description_content_type": "text/x-rst",
        "author_email": "Donald Stufft <donald@stufft.io>",
        "license_file": [
          "LICENSE",
          "LICENSE.APACHE",
          "LICENSE.BSD"
        ],
        "classifier": [
          "Development Status :: 5 - Production/Stable",
          "Intended Audience :: Developers",
          "License :: OSI Approved :: Apache Software License",
          "License :: OSI Approved :: BSD License",
          "Programming Language :: Python",
          "Programming Language :: Python :: 3",
          "Programming Language :: Python :: 3 :: Only",
          "Programming Language :: Python :: 3.8",
          "Programming Language :: Python :: 3.9",
          "Programming Language :: Python :: 3.10",
          "Programming Language :: Python :: 3.11",
          "Programming Language :: Python :: 3.12",
          "Programming Language :: Python :: 3.13",
          "Programming Language :: Python :: Implementation :: CPython",
          "Programming Language :: Python :: Implementation :: PyPy",
          "Typing :: Typed"
        ],
        "requires_python": ">=3.8",
        "project_url": [
          "Documentation, https://packaging.pypa.io/",
          "Source, https://github.com/pypa/packaging"
        ],
        "description": "packaging\n=========\n\n.. start-intro\n\nReusable core utilities for various Python Packaging\n`interoperability specifications <https://packaging.python.org/specifications/>`_.\n\nThis library provides utilities that implement the interoperability\nspecifications which have clearly one correct behaviour (eg: :pep:`440`)\nor benefit greatly from having a single shared implementation (eg: :pep:`425`).\n\n.. end-intro\n\nThe ``packaging`` project includes the following: version handling, specifiers,\nmarkers, requirements, tags, utilities.\n\nDocumentation\n-------------\n\nThe `documentation`_ provides information and the API for the following:\n\n- Version Handling\n- Specifiers\n- Markers\n- Requirements\n- Tags\n- Utilities\n\nInstallation\n------------\n\nUse ``pip`` to install these utilities::\n\n    pip install packaging\n\nThe ``packaging`` library uses calendar-based versioning (``YY.N``).\n\nDiscussion\n----------\n\nIf you run into bugs, you can file them in our `issue tracker`_.\n\nYou can also join ``#pypa`` on Freenode to ask questions or get involved.\n\n\n.. _`documentation`: https://packaging.pypa.io/\n.. _`issue tracker`: https://github.com/pypa/packaging/issues\n\n\nCode of Conduct\n---------------\n\nEveryone interacting in the packaging project's codebases, issue trackers, chat\nrooms, and mailing lists is expected to follow the `PSF Code of Conduct`_.\n\n.. _PSF Code of Conduct: https://github.com/pypa/.github/blob/main/CODE_OF_CONDUCT.md\n\nContributing\n------------\n\nThe ``CONTRIBUTING.rst`` file outlines how to contribute to this project as\nwell as how to report a potential security issue. The documentation for this\nproject also covers information about `project development`_ and `security`_.\n\n.. _`project development`: https://packaging.pypa.io/en/latest/development/\n.. _`security`: https://packaging.pypa.io/en/latest/security/\n\nProject History\n---------------\n\nPlease review the ``CHANGELOG.rst`` file or the `Changelog documentation`_ for\nrecent changes and project history.\n\n.. _`Changelog documentation`: https://packaging.pypa.io/en/latest/changelog/\n\n"
      }
    },
    {
      "download_info": {
        "url": "https://pypi.tuna.tsinghua.edu.cn/packages/cb/28/3bfe2fa5a7b9c46fe7e13c97bda14c895fb10fa2ebf1d0abb90e0cea7ee1/platformdirs-4.5.1-py3-none-any.whl",
        "archive_info": {
          "hash": "sha256=d03afa3963c806a9bed9d5125c8f4cb2fdaf74a55ab60e5d59b3fde758104d31",
          "hashes": {
            "sha256": "d03afa3963c806a9bed9d5125c8f4cb2fdaf74a55ab60e5d59b3fde758104d31"
          }
        }
      },
      "is_direct": false,
      "is_yanked": false,
      "requested": false,
      "metadata": {
        "metadata_version": "2.4",
        "name": "platformdirs",
        "version": "4.5.1",
        "summary": "A small Python package for determining appropriate platform-specific dirs, e.g. a `user data dir`.",
        "description_content_type": "text/x-rst",
        "keywords": [
          "appdirs",
          "application",
          "cache",
          "directory",
          "log",
          "user"
        ],
        "maintainer_email": "Bern√°t G√°bor <gaborjbernat@gmail.com>, Julian Berman <Julian@GrayVines.com>, Ofek Lev <oss@ofek.dev>, Ronny Pfannschmidt <opensource@ronnypfannschmidt.de>",
        "license_expression": "MIT",
        "license_file": [
          "LICENSE"
        ],
        "classifier": [
          "Development Status :: 5 - Production/Stable",
          "Intended Audience :: Developers",
          "License :: OSI Approved :: MIT License",
          "Operating System :: OS Independent",
          "Programming Language :: Python",
          "Programming Language :: Python :: 3 :: Only",
          "Programming Language :: Python :: 3.10",
          "Programming Language :: Python :: 3.11",
          "Programming Language :: Python :: 3.12",
          "Programming Language :: Python :: 3.13",
          "Programming Language :: Python :: 3.14",
          "Programming Language :: Python :: Implementation :: CPython",
          "Programming Language :: Python :: Implementation :: PyPy",
          "Topic :: Software Development :: Libraries :: Python Modules"
        ],
        "requires_dist": [
          "furo>=2025.9.25; extra == 'docs'",
          "proselint>=0.14; extra == 'docs'",
          "sphinx-autodoc-typehints>=3.2; extra == 'docs'",
          "sphinx>=8.2.3; extra == 'docs'",
          "appdirs==1.4.4; extra == 'test'",
          "covdefaults>=2.3; extra == 'test'",
          "pytest-cov>=7; extra == 'test'",
          "pytest-mock>=3.15.1; extra == 'test'",
          "pytest>=8.4.2; extra == 'test'",
          "mypy>=1.18.2; extra == 'type'"
        ],
        "requires_python": ">=3.10",
        "project_url": [
          "Changelog, https://github.com/tox-dev/platformdirs/releases",
          "Documentation, https://platformdirs.readthedocs.io",
          "Homepage, https://github.com/tox-dev/platformdirs",
          "Source, https://github.com/tox-dev/platformdirs",
          "Tracker, https://github.com/tox-dev/platformdirs/issues"
        ],
        "provides_extra": [
          "docs",
          "test",
          "type"
        ],
        "description": "The problem\n===========\n\n.. image:: https://badge.fury.io/py/platformdirs.svg\n   :target: https://badge.fury.io/py/platformdirs\n.. image:: https://img.shields.io/pypi/pyversions/platformdirs.svg\n   :target: https://pypi.python.org/pypi/platformdirs/\n.. image:: https://github.com/tox-dev/platformdirs/actions/workflows/check.yaml/badge.svg\n   :target: https://github.com/platformdirs/platformdirs/actions\n.. image:: https://static.pepy.tech/badge/platformdirs/month\n   :target: https://pepy.tech/project/platformdirs\n\nWhen writing desktop application, finding the right location to store user data\nand configuration varies per platform. Even for single-platform apps, there\nmay by plenty of nuances in figuring out the right location.\n\nFor example, if running on macOS, you should use::\n\n    ~/Library/Application Support/<AppName>\n\nIf on Windows (at least English Win) that should be::\n\n    C:\\Users\\<User>\\Application Data\\Local Settings\\<AppAuthor>\\<AppName>\n\nor possibly::\n\n    C:\\Users\\<User>\\Application Data\\<AppAuthor>\\<AppName>\n\nfor `roaming profiles <https://docs.microsoft.com/en-us/previous-versions/windows/it-pro/windows-vista/cc766489(v=ws.10)>`_ but that is another story.\n\nOn Linux (and other Unices), according to the `XDG Basedir Spec`_, it should be::\n\n    ~/.local/share/<AppName>\n\n.. _XDG Basedir Spec: https://specifications.freedesktop.org/basedir-spec/basedir-spec-latest.html\n\n``platformdirs`` to the rescue\n==============================\n\nThis kind of thing is what the ``platformdirs`` package is for.\n``platformdirs`` will help you choose an appropriate:\n\n- user data dir (``user_data_dir``)\n- user config dir (``user_config_dir``)\n- user cache dir (``user_cache_dir``)\n- site data dir (``site_data_dir``)\n- site config dir (``site_config_dir``)\n- user log dir (``user_log_dir``)\n- user documents dir (``user_documents_dir``)\n- user downloads dir (``user_downloads_dir``)\n- user pictures dir (``user_pictures_dir``)\n- user videos dir (``user_videos_dir``)\n- user music dir (``user_music_dir``)\n- user desktop dir (``user_desktop_dir``)\n- user runtime dir (``user_runtime_dir``)\n\nAnd also:\n\n- Is slightly opinionated on the directory names used. Look for \"OPINION\" in\n  documentation and code for when an opinion is being applied.\n\nExample output\n==============\n\nOn macOS:\n\n.. code-block:: pycon\n\n    >>> from platformdirs import *\n    >>> appname = \"SuperApp\"\n    >>> appauthor = \"Acme\"\n    >>> user_data_dir(appname, appauthor)\n    '/Users/trentm/Library/Application Support/SuperApp'\n    >>> user_config_dir(appname, appauthor)\n    '/Users/trentm/Library/Application Support/SuperApp'\n    >>> user_cache_dir(appname, appauthor)\n    '/Users/trentm/Library/Caches/SuperApp'\n    >>> site_data_dir(appname, appauthor)\n    '/Library/Application Support/SuperApp'\n    >>> site_config_dir(appname, appauthor)\n    '/Library/Application Support/SuperApp'\n    >>> user_log_dir(appname, appauthor)\n    '/Users/trentm/Library/Logs/SuperApp'\n    >>> user_documents_dir()\n    '/Users/trentm/Documents'\n    >>> user_downloads_dir()\n    '/Users/trentm/Downloads'\n    >>> user_pictures_dir()\n    '/Users/trentm/Pictures'\n    >>> user_videos_dir()\n    '/Users/trentm/Movies'\n    >>> user_music_dir()\n    '/Users/trentm/Music'\n    >>> user_desktop_dir()\n    '/Users/trentm/Desktop'\n    >>> user_runtime_dir(appname, appauthor)\n    '/Users/trentm/Library/Caches/TemporaryItems/SuperApp'\n\nOn Windows:\n\n.. code-block:: pycon\n\n    >>> from platformdirs import *\n    >>> appname = \"SuperApp\"\n    >>> appauthor = \"Acme\"\n    >>> user_data_dir(appname, appauthor)\n    'C:\\\\Users\\\\trentm\\\\AppData\\\\Local\\\\Acme\\\\SuperApp'\n    >>> user_data_dir(appname, appauthor, roaming=True)\n    'C:\\\\Users\\\\trentm\\\\AppData\\\\Roaming\\\\Acme\\\\SuperApp'\n    >>> user_config_dir(appname, appauthor)\n    'C:\\\\Users\\\\trentm\\\\AppData\\\\Local\\\\Acme\\\\SuperApp'\n    >>> user_cache_dir(appname, appauthor)\n    'C:\\\\Users\\\\trentm\\\\AppData\\\\Local\\\\Acme\\\\SuperApp\\\\Cache'\n    >>> site_data_dir(appname, appauthor)\n    'C:\\\\ProgramData\\\\Acme\\\\SuperApp'\n    >>> site_config_dir(appname, appauthor)\n    'C:\\\\ProgramData\\\\Acme\\\\SuperApp'\n    >>> user_log_dir(appname, appauthor)\n    'C:\\\\Users\\\\trentm\\\\AppData\\\\Local\\\\Acme\\\\SuperApp\\\\Logs'\n    >>> user_documents_dir()\n    'C:\\\\Users\\\\trentm\\\\Documents'\n    >>> user_downloads_dir()\n    'C:\\\\Users\\\\trentm\\\\Downloads'\n    >>> user_pictures_dir()\n    'C:\\\\Users\\\\trentm\\\\Pictures'\n    >>> user_videos_dir()\n    'C:\\\\Users\\\\trentm\\\\Videos'\n    >>> user_music_dir()\n    'C:\\\\Users\\\\trentm\\\\Music'\n    >>> user_desktop_dir()\n    'C:\\\\Users\\\\trentm\\\\Desktop'\n    >>> user_runtime_dir(appname, appauthor)\n    'C:\\\\Users\\\\trentm\\\\AppData\\\\Local\\\\Temp\\\\Acme\\\\SuperApp'\n\nOn Linux:\n\n.. code-block:: pycon\n\n    >>> from platformdirs import *\n    >>> appname = \"SuperApp\"\n    >>> appauthor = \"Acme\"\n    >>> user_data_dir(appname, appauthor)\n    '/home/trentm/.local/share/SuperApp'\n    >>> user_config_dir(appname)\n    '/home/trentm/.config/SuperApp'\n    >>> user_cache_dir(appname, appauthor)\n    '/home/trentm/.cache/SuperApp'\n    >>> site_data_dir(appname, appauthor)\n    '/usr/local/share/SuperApp'\n    >>> site_data_dir(appname, appauthor, multipath=True)\n    '/usr/local/share/SuperApp:/usr/share/SuperApp'\n    >>> site_config_dir(appname)\n    '/etc/xdg/SuperApp'\n    >>> os.environ[\"XDG_CONFIG_DIRS\"] = \"/etc:/usr/local/etc\"\n    >>> site_config_dir(appname, multipath=True)\n    '/etc/SuperApp:/usr/local/etc/SuperApp'\n    >>> user_log_dir(appname, appauthor)\n    '/home/trentm/.local/state/SuperApp/log'\n    >>> user_documents_dir()\n    '/home/trentm/Documents'\n    >>> user_downloads_dir()\n    '/home/trentm/Downloads'\n    >>> user_pictures_dir()\n    '/home/trentm/Pictures'\n    >>> user_videos_dir()\n    '/home/trentm/Videos'\n    >>> user_music_dir()\n    '/home/trentm/Music'\n    >>> user_desktop_dir()\n    '/home/trentm/Desktop'\n    >>> user_runtime_dir(appname, appauthor)\n    '/run/user/{os.getuid()}/SuperApp'\n\nOn Android::\n\n    >>> from platformdirs import *\n    >>> appname = \"SuperApp\"\n    >>> appauthor = \"Acme\"\n    >>> user_data_dir(appname, appauthor)\n    '/data/data/com.myApp/files/SuperApp'\n    >>> user_config_dir(appname)\n    '/data/data/com.myApp/shared_prefs/SuperApp'\n    >>> user_cache_dir(appname, appauthor)\n    '/data/data/com.myApp/cache/SuperApp'\n    >>> site_data_dir(appname, appauthor)\n    '/data/data/com.myApp/files/SuperApp'\n    >>> site_config_dir(appname)\n    '/data/data/com.myApp/shared_prefs/SuperApp'\n    >>> user_log_dir(appname, appauthor)\n    '/data/data/com.myApp/cache/SuperApp/log'\n    >>> user_documents_dir()\n    '/storage/emulated/0/Documents'\n    >>> user_downloads_dir()\n    '/storage/emulated/0/Downloads'\n    >>> user_pictures_dir()\n    '/storage/emulated/0/Pictures'\n    >>> user_videos_dir()\n    '/storage/emulated/0/DCIM/Camera'\n    >>> user_music_dir()\n    '/storage/emulated/0/Music'\n    >>> user_desktop_dir()\n    '/storage/emulated/0/Desktop'\n    >>> user_runtime_dir(appname, appauthor)\n    '/data/data/com.myApp/cache/SuperApp/tmp'\n\nNote: Some android apps like Termux and Pydroid are used as shells. These\napps are used by the end user to emulate Linux environment. Presence of\n``SHELL`` environment variable is used by Platformdirs to differentiate\nbetween general android apps and android apps used as shells. Shell android\napps also support ``XDG_*`` environment variables.\n\n\n``PlatformDirs`` for convenience\n================================\n\n.. code-block:: pycon\n\n    >>> from platformdirs import PlatformDirs\n    >>> dirs = PlatformDirs(\"SuperApp\", \"Acme\")\n    >>> dirs.user_data_dir\n    '/Users/trentm/Library/Application Support/SuperApp'\n    >>> dirs.user_config_dir\n    '/Users/trentm/Library/Application Support/SuperApp'\n    >>> dirs.user_cache_dir\n    '/Users/trentm/Library/Caches/SuperApp'\n    >>> dirs.site_data_dir\n    '/Library/Application Support/SuperApp'\n    >>> dirs.site_config_dir\n    '/Library/Application Support/SuperApp'\n    >>> dirs.user_cache_dir\n    '/Users/trentm/Library/Caches/SuperApp'\n    >>> dirs.user_log_dir\n    '/Users/trentm/Library/Logs/SuperApp'\n    >>> dirs.user_documents_dir\n    '/Users/trentm/Documents'\n    >>> dirs.user_downloads_dir\n    '/Users/trentm/Downloads'\n    >>> dirs.user_pictures_dir\n    '/Users/trentm/Pictures'\n    >>> dirs.user_videos_dir\n    '/Users/trentm/Movies'\n    >>> dirs.user_music_dir\n    '/Users/trentm/Music'\n    >>> dirs.user_desktop_dir\n    '/Users/trentm/Desktop'\n    >>> dirs.user_runtime_dir\n    '/Users/trentm/Library/Caches/TemporaryItems/SuperApp'\n\nPer-version isolation\n=====================\n\nIf you have multiple versions of your app in use that you want to be\nable to run side-by-side, then you may want version-isolation for these\ndirs::\n\n    >>> from platformdirs import PlatformDirs\n    >>> dirs = PlatformDirs(\"SuperApp\", \"Acme\", version=\"1.0\")\n    >>> dirs.user_data_dir\n    '/Users/trentm/Library/Application Support/SuperApp/1.0'\n    >>> dirs.user_config_dir\n    '/Users/trentm/Library/Application Support/SuperApp/1.0'\n    >>> dirs.user_cache_dir\n    '/Users/trentm/Library/Caches/SuperApp/1.0'\n    >>> dirs.site_data_dir\n    '/Library/Application Support/SuperApp/1.0'\n    >>> dirs.site_config_dir\n    '/Library/Application Support/SuperApp/1.0'\n    >>> dirs.user_log_dir\n    '/Users/trentm/Library/Logs/SuperApp/1.0'\n    >>> dirs.user_documents_dir\n    '/Users/trentm/Documents'\n    >>> dirs.user_downloads_dir\n    '/Users/trentm/Downloads'\n    >>> dirs.user_pictures_dir\n    '/Users/trentm/Pictures'\n    >>> dirs.user_videos_dir\n    '/Users/trentm/Movies'\n    >>> dirs.user_music_dir\n    '/Users/trentm/Music'\n    >>> dirs.user_desktop_dir\n    '/Users/trentm/Desktop'\n    >>> dirs.user_runtime_dir\n    '/Users/trentm/Library/Caches/TemporaryItems/SuperApp/1.0'\n\nBe wary of using this for configuration files though; you'll need to handle\nmigrating configuration files manually.\n\nWhy this Fork?\n==============\n\nThis repository is a friendly fork of the wonderful work started by\n`ActiveState <https://github.com/ActiveState/appdirs>`_ who created\n``appdirs``, this package's ancestor.\n\nMaintaining an open source project is no easy task, particularly\nfrom within an organization, and the Python community is indebted\nto ``appdirs`` (and to Trent Mick and Jeff Rouse in particular) for\ncreating an incredibly useful simple module, as evidenced by the wide\nnumber of users it has attracted over the years.\n\nNonetheless, given the number of long-standing open issues\nand pull requests, and no clear path towards `ensuring\nthat maintenance of the package would continue or grow\n<https://github.com/ActiveState/appdirs/issues/79>`_, this fork was\ncreated.\n\nContributions are most welcome.\n"
      }
    },
    {
      "download_info": {
        "url": "https://pypi.tuna.tsinghua.edu.cn/packages/1e/db/4254e3eabe8020b458f1a747140d32277ec7a271daf1d235b70dc0b4e6e3/requests-2.32.5-py3-none-any.whl",
        "archive_info": {
          "hash": "sha256=2462f94637a34fd532264295e186976db0f5d453d1cdd31473c85a6a161affb6",
          "hashes": {
            "sha256": "2462f94637a34fd532264295e186976db0f5d453d1cdd31473c85a6a161affb6"
          }
        }
      },
      "is_direct": false,
      "is_yanked": false,
      "requested": false,
      "metadata": {
        "metadata_version": "2.4",
        "name": "requests",
        "version": "2.32.5",
        "dynamic": [
          "author",
          "author-email",
          "classifier",
          "description",
          "description-content-type",
          "home-page",
          "license",
          "license-file",
          "project-url",
          "provides-extra",
          "requires-dist",
          "requires-python",
          "summary"
        ],
        "summary": "Python HTTP for Humans.",
        "description_content_type": "text/markdown",
        "home_page": "https://requests.readthedocs.io",
        "author": "Kenneth Reitz",
        "author_email": "me@kennethreitz.org",
        "license": "Apache-2.0",
        "license_file": [
          "LICENSE"
        ],
        "classifier": [
          "Development Status :: 5 - Production/Stable",
          "Environment :: Web Environment",
          "Intended Audience :: Developers",
          "License :: OSI Approved :: Apache Software License",
          "Natural Language :: English",
          "Operating System :: OS Independent",
          "Programming Language :: Python",
          "Programming Language :: Python :: 3",
          "Programming Language :: Python :: 3.9",
          "Programming Language :: Python :: 3.10",
          "Programming Language :: Python :: 3.11",
          "Programming Language :: Python :: 3.12",
          "Programming Language :: Python :: 3.13",
          "Programming Language :: Python :: 3.14",
          "Programming Language :: Python :: 3 :: Only",
          "Programming Language :: Python :: Implementation :: CPython",
          "Programming Language :: Python :: Implementation :: PyPy",
          "Topic :: Internet :: WWW/HTTP",
          "Topic :: Software Development :: Libraries"
        ],
        "requires_dist": [
          "charset_normalizer<4,>=2",
          "idna<4,>=2.5",
          "urllib3<3,>=1.21.1",
          "certifi>=2017.4.17",
          "PySocks!=1.5.7,>=1.5.6; extra == \"socks\"",
          "chardet<6,>=3.0.2; extra == \"use-chardet-on-py3\""
        ],
        "requires_python": ">=3.9",
        "project_url": [
          "Documentation, https://requests.readthedocs.io",
          "Source, https://github.com/psf/requests"
        ],
        "provides_extra": [
          "security",
          "socks",
          "use-chardet-on-py3"
        ],
        "description": "# Requests\n\n**Requests** is a simple, yet elegant, HTTP library.\n\n```python\n>>> import requests\n>>> r = requests.get('https://httpbin.org/basic-auth/user/pass', auth=('user', 'pass'))\n>>> r.status_code\n200\n>>> r.headers['content-type']\n'application/json; charset=utf8'\n>>> r.encoding\n'utf-8'\n>>> r.text\n'{\"authenticated\": true, ...'\n>>> r.json()\n{'authenticated': True, ...}\n```\n\nRequests allows you to send HTTP/1.1 requests extremely easily. There‚Äôs no need to manually add query strings to your URLs, or to form-encode your `PUT` & `POST` data ‚Äî but nowadays, just use the `json` method!\n\nRequests is one of the most downloaded Python packages today, pulling in around `30M downloads / week`‚Äî according to GitHub, Requests is currently [depended upon](https://github.com/psf/requests/network/dependents?package_id=UGFja2FnZS01NzA4OTExNg%3D%3D) by `1,000,000+` repositories. You may certainly put your trust in this code.\n\n[![Downloads](https://static.pepy.tech/badge/requests/month)](https://pepy.tech/project/requests)\n[![Supported Versions](https://img.shields.io/pypi/pyversions/requests.svg)](https://pypi.org/project/requests)\n[![Contributors](https://img.shields.io/github/contributors/psf/requests.svg)](https://github.com/psf/requests/graphs/contributors)\n\n## Installing Requests and Supported Versions\n\nRequests is available on PyPI:\n\n```console\n$ python -m pip install requests\n```\n\nRequests officially supports Python 3.9+.\n\n## Supported Features & Best‚ÄìPractices\n\nRequests is ready for the demands of building robust and reliable HTTP‚Äìspeaking applications, for the needs of today.\n\n- Keep-Alive & Connection Pooling\n- International Domains and URLs\n- Sessions with Cookie Persistence\n- Browser-style TLS/SSL Verification\n- Basic & Digest Authentication\n- Familiar `dict`‚Äìlike Cookies\n- Automatic Content Decompression and Decoding\n- Multi-part File Uploads\n- SOCKS Proxy Support\n- Connection Timeouts\n- Streaming Downloads\n- Automatic honoring of `.netrc`\n- Chunked HTTP Requests\n\n## API Reference and User Guide available on [Read the Docs](https://requests.readthedocs.io)\n\n[![Read the Docs](https://raw.githubusercontent.com/psf/requests/main/ext/ss.png)](https://requests.readthedocs.io)\n\n## Cloning the repository\n\nWhen cloning the Requests repository, you may need to add the `-c\nfetch.fsck.badTimezone=ignore` flag to avoid an error about a bad commit timestamp (see\n[this issue](https://github.com/psf/requests/issues/2690) for more background):\n\n```shell\ngit clone -c fetch.fsck.badTimezone=ignore https://github.com/psf/requests.git\n```\n\nYou can also apply this setting to your global Git config:\n\n```shell\ngit config --global fetch.fsck.badTimezone ignore\n```\n\n---\n\n[![Kenneth Reitz](https://raw.githubusercontent.com/psf/requests/main/ext/kr.png)](https://kennethreitz.org) [![Python Software Foundation](https://raw.githubusercontent.com/psf/requests/main/ext/psf.png)](https://www.python.org/psf)\n"
      }
    },
    {
      "download_info": {
        "url": "https://pypi.tuna.tsinghua.edu.cn/packages/5c/af/1f9d7f7faafe2ddfb6f72a2e07a548a629c61ad510fe60f9630309908fef/charset_normalizer-3.4.4-cp310-cp310-manylinux2014_aarch64.manylinux_2_17_aarch64.manylinux_2_28_aarch64.whl",
        "archive_info": {
          "hash": "sha256=4bd5d4137d500351a30687c2d3971758aac9a19208fc110ccb9d7188fbe709e8",
          "hashes": {
            "sha256": "4bd5d4137d500351a30687c2d3971758aac9a19208fc110ccb9d7188fbe709e8"
          }
        }
      },
      "is_direct": false,
      "is_yanked": false,
      "requested": false,
      "metadata": {
        "metadata_version": "2.4",
        "name": "charset-normalizer",
        "version": "3.4.4",
        "dynamic": [
          "license-file"
        ],
        "summary": "The Real First Universal Charset Detector. Open, modern and actively maintained alternative to Chardet.",
        "description_content_type": "text/markdown",
        "keywords": [
          "encoding",
          "charset",
          "charset-detector",
          "detector",
          "normalization",
          "unicode",
          "chardet",
          "detect"
        ],
        "author_email": "\"Ahmed R. TAHRI\" <tahri.ahmed@proton.me>",
        "maintainer_email": "\"Ahmed R. TAHRI\" <tahri.ahmed@proton.me>",
        "license": "MIT",
        "license_file": [
          "LICENSE"
        ],
        "classifier": [
          "Development Status :: 5 - Production/Stable",
          "Intended Audience :: Developers",
          "Operating System :: OS Independent",
          "Programming Language :: Python",
          "Programming Language :: Python :: 3",
          "Programming Language :: Python :: 3.7",
          "Programming Language :: Python :: 3.8",
          "Programming Language :: Python :: 3.9",
          "Programming Language :: Python :: 3.10",
          "Programming Language :: Python :: 3.11",
          "Programming Language :: Python :: 3.12",
          "Programming Language :: Python :: 3.13",
          "Programming Language :: Python :: 3.14",
          "Programming Language :: Python :: 3 :: Only",
          "Programming Language :: Python :: Implementation :: CPython",
          "Programming Language :: Python :: Implementation :: PyPy",
          "Topic :: Text Processing :: Linguistic",
          "Topic :: Utilities",
          "Typing :: Typed"
        ],
        "requires_python": ">=3.7",
        "project_url": [
          "Changelog, https://github.com/jawah/charset_normalizer/blob/master/CHANGELOG.md",
          "Documentation, https://charset-normalizer.readthedocs.io/",
          "Code, https://github.com/jawah/charset_normalizer",
          "Issue tracker, https://github.com/jawah/charset_normalizer/issues"
        ],
        "provides_extra": [
          "unicode-backport"
        ],
        "description": "<h1 align=\"center\">Charset Detection, for Everyone üëã</h1>\n\n<p align=\"center\">\n  <sup>The Real First Universal Charset Detector</sup><br>\n  <a href=\"https://pypi.org/project/charset-normalizer\">\n    <img src=\"https://img.shields.io/pypi/pyversions/charset_normalizer.svg?orange=blue\" />\n  </a>\n  <a href=\"https://pepy.tech/project/charset-normalizer/\">\n    <img alt=\"Download Count Total\" src=\"https://static.pepy.tech/badge/charset-normalizer/month\" />\n  </a>\n  <a href=\"https://bestpractices.coreinfrastructure.org/projects/7297\">\n    <img src=\"https://bestpractices.coreinfrastructure.org/projects/7297/badge\">\n  </a>\n</p>\n<p align=\"center\">\n  <sup><i>Featured Packages</i></sup><br>\n  <a href=\"https://github.com/jawah/niquests\">\n   <img alt=\"Static Badge\" src=\"https://img.shields.io/badge/Niquests-Most_Advanced_HTTP_Client-cyan\">\n  </a>\n  <a href=\"https://github.com/jawah/wassima\">\n   <img alt=\"Static Badge\" src=\"https://img.shields.io/badge/Wassima-Certifi_Replacement-cyan\">\n  </a>\n</p>\n<p align=\"center\">\n  <sup><i>In other language (unofficial port - by the community)</i></sup><br>\n  <a href=\"https://github.com/nickspring/charset-normalizer-rs\">\n   <img alt=\"Static Badge\" src=\"https://img.shields.io/badge/Rust-red\">\n  </a>\n</p>\n\n> A library that helps you read text from an unknown charset encoding.<br /> Motivated by `chardet`,\n> I'm trying to resolve the issue by taking a new approach.\n> All IANA character set names for which the Python core library provides codecs are supported.\n\n<p align=\"center\">\n  >>>>> <a href=\"https://charsetnormalizerweb.ousret.now.sh\" target=\"_blank\">üëâ Try Me Online Now, Then Adopt Me üëà </a> <<<<<\n</p>\n\nThis project offers you an alternative to **Universal Charset Encoding Detector**, also known as **Chardet**.\n\n| Feature                                          | [Chardet](https://github.com/chardet/chardet) |                                         Charset Normalizer                                         | [cChardet](https://github.com/PyYoshi/cChardet) |\n|--------------------------------------------------|:---------------------------------------------:|:--------------------------------------------------------------------------------------------------:|:-----------------------------------------------:|\n| `Fast`                                           |                       ‚ùå                       |                                                 ‚úÖ                                                  |                        ‚úÖ                        |\n| `Universal**`                                    |                       ‚ùå                       |                                                 ‚úÖ                                                  |                        ‚ùå                        |\n| `Reliable` **without** distinguishable standards |                       ‚ùå                       |                                                 ‚úÖ                                                  |                        ‚úÖ                        |\n| `Reliable` **with** distinguishable standards    |                       ‚úÖ                       |                                                 ‚úÖ                                                  |                        ‚úÖ                        |\n| `License`                                        |           LGPL-2.1<br>_restrictive_           |                                                MIT                                                 |            MPL-1.1<br>_restrictive_             |\n| `Native Python`                                  |                       ‚úÖ                       |                                                 ‚úÖ                                                  |                        ‚ùå                        |\n| `Detect spoken language`                         |                       ‚ùå                       |                                                 ‚úÖ                                                  |                       N/A                       |\n| `UnicodeDecodeError Safety`                      |                       ‚ùå                       |                                                 ‚úÖ                                                  |                        ‚ùå                        |\n| `Whl Size (min)`                                 |                   193.6 kB                    |                                               42 kB                                                |                     ~200 kB                     |\n| `Supported Encoding`                             |                      33                       | üéâ [99](https://charset-normalizer.readthedocs.io/en/latest/user/support.html#supported-encodings) |                       40                        |\n\n<p align=\"center\">\n<img src=\"https://i.imgflip.com/373iay.gif\" alt=\"Reading Normalized Text\" width=\"226\"/><img src=\"https://media.tenor.com/images/c0180f70732a18b4965448d33adba3d0/tenor.gif\" alt=\"Cat Reading Text\" width=\"200\"/>\n</p>\n\n*\\*\\* : They are clearly using specific code for a specific encoding even if covering most of used one*<br>\n\n## ‚ö° Performance\n\nThis package offer better performance than its counterpart Chardet. Here are some numbers.\n\n| Package                                       | Accuracy | Mean per file (ms) | File per sec (est) |\n|-----------------------------------------------|:--------:|:------------------:|:------------------:|\n| [chardet](https://github.com/chardet/chardet) |   86 %   |       63 ms        |    16 file/sec     |\n| charset-normalizer                            | **98 %** |     **10 ms**      |    100 file/sec    |\n\n| Package                                       | 99th percentile | 95th percentile | 50th percentile |\n|-----------------------------------------------|:---------------:|:---------------:|:---------------:|\n| [chardet](https://github.com/chardet/chardet) |     265 ms      |      71 ms      |      7 ms       |\n| charset-normalizer                            |     100 ms      |      50 ms      |      5 ms       |\n\n_updated as of december 2024 using CPython 3.12_\n\nChardet's performance on larger file (1MB+) are very poor. Expect huge difference on large payload.\n\n> Stats are generated using 400+ files using default parameters. More details on used files, see GHA workflows.\n> And yes, these results might change at any time. The dataset can be updated to include more files.\n> The actual delays heavily depends on your CPU capabilities. The factors should remain the same.\n> Keep in mind that the stats are generous and that Chardet accuracy vs our is measured using Chardet initial capability\n> (e.g. Supported Encoding) Challenge-them if you want.\n\n## ‚ú® Installation\n\nUsing pip:\n\n```sh\npip install charset-normalizer -U\n```\n\n## üöÄ Basic Usage\n\n### CLI\nThis package comes with a CLI.\n\n```\nusage: normalizer [-h] [-v] [-a] [-n] [-m] [-r] [-f] [-t THRESHOLD]\n                  file [file ...]\n\nThe Real First Universal Charset Detector. Discover originating encoding used\non text file. Normalize text to unicode.\n\npositional arguments:\n  files                 File(s) to be analysed\n\noptional arguments:\n  -h, --help            show this help message and exit\n  -v, --verbose         Display complementary information about file if any.\n                        Stdout will contain logs about the detection process.\n  -a, --with-alternative\n                        Output complementary possibilities if any. Top-level\n                        JSON WILL be a list.\n  -n, --normalize       Permit to normalize input file. If not set, program\n                        does not write anything.\n  -m, --minimal         Only output the charset detected to STDOUT. Disabling\n                        JSON output.\n  -r, --replace         Replace file when trying to normalize it instead of\n                        creating a new one.\n  -f, --force           Replace file without asking if you are sure, use this\n                        flag with caution.\n  -t THRESHOLD, --threshold THRESHOLD\n                        Define a custom maximum amount of chaos allowed in\n                        decoded content. 0. <= chaos <= 1.\n  --version             Show version information and exit.\n```\n\n```bash\nnormalizer ./data/sample.1.fr.srt\n```\n\nor\n\n```bash\npython -m charset_normalizer ./data/sample.1.fr.srt\n```\n\nüéâ Since version 1.4.0 the CLI produce easily usable stdout result in JSON format.\n\n```json\n{\n    \"path\": \"/home/default/projects/charset_normalizer/data/sample.1.fr.srt\",\n    \"encoding\": \"cp1252\",\n    \"encoding_aliases\": [\n        \"1252\",\n        \"windows_1252\"\n    ],\n    \"alternative_encodings\": [\n        \"cp1254\",\n        \"cp1256\",\n        \"cp1258\",\n        \"iso8859_14\",\n        \"iso8859_15\",\n        \"iso8859_16\",\n        \"iso8859_3\",\n        \"iso8859_9\",\n        \"latin_1\",\n        \"mbcs\"\n    ],\n    \"language\": \"French\",\n    \"alphabets\": [\n        \"Basic Latin\",\n        \"Latin-1 Supplement\"\n    ],\n    \"has_sig_or_bom\": false,\n    \"chaos\": 0.149,\n    \"coherence\": 97.152,\n    \"unicode_path\": null,\n    \"is_preferred\": true\n}\n```\n\n### Python\n*Just print out normalized text*\n```python\nfrom charset_normalizer import from_path\n\nresults = from_path('./my_subtitle.srt')\n\nprint(str(results.best()))\n```\n\n*Upgrade your code without effort*\n```python\nfrom charset_normalizer import detect\n```\n\nThe above code will behave the same as **chardet**. We ensure that we offer the best (reasonable) BC result possible.\n\nSee the docs for advanced usage : [readthedocs.io](https://charset-normalizer.readthedocs.io/en/latest/)\n\n## üòá Why\n\nWhen I started using Chardet, I noticed that it was not suited to my expectations, and I wanted to propose a\nreliable alternative using a completely different method. Also! I never back down on a good challenge!\n\nI **don't care** about the **originating charset** encoding, because **two different tables** can\nproduce **two identical rendered string.**\nWhat I want is to get readable text, the best I can.\n\nIn a way, **I'm brute forcing text decoding.** How cool is that ? üòé\n\nDon't confuse package **ftfy** with charset-normalizer or chardet. ftfy goal is to repair Unicode string whereas charset-normalizer to convert raw file in unknown encoding to unicode.\n\n## üç∞ How\n\n  - Discard all charset encoding table that could not fit the binary content.\n  - Measure noise, or the mess once opened (by chunks) with a corresponding charset encoding.\n  - Extract matches with the lowest mess detected.\n  - Additionally, we measure coherence / probe for a language.\n\n**Wait a minute**, what is noise/mess and coherence according to **YOU ?**\n\n*Noise :* I opened hundred of text files, **written by humans**, with the wrong encoding table. **I observed**, then\n**I established** some ground rules about **what is obvious** when **it seems like** a mess (aka. defining noise in rendered text).\n I know that my interpretation of what is noise is probably incomplete, feel free to contribute in order to\n improve or rewrite it.\n\n*Coherence :* For each language there is on earth, we have computed ranked letter appearance occurrences (the best we can). So I thought\nthat intel is worth something here. So I use those records against decoded text to check if I can detect intelligent design.\n\n## ‚ö° Known limitations\n\n  - Language detection is unreliable when text contains two or more languages sharing identical letters. (eg. HTML (english tags) + Turkish content (Sharing Latin characters))\n  - Every charset detector heavily depends on sufficient content. In common cases, do not bother run detection on very tiny content.\n\n## ‚ö†Ô∏è About Python EOLs\n\n**If you are running:**\n\n- Python >=2.7,<3.5: Unsupported\n- Python 3.5: charset-normalizer < 2.1\n- Python 3.6: charset-normalizer < 3.1\n- Python 3.7: charset-normalizer < 4.0\n\nUpgrade your Python interpreter as soon as possible.\n\n## üë§ Contributing\n\nContributions, issues and feature requests are very much welcome.<br />\nFeel free to check [issues page](https://github.com/ousret/charset_normalizer/issues) if you want to contribute.\n\n## üìù License\n\nCopyright ¬© [Ahmed TAHRI @Ousret](https://github.com/Ousret).<br />\nThis project is [MIT](https://github.com/Ousret/charset_normalizer/blob/master/LICENSE) licensed.\n\nCharacters frequencies used in this project ¬© 2012 [Denny Vrandeƒçiƒá](http://simia.net/letters/)\n\n## üíº For Enterprise\n\nProfessional support for charset-normalizer is available as part of the [Tidelift\nSubscription][1]. Tidelift gives software development teams a single source for\npurchasing and maintaining their software, with professional grade assurances\nfrom the experts who know it best, while seamlessly integrating with existing\ntools.\n\n[1]: https://tidelift.com/subscription/pkg/pypi-charset-normalizer?utm_source=pypi-charset-normalizer&utm_medium=readme\n\n[![OpenSSF Best Practices](https://www.bestpractices.dev/projects/7297/badge)](https://www.bestpractices.dev/projects/7297)\n\n# Changelog\nAll notable changes to charset-normalizer will be documented in this file. This project adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0.html).\nThe format is based on [Keep a Changelog](https://keepachangelog.com/en/1.0.0/).\n\n## [3.4.4](https://github.com/Ousret/charset_normalizer/compare/3.4.2...3.4.4) (2025-10-13)\n\n### Changed\n- Bound `setuptools` to a specific constraint `setuptools>=68,<=81`.\n- Raised upper bound of mypyc for the optional pre-built extension to v1.18.2\n\n### Removed\n- `setuptools-scm` as a build dependency.\n\n### Misc\n- Enforced hashes in `dev-requirements.txt` and created `ci-requirements.txt` for security purposes.\n- Additional pre-built wheels for riscv64, s390x, and armv7l architectures.\n- Restore ` multiple.intoto.jsonl` in GitHub releases in addition to individual attestation file per wheel.\n\n## [3.4.3](https://github.com/Ousret/charset_normalizer/compare/3.4.2...3.4.3) (2025-08-09)\n\n### Changed\n- mypy(c) is no longer a required dependency at build time if `CHARSET_NORMALIZER_USE_MYPYC` isn't set to `1`. (#595) (#583)\n- automatically lower confidence on small bytes samples that are not Unicode in `detect` output legacy function. (#391)\n\n### Added\n- Custom build backend to overcome inability to mark mypy as an optional dependency in the build phase.\n- Support for Python 3.14\n\n### Fixed\n- sdist archive contained useless directories.\n- automatically fallback on valid UTF-16 or UTF-32 even if the md says it's noisy. (#633)\n\n### Misc\n- SBOM are automatically published to the relevant GitHub release to comply with regulatory changes.\n  Each published wheel comes with its SBOM. We choose CycloneDX as the format.\n- Prebuilt optimized wheel are no longer distributed by default for CPython 3.7 due to a change in cibuildwheel.\n\n## [3.4.2](https://github.com/Ousret/charset_normalizer/compare/3.4.1...3.4.2) (2025-05-02)\n\n### Fixed\n- Addressed the DeprecationWarning in our CLI regarding `argparse.FileType` by backporting the target class into the package. (#591)\n- Improved the overall reliability of the detector with CJK Ideographs. (#605) (#587)\n\n### Changed\n- Optional mypyc compilation upgraded to version 1.15 for Python >= 3.8\n\n## [3.4.1](https://github.com/Ousret/charset_normalizer/compare/3.4.0...3.4.1) (2024-12-24)\n\n### Changed\n- Project metadata are now stored using `pyproject.toml` instead of `setup.cfg` using setuptools as the build backend.\n- Enforce annotation delayed loading for a simpler and consistent types in the project.\n- Optional mypyc compilation upgraded to version 1.14 for Python >= 3.8\n\n### Added\n- pre-commit configuration.\n- noxfile.\n\n### Removed\n- `build-requirements.txt` as per using `pyproject.toml` native build configuration.\n- `bin/integration.py` and `bin/serve.py` in favor of downstream integration test (see noxfile).\n- `setup.cfg` in favor of `pyproject.toml` metadata configuration.\n- Unused `utils.range_scan` function.\n\n### Fixed\n- Converting content to Unicode bytes may insert `utf_8` instead of preferred `utf-8`. (#572)\n- Deprecation warning \"'count' is passed as positional argument\" when converting to Unicode bytes on Python 3.13+\n\n## [3.4.0](https://github.com/Ousret/charset_normalizer/compare/3.3.2...3.4.0) (2024-10-08)\n\n### Added\n- Argument `--no-preemptive` in the CLI to prevent the detector to search for hints.\n- Support for Python 3.13 (#512)\n\n### Fixed\n- Relax the TypeError exception thrown when trying to compare a CharsetMatch with anything else than a CharsetMatch.\n- Improved the general reliability of the detector based on user feedbacks. (#520) (#509) (#498) (#407) (#537)\n- Declared charset in content (preemptive detection) not changed when converting to utf-8 bytes. (#381)\n\n## [3.3.2](https://github.com/Ousret/charset_normalizer/compare/3.3.1...3.3.2) (2023-10-31)\n\n### Fixed\n- Unintentional memory usage regression when using large payload that match several encoding (#376)\n- Regression on some detection case showcased in the documentation (#371)\n\n### Added\n- Noise (md) probe that identify malformed arabic representation due to the presence of letters in isolated form (credit to my wife)\n\n## [3.3.1](https://github.com/Ousret/charset_normalizer/compare/3.3.0...3.3.1) (2023-10-22)\n\n### Changed\n- Optional mypyc compilation upgraded to version 1.6.1 for Python >= 3.8\n- Improved the general detection reliability based on reports from the community\n\n## [3.3.0](https://github.com/Ousret/charset_normalizer/compare/3.2.0...3.3.0) (2023-09-30)\n\n### Added\n- Allow to execute the CLI (e.g. normalizer) through `python -m charset_normalizer.cli` or `python -m charset_normalizer`\n- Support for 9 forgotten encoding that are supported by Python but unlisted in `encoding.aliases` as they have no alias (#323)\n\n### Removed\n- (internal) Redundant utils.is_ascii function and unused function is_private_use_only\n- (internal) charset_normalizer.assets is moved inside charset_normalizer.constant\n\n### Changed\n- (internal) Unicode code blocks in constants are updated using the latest v15.0.0 definition to improve detection\n- Optional mypyc compilation upgraded to version 1.5.1 for Python >= 3.8\n\n### Fixed\n- Unable to properly sort CharsetMatch when both chaos/noise and coherence were close due to an unreachable condition in \\_\\_lt\\_\\_ (#350)\n\n## [3.2.0](https://github.com/Ousret/charset_normalizer/compare/3.1.0...3.2.0) (2023-06-07)\n\n### Changed\n- Typehint for function `from_path` no longer enforce `PathLike` as its first argument\n- Minor improvement over the global detection reliability\n\n### Added\n- Introduce function `is_binary` that relies on main capabilities, and optimized to detect binaries\n- Propagate `enable_fallback` argument throughout `from_bytes`, `from_path`, and `from_fp` that allow a deeper control over the detection (default True)\n- Explicit support for Python 3.12\n\n### Fixed\n- Edge case detection failure where a file would contain 'very-long' camel cased word (Issue #289)\n\n## [3.1.0](https://github.com/Ousret/charset_normalizer/compare/3.0.1...3.1.0) (2023-03-06)\n\n### Added\n- Argument `should_rename_legacy` for legacy function `detect` and disregard any new arguments without errors (PR #262)\n\n### Removed\n- Support for Python 3.6 (PR #260)\n\n### Changed\n- Optional speedup provided by mypy/c 1.0.1\n\n## [3.0.1](https://github.com/Ousret/charset_normalizer/compare/3.0.0...3.0.1) (2022-11-18)\n\n### Fixed\n- Multi-bytes cutter/chunk generator did not always cut correctly (PR #233)\n\n### Changed\n- Speedup provided by mypy/c 0.990 on Python >= 3.7\n\n## [3.0.0](https://github.com/Ousret/charset_normalizer/compare/2.1.1...3.0.0) (2022-10-20)\n\n### Added\n- Extend the capability of explain=True when cp_isolation contains at most two entries (min one), will log in details of the Mess-detector results\n- Support for alternative language frequency set in charset_normalizer.assets.FREQUENCIES\n- Add parameter `language_threshold` in `from_bytes`, `from_path` and `from_fp` to adjust the minimum expected coherence ratio\n- `normalizer --version` now specify if current version provide extra speedup (meaning mypyc compilation whl)\n\n### Changed\n- Build with static metadata using 'build' frontend\n- Make the language detection stricter\n- Optional: Module `md.py` can be compiled using Mypyc to provide an extra speedup up to 4x faster than v2.1\n\n### Fixed\n- CLI with opt --normalize fail when using full path for files\n- TooManyAccentuatedPlugin induce false positive on the mess detection when too few alpha character have been fed to it\n- Sphinx warnings when generating the documentation\n\n### Removed\n- Coherence detector no longer return 'Simple English' instead return 'English'\n- Coherence detector no longer return 'Classical Chinese' instead return 'Chinese'\n- Breaking: Method `first()` and `best()` from CharsetMatch\n- UTF-7 will no longer appear as \"detected\" without a recognized SIG/mark (is unreliable/conflict with ASCII)\n- Breaking: Class aliases CharsetDetector, CharsetDoctor, CharsetNormalizerMatch and CharsetNormalizerMatches\n- Breaking: Top-level function `normalize`\n- Breaking: Properties `chaos_secondary_pass`, `coherence_non_latin` and `w_counter` from CharsetMatch\n- Support for the backport `unicodedata2`\n\n## [3.0.0rc1](https://github.com/Ousret/charset_normalizer/compare/3.0.0b2...3.0.0rc1) (2022-10-18)\n\n### Added\n- Extend the capability of explain=True when cp_isolation contains at most two entries (min one), will log in details of the Mess-detector results\n- Support for alternative language frequency set in charset_normalizer.assets.FREQUENCIES\n- Add parameter `language_threshold` in `from_bytes`, `from_path` and `from_fp` to adjust the minimum expected coherence ratio\n\n### Changed\n- Build with static metadata using 'build' frontend\n- Make the language detection stricter\n\n### Fixed\n- CLI with opt --normalize fail when using full path for files\n- TooManyAccentuatedPlugin induce false positive on the mess detection when too few alpha character have been fed to it\n\n### Removed\n- Coherence detector no longer return 'Simple English' instead return 'English'\n- Coherence detector no longer return 'Classical Chinese' instead return 'Chinese'\n\n## [3.0.0b2](https://github.com/Ousret/charset_normalizer/compare/3.0.0b1...3.0.0b2) (2022-08-21)\n\n### Added\n- `normalizer --version` now specify if current version provide extra speedup (meaning mypyc compilation whl)\n\n### Removed\n- Breaking: Method `first()` and `best()` from CharsetMatch\n- UTF-7 will no longer appear as \"detected\" without a recognized SIG/mark (is unreliable/conflict with ASCII)\n\n### Fixed\n- Sphinx warnings when generating the documentation\n\n## [3.0.0b1](https://github.com/Ousret/charset_normalizer/compare/2.1.0...3.0.0b1) (2022-08-15)\n\n### Changed\n- Optional: Module `md.py` can be compiled using Mypyc to provide an extra speedup up to 4x faster than v2.1\n\n### Removed\n- Breaking: Class aliases CharsetDetector, CharsetDoctor, CharsetNormalizerMatch and CharsetNormalizerMatches\n- Breaking: Top-level function `normalize`\n- Breaking: Properties `chaos_secondary_pass`, `coherence_non_latin` and `w_counter` from CharsetMatch\n- Support for the backport `unicodedata2`\n\n## [2.1.1](https://github.com/Ousret/charset_normalizer/compare/2.1.0...2.1.1) (2022-08-19)\n\n### Deprecated\n- Function `normalize` scheduled for removal in 3.0\n\n### Changed\n- Removed useless call to decode in fn is_unprintable (#206)\n\n### Fixed\n- Third-party library (i18n xgettext) crashing not recognizing utf_8 (PEP 263) with underscore from [@aleksandernovikov](https://github.com/aleksandernovikov) (#204)\n\n## [2.1.0](https://github.com/Ousret/charset_normalizer/compare/2.0.12...2.1.0) (2022-06-19)\n\n### Added\n- Output the Unicode table version when running the CLI with `--version` (PR #194)\n\n### Changed\n- Re-use decoded buffer for single byte character sets from [@nijel](https://github.com/nijel) (PR #175)\n- Fixing some performance bottlenecks from [@deedy5](https://github.com/deedy5) (PR #183)\n\n### Fixed\n- Workaround potential bug in cpython with Zero Width No-Break Space located in Arabic Presentation Forms-B, Unicode 1.1 not acknowledged as space (PR #175)\n- CLI default threshold aligned with the API threshold from [@oleksandr-kuzmenko](https://github.com/oleksandr-kuzmenko) (PR #181)\n\n### Removed\n- Support for Python 3.5 (PR #192)\n\n### Deprecated\n- Use of backport unicodedata from `unicodedata2` as Python is quickly catching up, scheduled for removal in 3.0 (PR #194)\n\n## [2.0.12](https://github.com/Ousret/charset_normalizer/compare/2.0.11...2.0.12) (2022-02-12)\n\n### Fixed\n- ASCII miss-detection on rare cases (PR #170)\n\n## [2.0.11](https://github.com/Ousret/charset_normalizer/compare/2.0.10...2.0.11) (2022-01-30)\n\n### Added\n- Explicit support for Python 3.11 (PR #164)\n\n### Changed\n- The logging behavior have been completely reviewed, now using only TRACE and DEBUG levels (PR #163 #165)\n\n## [2.0.10](https://github.com/Ousret/charset_normalizer/compare/2.0.9...2.0.10) (2022-01-04)\n\n### Fixed\n- Fallback match entries might lead to UnicodeDecodeError for large bytes sequence (PR #154)\n\n### Changed\n- Skipping the language-detection (CD) on ASCII (PR #155)\n\n## [2.0.9](https://github.com/Ousret/charset_normalizer/compare/2.0.8...2.0.9) (2021-12-03)\n\n### Changed\n- Moderating the logging impact (since 2.0.8) for specific environments (PR #147)\n\n### Fixed\n- Wrong logging level applied when setting kwarg `explain` to True (PR #146)\n\n## [2.0.8](https://github.com/Ousret/charset_normalizer/compare/2.0.7...2.0.8) (2021-11-24)\n### Changed\n- Improvement over Vietnamese detection (PR #126)\n- MD improvement on trailing data and long foreign (non-pure latin) data (PR #124)\n- Efficiency improvements in cd/alphabet_languages from [@adbar](https://github.com/adbar) (PR #122)\n- call sum() without an intermediary list following PEP 289 recommendations from [@adbar](https://github.com/adbar) (PR #129)\n- Code style as refactored by Sourcery-AI (PR #131)\n- Minor adjustment on the MD around european words (PR #133)\n- Remove and replace SRTs from assets / tests (PR #139)\n- Initialize the library logger with a `NullHandler` by default from [@nmaynes](https://github.com/nmaynes) (PR #135)\n- Setting kwarg `explain` to True will add provisionally (bounded to function lifespan) a specific stream handler (PR #135)\n\n### Fixed\n- Fix large (misleading) sequence giving UnicodeDecodeError (PR #137)\n- Avoid using too insignificant chunk (PR #137)\n\n### Added\n- Add and expose function `set_logging_handler` to configure a specific StreamHandler from [@nmaynes](https://github.com/nmaynes) (PR #135)\n- Add `CHANGELOG.md` entries, format is based on [Keep a Changelog](https://keepachangelog.com/en/1.0.0/) (PR #141)\n\n## [2.0.7](https://github.com/Ousret/charset_normalizer/compare/2.0.6...2.0.7) (2021-10-11)\n### Added\n- Add support for Kazakh (Cyrillic) language detection (PR #109)\n\n### Changed\n- Further, improve inferring the language from a given single-byte code page (PR #112)\n- Vainly trying to leverage PEP263 when PEP3120 is not supported (PR #116)\n- Refactoring for potential performance improvements in loops from [@adbar](https://github.com/adbar) (PR #113)\n- Various detection improvement (MD+CD) (PR #117)\n\n### Removed\n- Remove redundant logging entry about detected language(s) (PR #115)\n\n### Fixed\n- Fix a minor inconsistency between Python 3.5 and other versions regarding language detection (PR #117 #102)\n\n## [2.0.6](https://github.com/Ousret/charset_normalizer/compare/2.0.5...2.0.6) (2021-09-18)\n### Fixed\n- Unforeseen regression with the loss of the backward-compatibility with some older minor of Python 3.5.x (PR #100)\n- Fix CLI crash when using --minimal output in certain cases (PR #103)\n\n### Changed\n- Minor improvement to the detection efficiency (less than 1%) (PR #106 #101)\n\n## [2.0.5](https://github.com/Ousret/charset_normalizer/compare/2.0.4...2.0.5) (2021-09-14)\n### Changed\n- The project now comply with: flake8, mypy, isort and black to ensure a better overall quality (PR #81)\n- The BC-support with v1.x was improved, the old staticmethods are restored (PR #82)\n- The Unicode detection is slightly improved (PR #93)\n- Add syntax sugar \\_\\_bool\\_\\_ for results CharsetMatches list-container (PR #91)\n\n### Removed\n- The project no longer raise warning on tiny content given for detection, will be simply logged as warning instead (PR #92)\n\n### Fixed\n- In some rare case, the chunks extractor could cut in the middle of a multi-byte character and could mislead the mess detection (PR #95)\n- Some rare 'space' characters could trip up the UnprintablePlugin/Mess detection (PR #96)\n- The MANIFEST.in was not exhaustive (PR #78)\n\n## [2.0.4](https://github.com/Ousret/charset_normalizer/compare/2.0.3...2.0.4) (2021-07-30)\n### Fixed\n- The CLI no longer raise an unexpected exception when no encoding has been found (PR #70)\n- Fix accessing the 'alphabets' property when the payload contains surrogate characters (PR #68)\n- The logger could mislead (explain=True) on detected languages and the impact of one MBCS match (PR #72)\n- Submatch factoring could be wrong in rare edge cases (PR #72)\n- Multiple files given to the CLI were ignored when publishing results to STDOUT. (After the first path) (PR #72)\n- Fix line endings from CRLF to LF for certain project files (PR #67)\n\n### Changed\n- Adjust the MD to lower the sensitivity, thus improving the global detection reliability (PR #69 #76)\n- Allow fallback on specified encoding if any (PR #71)\n\n## [2.0.3](https://github.com/Ousret/charset_normalizer/compare/2.0.2...2.0.3) (2021-07-16)\n### Changed\n- Part of the detection mechanism has been improved to be less sensitive, resulting in more accurate detection results. Especially ASCII. (PR #63)\n- According to the community wishes, the detection will fall back on ASCII or UTF-8 in a last-resort case. (PR #64)\n\n## [2.0.2](https://github.com/Ousret/charset_normalizer/compare/2.0.1...2.0.2) (2021-07-15)\n### Fixed\n- Empty/Too small JSON payload miss-detection fixed. Report from [@tseaver](https://github.com/tseaver) (PR #59)\n\n### Changed\n- Don't inject unicodedata2 into sys.modules from [@akx](https://github.com/akx) (PR #57)\n\n## [2.0.1](https://github.com/Ousret/charset_normalizer/compare/2.0.0...2.0.1) (2021-07-13)\n### Fixed\n- Make it work where there isn't a filesystem available, dropping assets frequencies.json. Report from [@sethmlarson](https://github.com/sethmlarson). (PR #55)\n- Using explain=False permanently disable the verbose output in the current runtime (PR #47)\n- One log entry (language target preemptive) was not show in logs when using explain=True (PR #47)\n- Fix undesired exception (ValueError) on getitem of instance CharsetMatches (PR #52)\n\n### Changed\n- Public function normalize default args values were not aligned with from_bytes (PR #53)\n\n### Added\n- You may now use charset aliases in cp_isolation and cp_exclusion arguments (PR #47)\n\n## [2.0.0](https://github.com/Ousret/charset_normalizer/compare/1.4.1...2.0.0) (2021-07-02)\n### Changed\n- 4x to 5 times faster than the previous 1.4.0 release. At least 2x faster than Chardet.\n- Accent has been made on UTF-8 detection, should perform rather instantaneous.\n- The backward compatibility with Chardet has been greatly improved. The legacy detect function returns an identical charset name whenever possible.\n- The detection mechanism has been slightly improved, now Turkish content is detected correctly (most of the time)\n- The program has been rewritten to ease the readability and maintainability. (+Using static typing)+\n- utf_7 detection has been reinstated.\n\n### Removed\n- This package no longer require anything when used with Python 3.5 (Dropped cached_property)\n- Removed support for these languages: Catalan, Esperanto, Kazakh, Baque, Volap√ºk, Azeri, Galician, Nynorsk, Macedonian, and Serbocroatian.\n- The exception hook on UnicodeDecodeError has been removed.\n\n### Deprecated\n- Methods coherence_non_latin, w_counter, chaos_secondary_pass of the class CharsetMatch are now deprecated and scheduled for removal in v3.0\n\n### Fixed\n- The CLI output used the relative path of the file(s). Should be absolute.\n\n## [1.4.1](https://github.com/Ousret/charset_normalizer/compare/1.4.0...1.4.1) (2021-05-28)\n### Fixed\n- Logger configuration/usage no longer conflict with others (PR #44)\n\n## [1.4.0](https://github.com/Ousret/charset_normalizer/compare/1.3.9...1.4.0) (2021-05-21)\n### Removed\n- Using standard logging instead of using the package loguru.\n- Dropping nose test framework in favor of the maintained pytest.\n- Choose to not use dragonmapper package to help with gibberish Chinese/CJK text.\n- Require cached_property only for Python 3.5 due to constraint. Dropping for every other interpreter version.\n- Stop support for UTF-7 that does not contain a SIG.\n- Dropping PrettyTable, replaced with pure JSON output in CLI.\n\n### Fixed\n- BOM marker in a CharsetNormalizerMatch instance could be False in rare cases even if obviously present. Due to the sub-match factoring process.\n- Not searching properly for the BOM when trying utf32/16 parent codec.\n\n### Changed\n- Improving the package final size by compressing frequencies.json.\n- Huge improvement over the larges payload.\n\n### Added\n- CLI now produces JSON consumable output.\n- Return ASCII if given sequences fit. Given reasonable confidence.\n\n## [1.3.9](https://github.com/Ousret/charset_normalizer/compare/1.3.8...1.3.9) (2021-05-13)\n\n### Fixed\n- In some very rare cases, you may end up getting encode/decode errors due to a bad bytes payload (PR #40)\n\n## [1.3.8](https://github.com/Ousret/charset_normalizer/compare/1.3.7...1.3.8) (2021-05-12)\n\n### Fixed\n- Empty given payload for detection may cause an exception if trying to access the `alphabets` property. (PR #39)\n\n## [1.3.7](https://github.com/Ousret/charset_normalizer/compare/1.3.6...1.3.7) (2021-05-12)\n\n### Fixed\n- The legacy detect function should return UTF-8-SIG if sig is present in the payload. (PR #38)\n\n## [1.3.6](https://github.com/Ousret/charset_normalizer/compare/1.3.5...1.3.6) (2021-02-09)\n\n### Changed\n- Amend the previous release to allow prettytable 2.0 (PR #35)\n\n## [1.3.5](https://github.com/Ousret/charset_normalizer/compare/1.3.4...1.3.5) (2021-02-08)\n\n### Fixed\n- Fix error while using the package with a python pre-release interpreter (PR #33)\n\n### Changed\n- Dependencies refactoring, constraints revised.\n\n### Added\n- Add python 3.9 and 3.10 to the supported interpreters\n\nMIT License\n\nCopyright (c) 2025 TAHRI Ahmed R.\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n"
      }
    },
    {
      "download_info": {
        "url": "https://pypi.tuna.tsinghua.edu.cn/packages/0e/61/66938bbb5fc52dbdf84594873d5b51fb1f7c7794e9c0f5bd885f30bc507b/idna-3.11-py3-none-any.whl",
        "archive_info": {
          "hash": "sha256=771a87f49d9defaf64091e6e6fe9c18d4833f140bd19464795bc32d966ca37ea",
          "hashes": {
            "sha256": "771a87f49d9defaf64091e6e6fe9c18d4833f140bd19464795bc32d966ca37ea"
          }
        }
      },
      "is_direct": false,
      "is_yanked": false,
      "requested": false,
      "metadata": {
        "metadata_version": "2.4",
        "name": "idna",
        "version": "3.11",
        "summary": "Internationalized Domain Names in Applications (IDNA)",
        "description_content_type": "text/x-rst",
        "author_email": "Kim Davies <kim+pypi@gumleaf.org>",
        "license_expression": "BSD-3-Clause",
        "license_file": [
          "LICENSE.md"
        ],
        "classifier": [
          "Development Status :: 5 - Production/Stable",
          "Intended Audience :: Developers",
          "Intended Audience :: System Administrators",
          "Operating System :: OS Independent",
          "Programming Language :: Python",
          "Programming Language :: Python :: 3",
          "Programming Language :: Python :: 3 :: Only",
          "Programming Language :: Python :: 3.8",
          "Programming Language :: Python :: 3.9",
          "Programming Language :: Python :: 3.10",
          "Programming Language :: Python :: 3.11",
          "Programming Language :: Python :: 3.12",
          "Programming Language :: Python :: 3.13",
          "Programming Language :: Python :: 3.14",
          "Programming Language :: Python :: Implementation :: CPython",
          "Programming Language :: Python :: Implementation :: PyPy",
          "Topic :: Internet :: Name Service (DNS)",
          "Topic :: Software Development :: Libraries :: Python Modules",
          "Topic :: Utilities"
        ],
        "requires_dist": [
          "ruff >= 0.6.2 ; extra == \"all\"",
          "mypy >= 1.11.2 ; extra == \"all\"",
          "pytest >= 8.3.2 ; extra == \"all\"",
          "flake8 >= 7.1.1 ; extra == \"all\""
        ],
        "requires_python": ">=3.8",
        "project_url": [
          "Changelog, https://github.com/kjd/idna/blob/master/HISTORY.rst",
          "Issue tracker, https://github.com/kjd/idna/issues",
          "Source, https://github.com/kjd/idna"
        ],
        "provides_extra": [
          "all"
        ],
        "description": "Internationalized Domain Names in Applications (IDNA)\n=====================================================\n\nSupport for `Internationalized Domain Names in\nApplications (IDNA) <https://tools.ietf.org/html/rfc5891>`_\nand `Unicode IDNA Compatibility Processing\n<https://unicode.org/reports/tr46/>`_.\n\nThe latest versions of these standards supplied here provide\nmore comprehensive language coverage and reduce the potential of\nallowing domains with known security vulnerabilities. This library\nis a suitable replacement for the ‚Äúencodings.idna‚Äù\nmodule that comes with the Python standard library, but which\nonly supports an older superseded IDNA specification from 2003.\n\nBasic functions are simply executed:\n\n.. code-block:: pycon\n\n    >>> import idna\n    >>> idna.encode('„Éâ„É°„Ç§„É≥.„ÉÜ„Çπ„Éà')\n    b'xn--eckwd4c7c.xn--zckzah'\n    >>> print(idna.decode('xn--eckwd4c7c.xn--zckzah'))\n    „Éâ„É°„Ç§„É≥.„ÉÜ„Çπ„Éà\n\n\nInstallation\n------------\n\nThis package is available for installation from PyPI via the\ntypical mechanisms, such as:\n\n.. code-block:: bash\n\n    $ python3 -m pip install idna\n\n\nUsage\n-----\n\nFor typical usage, the ``encode`` and ``decode`` functions will take a\ndomain name argument and perform a conversion to ASCII compatible encoding\n(known as A-labels), or to Unicode strings (known as U-labels)\nrespectively.\n\n.. code-block:: pycon\n\n    >>> import idna\n    >>> idna.encode('„Éâ„É°„Ç§„É≥.„ÉÜ„Çπ„Éà')\n    b'xn--eckwd4c7c.xn--zckzah'\n    >>> print(idna.decode('xn--eckwd4c7c.xn--zckzah'))\n    „Éâ„É°„Ç§„É≥.„ÉÜ„Çπ„Éà\n\nConversions can be applied at a per-label basis using the ``ulabel`` or\n``alabel`` functions if necessary:\n\n.. code-block:: pycon\n\n    >>> idna.alabel('ÊµãËØï')\n    b'xn--0zwm56d'\n\n\nCompatibility Mapping (UTS #46)\n+++++++++++++++++++++++++++++++\n\nThis library provides support for `Unicode IDNA Compatibility\nProcessing <https://unicode.org/reports/tr46/>`_ which normalizes input from\ndifferent potential ways a user may input a domain prior to performing the IDNA\nconversion operations. This functionality, known as a \n`mapping <https://tools.ietf.org/html/rfc5895>`_, is considered by the\nspecification to be a local user-interface issue distinct from IDNA\nconversion functionality.\n\nFor example, ‚ÄúK√∂nigsg√§√üchen‚Äù is not a permissible label as *LATIN\nCAPITAL LETTER K* is not allowed (nor are capital letters in general).\nUTS 46 will convert this into lower case prior to applying the IDNA\nconversion.\n\n.. code-block:: pycon\n\n    >>> import idna\n    >>> idna.encode('K√∂nigsg√§√üchen')\n    ...\n    idna.core.InvalidCodepoint: Codepoint U+004B at position 1 of 'K√∂nigsg√§√üchen' not allowed\n    >>> idna.encode('K√∂nigsg√§√üchen', uts46=True)\n    b'xn--knigsgchen-b4a3dun'\n    >>> print(idna.decode('xn--knigsgchen-b4a3dun'))\n    k√∂nigsg√§√üchen\n\n\nExceptions\n----------\n\nAll errors raised during the conversion following the specification\nshould raise an exception derived from the ``idna.IDNAError`` base\nclass.\n\nMore specific exceptions that may be generated as ``idna.IDNABidiError``\nwhen the error reflects an illegal combination of left-to-right and\nright-to-left characters in a label; ``idna.InvalidCodepoint`` when\na specific codepoint is an illegal character in an IDN label (i.e.\nINVALID); and ``idna.InvalidCodepointContext`` when the codepoint is\nillegal based on its position in the string (i.e. it is CONTEXTO or CONTEXTJ\nbut the contextual requirements are not satisfied.)\n\nBuilding and Diagnostics\n------------------------\n\nThe IDNA and UTS 46 functionality relies upon pre-calculated lookup\ntables for performance. These tables are derived from computing against\neligibility criteria in the respective standards using the command-line\nscript ``tools/idna-data``.\n\nThis tool will fetch relevant codepoint data from the Unicode repository\nand perform the required calculations to identify eligibility. There are\nthree main modes:\n\n* ``idna-data make-libdata``. Generates ``idnadata.py`` and\n  ``uts46data.py``, the pre-calculated lookup tables used for IDNA and\n  UTS 46 conversions. Implementers who wish to track this library against\n  a different Unicode version may use this tool to manually generate a\n  different version of the ``idnadata.py`` and ``uts46data.py`` files.\n\n* ``idna-data make-table``. Generate a table of the IDNA disposition\n  (e.g. PVALID, CONTEXTJ, CONTEXTO) in the format found in Appendix\n  B.1 of RFC 5892 and the pre-computed tables published by `IANA\n  <https://www.iana.org/>`_.\n\n* ``idna-data U+0061``. Prints debugging output on the various\n  properties associated with an individual Unicode codepoint (in this\n  case, U+0061), that are used to assess the IDNA and UTS 46 status of a\n  codepoint. This is helpful in debugging or analysis.\n\nThe tool accepts a number of arguments, described using ``idna-data\n-h``. Most notably, the ``--version`` argument allows the specification\nof the version of Unicode to be used in computing the table data. For\nexample, ``idna-data --version 9.0.0 make-libdata`` will generate\nlibrary data against Unicode 9.0.0.\n\n\nAdditional Notes\n----------------\n\n* **Packages**. The latest tagged release version is published in the\n  `Python Package Index <https://pypi.org/project/idna/>`_.\n\n* **Version support**. This library supports Python 3.8 and higher.\n  As this library serves as a low-level toolkit for a variety of\n  applications, many of which strive for broad compatibility with older\n  Python versions, there is no rush to remove older interpreter support.\n  Support for older versions are likely to be removed from new releases\n  as automated tests can no longer easily be run, i.e. once the Python\n  version is officially end-of-life.\n\n* **Testing**. The library has a test suite based on each rule of the\n  IDNA specification, as well as tests that are provided as part of the\n  Unicode Technical Standard 46, `Unicode IDNA Compatibility Processing\n  <https://unicode.org/reports/tr46/>`_.\n\n* **Emoji**. It is an occasional request to support emoji domains in\n  this library. Encoding of symbols like emoji is expressly prohibited by\n  the technical standard IDNA 2008 and emoji domains are broadly phased\n  out across the domain industry due to associated security risks. For\n  now, applications that need to support these non-compliant labels\n  may wish to consider trying the encode/decode operation in this library\n  first, and then falling back to using `encodings.idna`. See `the Github\n  project <https://github.com/kjd/idna/issues/18>`_ for more discussion.\n\n* **Transitional processing**. Unicode 16.0.0 removed transitional\n  processing so the `transitional` argument for the encode() method\n  no longer has any effect and will be removed at a later date.\n\n"
      }
    },
    {
      "download_info": {
        "url": "https://pypi.tuna.tsinghua.edu.cn/packages/6d/b9/4095b668ea3678bf6a0af005527f39de12fb026516fb3df17495a733b7f8/urllib3-2.6.2-py3-none-any.whl",
        "archive_info": {
          "hash": "sha256=ec21cddfe7724fc7cb4ba4bea7aa8e2ef36f607a4bab81aa6ce42a13dc3f03dd",
          "hashes": {
            "sha256": "ec21cddfe7724fc7cb4ba4bea7aa8e2ef36f607a4bab81aa6ce42a13dc3f03dd"
          }
        }
      },
      "is_direct": false,
      "is_yanked": false,
      "requested": false,
      "metadata": {
        "metadata_version": "2.4",
        "name": "urllib3",
        "version": "2.6.2",
        "summary": "HTTP library with thread-safe connection pooling, file post, and more.",
        "description_content_type": "text/markdown",
        "keywords": [
          "filepost",
          "http",
          "httplib",
          "https",
          "pooling",
          "ssl",
          "threadsafe",
          "urllib"
        ],
        "author_email": "Andrey Petrov <andrey.petrov@shazow.net>",
        "maintainer_email": "Seth Michael Larson <sethmichaellarson@gmail.com>, Quentin Pradet <quentin@pradet.me>, Illia Volochii <illia.volochii@gmail.com>",
        "license_expression": "MIT",
        "license_file": [
          "LICENSE.txt"
        ],
        "classifier": [
          "Environment :: Web Environment",
          "Intended Audience :: Developers",
          "Operating System :: OS Independent",
          "Programming Language :: Python",
          "Programming Language :: Python :: 3",
          "Programming Language :: Python :: 3 :: Only",
          "Programming Language :: Python :: 3.9",
          "Programming Language :: Python :: 3.10",
          "Programming Language :: Python :: 3.11",
          "Programming Language :: Python :: 3.12",
          "Programming Language :: Python :: 3.13",
          "Programming Language :: Python :: 3.14",
          "Programming Language :: Python :: Free Threading :: 2 - Beta",
          "Programming Language :: Python :: Implementation :: CPython",
          "Programming Language :: Python :: Implementation :: PyPy",
          "Topic :: Internet :: WWW/HTTP",
          "Topic :: Software Development :: Libraries"
        ],
        "requires_dist": [
          "brotli>=1.2.0; (platform_python_implementation == 'CPython') and extra == 'brotli'",
          "brotlicffi>=1.2.0.0; (platform_python_implementation != 'CPython') and extra == 'brotli'",
          "h2<5,>=4; extra == 'h2'",
          "pysocks!=1.5.7,<2.0,>=1.5.6; extra == 'socks'",
          "backports-zstd>=1.0.0; (python_version < '3.14') and extra == 'zstd'"
        ],
        "requires_python": ">=3.9",
        "project_url": [
          "Changelog, https://github.com/urllib3/urllib3/blob/main/CHANGES.rst",
          "Documentation, https://urllib3.readthedocs.io",
          "Code, https://github.com/urllib3/urllib3",
          "Issue tracker, https://github.com/urllib3/urllib3/issues"
        ],
        "provides_extra": [
          "brotli",
          "h2",
          "socks",
          "zstd"
        ],
        "description": "<h1 align=\"center\">\n\n![urllib3](https://github.com/urllib3/urllib3/raw/main/docs/_static/banner_github.svg)\n\n</h1>\n\n<p align=\"center\">\n  <a href=\"https://pypi.org/project/urllib3\"><img alt=\"PyPI Version\" src=\"https://img.shields.io/pypi/v/urllib3.svg?maxAge=86400\" /></a>\n  <a href=\"https://pypi.org/project/urllib3\"><img alt=\"Python Versions\" src=\"https://img.shields.io/pypi/pyversions/urllib3.svg?maxAge=86400\" /></a>\n  <a href=\"https://discord.gg/urllib3\"><img alt=\"Join our Discord\" src=\"https://img.shields.io/discord/756342717725933608?color=%237289da&label=discord\" /></a>\n  <a href=\"https://github.com/urllib3/urllib3/actions?query=workflow%3ACI\"><img alt=\"Coverage Status\" src=\"https://img.shields.io/badge/coverage-100%25-success\" /></a>\n  <a href=\"https://github.com/urllib3/urllib3/actions/workflows/ci.yml?query=branch%3Amain\"><img alt=\"Build Status on GitHub\" src=\"https://github.com/urllib3/urllib3/actions/workflows/ci.yml/badge.svg?branch:main&workflow:CI\" /></a>\n  <a href=\"https://urllib3.readthedocs.io\"><img alt=\"Documentation Status\" src=\"https://readthedocs.org/projects/urllib3/badge/?version=latest\" /></a><br>\n  <a href=\"https://deps.dev/pypi/urllib3\"><img alt=\"OpenSSF Scorecard\" src=\"https://api.securityscorecards.dev/projects/github.com/urllib3/urllib3/badge\" /></a>\n  <a href=\"https://slsa.dev\"><img alt=\"SLSA 3\" src=\"https://slsa.dev/images/gh-badge-level3.svg\" /></a>\n  <a href=\"https://bestpractices.coreinfrastructure.org/projects/6227\"><img alt=\"CII Best Practices\" src=\"https://bestpractices.coreinfrastructure.org/projects/6227/badge\" /></a>\n</p>\n\nurllib3 is a powerful, *user-friendly* HTTP client for Python. Much of the\nPython ecosystem already uses urllib3 and you should too.\nurllib3 brings many critical features that are missing from the Python\nstandard libraries:\n\n- Thread safety.\n- Connection pooling.\n- Client-side SSL/TLS verification.\n- File uploads with multipart encoding.\n- Helpers for retrying requests and dealing with HTTP redirects.\n- Support for gzip, deflate, brotli, and zstd encoding.\n- Proxy support for HTTP and SOCKS.\n- 100% test coverage.\n\nurllib3 is powerful and easy to use:\n\n```python3\n>>> import urllib3\n>>> resp = urllib3.request(\"GET\", \"http://httpbin.org/robots.txt\")\n>>> resp.status\n200\n>>> resp.data\nb\"User-agent: *\\nDisallow: /deny\\n\"\n```\n\n## Installing\n\nurllib3 can be installed with [pip](https://pip.pypa.io):\n\n```bash\n$ python -m pip install urllib3\n```\n\nAlternatively, you can grab the latest source code from [GitHub](https://github.com/urllib3/urllib3):\n\n```bash\n$ git clone https://github.com/urllib3/urllib3.git\n$ cd urllib3\n$ pip install .\n```\n\n\n## Documentation\n\nurllib3 has usage and reference documentation at [urllib3.readthedocs.io](https://urllib3.readthedocs.io).\n\n\n## Community\n\nurllib3 has a [community Discord channel](https://discord.gg/urllib3) for asking questions and\ncollaborating with other contributors. Drop by and say hello üëã\n\n\n## Contributing\n\nurllib3 happily accepts contributions. Please see our\n[contributing documentation](https://urllib3.readthedocs.io/en/latest/contributing.html)\nfor some tips on getting started.\n\n\n## Security Disclosures\n\nTo report a security vulnerability, please use the\n[Tidelift security contact](https://tidelift.com/security).\nTidelift will coordinate the fix and disclosure with maintainers.\n\n\n## Maintainers\n\n- Lead: [@illia-v](https://github.com/illia-v) (Illia Volochii)\n- [@sethmlarson](https://github.com/sethmlarson) (Seth M. Larson)\n- [@pquentin](https://github.com/pquentin) (Quentin Pradet)\n- [@theacodes](https://github.com/theacodes) (Thea Flowers)\n- [@haikuginger](https://github.com/haikuginger) (Jess Shapiro)\n- [@lukasa](https://github.com/lukasa) (Cory Benfield)\n- [@sigmavirus24](https://github.com/sigmavirus24) (Ian Stapleton Cordasco)\n- [@shazow](https://github.com/shazow) (Andrey Petrov)\n\nüëã\n\n\n## Sponsorship\n\nIf your company benefits from this library, please consider [sponsoring its\ndevelopment](https://urllib3.readthedocs.io/en/latest/sponsors.html).\n\n\n## For Enterprise\n\nProfessional support for urllib3 is available as part of the [Tidelift\nSubscription][1].  Tidelift gives software development teams a single source for\npurchasing and maintaining their software, with professional grade assurances\nfrom the experts who know it best, while seamlessly integrating with existing\ntools.\n\n[1]: https://tidelift.com/subscription/pkg/pypi-urllib3?utm_source=pypi-urllib3&utm_medium=referral&utm_campaign=readme\n"
      }
    },
    {
      "download_info": {
        "url": "https://pypi.tuna.tsinghua.edu.cn/packages/70/7d/9bc192684cea499815ff478dfcdc13835ddf401365057044fb721ec6bddb/certifi-2025.11.12-py3-none-any.whl",
        "archive_info": {
          "hash": "sha256=97de8790030bbd5c2d96b7ec782fc2f7820ef8dba6db909ccf95449f2d062d4b",
          "hashes": {
            "sha256": "97de8790030bbd5c2d96b7ec782fc2f7820ef8dba6db909ccf95449f2d062d4b"
          }
        }
      },
      "is_direct": false,
      "is_yanked": false,
      "requested": false,
      "metadata": {
        "metadata_version": "2.4",
        "name": "certifi",
        "version": "2025.11.12",
        "dynamic": [
          "author",
          "author-email",
          "classifier",
          "description",
          "home-page",
          "license",
          "license-file",
          "project-url",
          "requires-python",
          "summary"
        ],
        "summary": "Python package for providing Mozilla's CA Bundle.",
        "home_page": "https://github.com/certifi/python-certifi",
        "author": "Kenneth Reitz",
        "author_email": "me@kennethreitz.com",
        "license": "MPL-2.0",
        "license_file": [
          "LICENSE"
        ],
        "classifier": [
          "Development Status :: 5 - Production/Stable",
          "Intended Audience :: Developers",
          "License :: OSI Approved :: Mozilla Public License 2.0 (MPL 2.0)",
          "Natural Language :: English",
          "Programming Language :: Python",
          "Programming Language :: Python :: 3",
          "Programming Language :: Python :: 3 :: Only",
          "Programming Language :: Python :: 3.7",
          "Programming Language :: Python :: 3.8",
          "Programming Language :: Python :: 3.9",
          "Programming Language :: Python :: 3.10",
          "Programming Language :: Python :: 3.11",
          "Programming Language :: Python :: 3.12",
          "Programming Language :: Python :: 3.13",
          "Programming Language :: Python :: 3.14"
        ],
        "requires_python": ">=3.7",
        "project_url": [
          "Source, https://github.com/certifi/python-certifi"
        ],
        "description": "Certifi: Python SSL Certificates\n================================\n\nCertifi provides Mozilla's carefully curated collection of Root Certificates for\nvalidating the trustworthiness of SSL certificates while verifying the identity\nof TLS hosts. It has been extracted from the `Requests`_ project.\n\nInstallation\n------------\n\n``certifi`` is available on PyPI. Simply install it with ``pip``::\n\n    $ pip install certifi\n\nUsage\n-----\n\nTo reference the installed certificate authority (CA) bundle, you can use the\nbuilt-in function::\n\n    >>> import certifi\n\n    >>> certifi.where()\n    '/usr/local/lib/python3.7/site-packages/certifi/cacert.pem'\n\nOr from the command line::\n\n    $ python -m certifi\n    /usr/local/lib/python3.7/site-packages/certifi/cacert.pem\n\nEnjoy!\n\n.. _`Requests`: https://requests.readthedocs.io/en/master/\n\nAddition/Removal of Certificates\n--------------------------------\n\nCertifi does not support any addition/removal or other modification of the\nCA trust store content. This project is intended to provide a reliable and\nhighly portable root of trust to python deployments. Look to upstream projects\nfor methods to use alternate trust.\n"
      }
    },
    {
      "download_info": {
        "url": "https://pypi.tuna.tsinghua.edu.cn/packages/2b/75/4311605069b5d220e7cf5adabb38535bd96f0079313cdbb04b291479b22a/scikit_learn-1.7.2-cp310-cp310-manylinux_2_27_aarch64.manylinux_2_28_aarch64.whl",
        "archive_info": {
          "hash": "sha256=4a847fea807e278f821a0406ca01e387f97653e284ecbd9750e3ee7c90347f18",
          "hashes": {
            "sha256": "4a847fea807e278f821a0406ca01e387f97653e284ecbd9750e3ee7c90347f18"
          }
        }
      },
      "is_direct": false,
      "is_yanked": false,
      "requested": false,
      "metadata": {
        "metadata_version": "2.4",
        "name": "scikit-learn",
        "version": "1.7.2",
        "summary": "A set of python modules for machine learning and data mining",
        "description_content_type": "text/x-rst",
        "maintainer_email": "scikit-learn developers <scikit-learn@python.org>",
        "license_expression": "BSD-3-Clause",
        "license_file": [
          "COPYING"
        ],
        "classifier": [
          "Intended Audience :: Science/Research",
          "Intended Audience :: Developers",
          "Programming Language :: C",
          "Programming Language :: Python",
          "Topic :: Software Development",
          "Topic :: Scientific/Engineering",
          "Development Status :: 5 - Production/Stable",
          "Operating System :: Microsoft :: Windows",
          "Operating System :: POSIX",
          "Operating System :: Unix",
          "Operating System :: MacOS",
          "Programming Language :: Python :: 3",
          "Programming Language :: Python :: 3.10",
          "Programming Language :: Python :: 3.11",
          "Programming Language :: Python :: 3.12",
          "Programming Language :: Python :: 3.13",
          "Programming Language :: Python :: Implementation :: CPython"
        ],
        "requires_dist": [
          "numpy>=1.22.0",
          "scipy>=1.8.0",
          "joblib>=1.2.0",
          "threadpoolctl>=3.1.0",
          "numpy>=1.22.0; extra == \"build\"",
          "scipy>=1.8.0; extra == \"build\"",
          "cython>=3.0.10; extra == \"build\"",
          "meson-python>=0.17.1; extra == \"build\"",
          "numpy>=1.22.0; extra == \"install\"",
          "scipy>=1.8.0; extra == \"install\"",
          "joblib>=1.2.0; extra == \"install\"",
          "threadpoolctl>=3.1.0; extra == \"install\"",
          "matplotlib>=3.5.0; extra == \"benchmark\"",
          "pandas>=1.4.0; extra == \"benchmark\"",
          "memory_profiler>=0.57.0; extra == \"benchmark\"",
          "matplotlib>=3.5.0; extra == \"docs\"",
          "scikit-image>=0.19.0; extra == \"docs\"",
          "pandas>=1.4.0; extra == \"docs\"",
          "seaborn>=0.9.0; extra == \"docs\"",
          "memory_profiler>=0.57.0; extra == \"docs\"",
          "sphinx>=7.3.7; extra == \"docs\"",
          "sphinx-copybutton>=0.5.2; extra == \"docs\"",
          "sphinx-gallery>=0.17.1; extra == \"docs\"",
          "numpydoc>=1.2.0; extra == \"docs\"",
          "Pillow>=8.4.0; extra == \"docs\"",
          "pooch>=1.6.0; extra == \"docs\"",
          "sphinx-prompt>=1.4.0; extra == \"docs\"",
          "sphinxext-opengraph>=0.9.1; extra == \"docs\"",
          "plotly>=5.14.0; extra == \"docs\"",
          "polars>=0.20.30; extra == \"docs\"",
          "sphinx-design>=0.5.0; extra == \"docs\"",
          "sphinx-design>=0.6.0; extra == \"docs\"",
          "sphinxcontrib-sass>=0.3.4; extra == \"docs\"",
          "pydata-sphinx-theme>=0.15.3; extra == \"docs\"",
          "sphinx-remove-toctrees>=1.0.0.post1; extra == \"docs\"",
          "towncrier>=24.8.0; extra == \"docs\"",
          "matplotlib>=3.5.0; extra == \"examples\"",
          "scikit-image>=0.19.0; extra == \"examples\"",
          "pandas>=1.4.0; extra == \"examples\"",
          "seaborn>=0.9.0; extra == \"examples\"",
          "pooch>=1.6.0; extra == \"examples\"",
          "plotly>=5.14.0; extra == \"examples\"",
          "matplotlib>=3.5.0; extra == \"tests\"",
          "scikit-image>=0.19.0; extra == \"tests\"",
          "pandas>=1.4.0; extra == \"tests\"",
          "pytest>=7.1.2; extra == \"tests\"",
          "pytest-cov>=2.9.0; extra == \"tests\"",
          "ruff>=0.11.7; extra == \"tests\"",
          "mypy>=1.15; extra == \"tests\"",
          "pyamg>=4.2.1; extra == \"tests\"",
          "polars>=0.20.30; extra == \"tests\"",
          "pyarrow>=12.0.0; extra == \"tests\"",
          "numpydoc>=1.2.0; extra == \"tests\"",
          "pooch>=1.6.0; extra == \"tests\"",
          "conda-lock==3.0.1; extra == \"maintenance\""
        ],
        "requires_python": ">=3.10",
        "project_url": [
          "homepage, https://scikit-learn.org",
          "source, https://github.com/scikit-learn/scikit-learn",
          "download, https://pypi.org/project/scikit-learn/#files",
          "tracker, https://github.com/scikit-learn/scikit-learn/issues",
          "release notes, https://scikit-learn.org/stable/whats_new"
        ],
        "provides_extra": [
          "build",
          "install",
          "benchmark",
          "docs",
          "examples",
          "tests",
          "maintenance"
        ],
        "description": ".. -*- mode: rst -*-\n\n|Azure| |Codecov| |CircleCI| |Nightly wheels| |Ruff| |PythonVersion| |PyPi| |DOI| |Benchmark|\n\n.. |Azure| image:: https://dev.azure.com/scikit-learn/scikit-learn/_apis/build/status/scikit-learn.scikit-learn?branchName=main\n   :target: https://dev.azure.com/scikit-learn/scikit-learn/_build/latest?definitionId=1&branchName=main\n\n.. |CircleCI| image:: https://circleci.com/gh/scikit-learn/scikit-learn/tree/main.svg?style=shield\n   :target: https://circleci.com/gh/scikit-learn/scikit-learn\n\n.. |Codecov| image:: https://codecov.io/gh/scikit-learn/scikit-learn/branch/main/graph/badge.svg?token=Pk8G9gg3y9\n   :target: https://codecov.io/gh/scikit-learn/scikit-learn\n\n.. |Nightly wheels| image:: https://github.com/scikit-learn/scikit-learn/actions/workflows/wheels.yml/badge.svg?event=schedule\n   :target: https://github.com/scikit-learn/scikit-learn/actions?query=workflow%3A%22Wheel+builder%22+event%3Aschedule\n\n.. |Ruff| image:: https://img.shields.io/badge/code%20style-ruff-000000.svg\n   :target: https://github.com/astral-sh/ruff\n\n.. |PythonVersion| image:: https://img.shields.io/pypi/pyversions/scikit-learn.svg\n   :target: https://pypi.org/project/scikit-learn/\n\n.. |PyPi| image:: https://img.shields.io/pypi/v/scikit-learn\n   :target: https://pypi.org/project/scikit-learn\n\n.. |DOI| image:: https://zenodo.org/badge/21369/scikit-learn/scikit-learn.svg\n   :target: https://zenodo.org/badge/latestdoi/21369/scikit-learn/scikit-learn\n\n.. |Benchmark| image:: https://img.shields.io/badge/Benchmarked%20by-asv-blue\n   :target: https://scikit-learn.org/scikit-learn-benchmarks\n\n.. |PythonMinVersion| replace:: 3.10\n.. |NumPyMinVersion| replace:: 1.22.0\n.. |SciPyMinVersion| replace:: 1.8.0\n.. |JoblibMinVersion| replace:: 1.2.0\n.. |ThreadpoolctlMinVersion| replace:: 3.1.0\n.. |MatplotlibMinVersion| replace:: 3.5.0\n.. |Scikit-ImageMinVersion| replace:: 0.19.0\n.. |PandasMinVersion| replace:: 1.4.0\n.. |SeabornMinVersion| replace:: 0.9.0\n.. |PytestMinVersion| replace:: 7.1.2\n.. |PlotlyMinVersion| replace:: 5.14.0\n\n.. image:: https://raw.githubusercontent.com/scikit-learn/scikit-learn/main/doc/logos/scikit-learn-logo.png\n  :target: https://scikit-learn.org/\n\n**scikit-learn** is a Python module for machine learning built on top of\nSciPy and is distributed under the 3-Clause BSD license.\n\nThe project was started in 2007 by David Cournapeau as a Google Summer\nof Code project, and since then many volunteers have contributed. See\nthe `About us <https://scikit-learn.org/dev/about.html#authors>`__ page\nfor a list of core contributors.\n\nIt is currently maintained by a team of volunteers.\n\nWebsite: https://scikit-learn.org\n\nInstallation\n------------\n\nDependencies\n~~~~~~~~~~~~\n\nscikit-learn requires:\n\n- Python (>= |PythonMinVersion|)\n- NumPy (>= |NumPyMinVersion|)\n- SciPy (>= |SciPyMinVersion|)\n- joblib (>= |JoblibMinVersion|)\n- threadpoolctl (>= |ThreadpoolctlMinVersion|)\n\n=======\n\nScikit-learn plotting capabilities (i.e., functions start with ``plot_`` and\nclasses end with ``Display``) require Matplotlib (>= |MatplotlibMinVersion|).\nFor running the examples Matplotlib >= |MatplotlibMinVersion| is required.\nA few examples require scikit-image >= |Scikit-ImageMinVersion|, a few examples\nrequire pandas >= |PandasMinVersion|, some examples require seaborn >=\n|SeabornMinVersion| and plotly >= |PlotlyMinVersion|.\n\nUser installation\n~~~~~~~~~~~~~~~~~\n\nIf you already have a working installation of NumPy and SciPy,\nthe easiest way to install scikit-learn is using ``pip``::\n\n    pip install -U scikit-learn\n\nor ``conda``::\n\n    conda install -c conda-forge scikit-learn\n\nThe documentation includes more detailed `installation instructions <https://scikit-learn.org/stable/install.html>`_.\n\n\nChangelog\n---------\n\nSee the `changelog <https://scikit-learn.org/dev/whats_new.html>`__\nfor a history of notable changes to scikit-learn.\n\nDevelopment\n-----------\n\nWe welcome new contributors of all experience levels. The scikit-learn\ncommunity goals are to be helpful, welcoming, and effective. The\n`Development Guide <https://scikit-learn.org/stable/developers/index.html>`_\nhas detailed information about contributing code, documentation, tests, and\nmore. We've included some basic information in this README.\n\nImportant links\n~~~~~~~~~~~~~~~\n\n- Official source code repo: https://github.com/scikit-learn/scikit-learn\n- Download releases: https://pypi.org/project/scikit-learn/\n- Issue tracker: https://github.com/scikit-learn/scikit-learn/issues\n\nSource code\n~~~~~~~~~~~\n\nYou can check the latest sources with the command::\n\n    git clone https://github.com/scikit-learn/scikit-learn.git\n\nContributing\n~~~~~~~~~~~~\n\nTo learn more about making a contribution to scikit-learn, please see our\n`Contributing guide\n<https://scikit-learn.org/dev/developers/contributing.html>`_.\n\nTesting\n~~~~~~~\n\nAfter installation, you can launch the test suite from outside the source\ndirectory (you will need to have ``pytest`` >= |PyTestMinVersion| installed)::\n\n    pytest sklearn\n\nSee the web page https://scikit-learn.org/dev/developers/contributing.html#testing-and-improving-test-coverage\nfor more information.\n\n    Random number generation can be controlled during testing by setting\n    the ``SKLEARN_SEED`` environment variable.\n\nSubmitting a Pull Request\n~~~~~~~~~~~~~~~~~~~~~~~~~\n\nBefore opening a Pull Request, have a look at the\nfull Contributing page to make sure your code complies\nwith our guidelines: https://scikit-learn.org/stable/developers/index.html\n\nProject History\n---------------\n\nThe project was started in 2007 by David Cournapeau as a Google Summer\nof Code project, and since then many volunteers have contributed. See\nthe `About us <https://scikit-learn.org/dev/about.html#authors>`__ page\nfor a list of core contributors.\n\nThe project is currently maintained by a team of volunteers.\n\n**Note**: `scikit-learn` was previously referred to as `scikits.learn`.\n\nHelp and Support\n----------------\n\nDocumentation\n~~~~~~~~~~~~~\n\n- HTML documentation (stable release): https://scikit-learn.org\n- HTML documentation (development version): https://scikit-learn.org/dev/\n- FAQ: https://scikit-learn.org/stable/faq.html\n\nCommunication\n~~~~~~~~~~~~~\n\nMain Channels\n^^^^^^^^^^^^^\n\n- **Website**: https://scikit-learn.org\n- **Blog**: https://blog.scikit-learn.org\n- **Mailing list**: https://mail.python.org/mailman/listinfo/scikit-learn\n\nDeveloper & Support\n^^^^^^^^^^^^^^^^^^^^^^\n\n- **GitHub Discussions**: https://github.com/scikit-learn/scikit-learn/discussions\n- **Stack Overflow**: https://stackoverflow.com/questions/tagged/scikit-learn\n- **Discord**: https://discord.gg/h9qyrK8Jc8\n\nSocial Media Platforms\n^^^^^^^^^^^^^^^^^^^^^^\n\n- **LinkedIn**: https://www.linkedin.com/company/scikit-learn\n- **YouTube**: https://www.youtube.com/channel/UCJosFjYm0ZYVUARxuOZqnnw/playlists\n- **Facebook**: https://www.facebook.com/scikitlearnofficial/\n- **Instagram**: https://www.instagram.com/scikitlearnofficial/\n- **TikTok**: https://www.tiktok.com/@scikit.learn\n- **Bluesky**: https://bsky.app/profile/scikit-learn.org\n- **Mastodon**: https://mastodon.social/@sklearn@fosstodon.org\n\nResources\n^^^^^^^^^\n\n- **Calendar**: https://blog.scikit-learn.org/calendar/\n- **Logos & Branding**: https://github.com/scikit-learn/scikit-learn/tree/main/doc/logos\n\nCitation\n~~~~~~~~\n\nIf you use scikit-learn in a scientific publication, we would appreciate citations: https://scikit-learn.org/stable/about.html#citing-scikit-learn\n"
      }
    },
    {
      "download_info": {
        "url": "https://pypi.tuna.tsinghua.edu.cn/packages/fc/91/00384166f110a3888ea8efd44523ba7168dd2dc39e3e43c931cc2d069fa9/soxr-1.0.0-cp310-cp310-manylinux_2_26_aarch64.manylinux_2_28_aarch64.whl",
        "archive_info": {
          "hash": "sha256=b89685faedebc45af71f08f9957b61cc6143bc94ba43fe38e97067f81e272969",
          "hashes": {
            "sha256": "b89685faedebc45af71f08f9957b61cc6143bc94ba43fe38e97067f81e272969"
          }
        }
      },
      "is_direct": false,
      "is_yanked": false,
      "requested": false,
      "metadata": {
        "metadata_version": "2.4",
        "name": "soxr",
        "version": "1.0.0",
        "summary": "High quality, one-dimensional sample-rate conversion library",
        "description_content_type": "text/markdown",
        "keywords": [
          "audio resampling",
          "samplerate conversion",
          "SRC",
          "signal processing",
          "resampler"
        ],
        "author": "KEUM Myungchul",
        "license_expression": "LGPL-2.1-or-later",
        "classifier": [
          "Development Status :: 5 - Production/Stable",
          "Intended Audience :: Developers",
          "Intended Audience :: Science/Research",
          "Intended Audience :: Telecommunications Industry",
          "Topic :: Multimedia :: Sound/Audio :: Analysis",
          "Topic :: Multimedia :: Sound/Audio :: Conversion",
          "Topic :: Scientific/Engineering",
          "Programming Language :: C",
          "Programming Language :: C++",
          "Programming Language :: Python :: 3",
          "Programming Language :: Python :: Free Threading :: 2 - Beta"
        ],
        "requires_dist": [
          "numpy",
          "sphinx; extra == \"docs\"",
          "sphinx-book-theme; extra == \"docs\"",
          "myst-parser; extra == \"docs\"",
          "linkify-it-py; extra == \"docs\"",
          "pytest; extra == \"test\""
        ],
        "requires_python": ">=3.9",
        "project_url": [
          "Homepage, https://github.com/dofuuz/python-soxr",
          "Documentation, https://python-soxr.readthedocs.io",
          "Source, https://github.com/dofuuz/python-soxr",
          "Bug Tracker, https://github.com/dofuuz/python-soxr/issues"
        ],
        "provides_extra": [
          "docs",
          "test"
        ],
        "description": "# Python-SoXR\n\n[![GitHub](https://img.shields.io/badge/GitHub-python--soxr-181717?logo=github)](https://github.com/dofuuz/python-soxr) [![PyPI](https://img.shields.io/pypi/v/soxr.svg?logo=pypi)](https://pypi.org/project/soxr/) [![conda-forge](https://img.shields.io/conda/vn/conda-forge/soxr-python?logo=conda-forge)](https://anaconda.org/conda-forge/soxr-python) [![Packaging status](https://repology.org/badge/tiny-repos/python:soxr.svg)](https://repology.org/project/python:soxr/versions) [![Read the Docs](https://img.shields.io/readthedocs/python-soxr?logo=read-the-docs)](https://python-soxr.readthedocs.io)\n\nHigh quality, one-dimensional sample-rate conversion library for Python.\n\n- Homepage: https://github.com/dofuuz/python-soxr\n- Documentation: https://python-soxr.readthedocs.io\n- PyPI: https://pypi.org/project/soxr/\n\nKeywords: Resampler, Audio resampling, Samplerate conversion, DSP(Digital Signal Processing)\n\nPython-SoXR is a Python wrapper of [libsoxr](https://sourceforge.net/projects/soxr/).\n\n\n## Installation\n\n```sh\npip install soxr\n```\n\nIf installation fails, upgrade pip with `python -m pip install --upgrade pip` and try again.\n\n\n### in Conda environment\n\n```sh\nconda install -c conda-forge soxr-python\n```\n\nNote: Conda packge name is `soxr-python`, not python-soxr.\n\n\n## Basic usage\n\n```python\nimport soxr\n\ny = soxr.resample(\n    x,          # input array ‚Äì mono(1D) or multi-channel(2D of [frame, channel])\n    48000,      # input samplerate\n    16000       # target samplerate\n)\n```\nIf input is not `numpy.ndarray`, it will be converted to `numpy.ndarray(dtype='float32')`.  \ndtype should be one of float32, float64, int16, int32.\n\nOutput is `numpy.ndarray` with same dimension and data type of input.\n\n\n## Streaming usage\n\nUse `ResampleStream` for real-time processing or very long signal.\n\n```python\nimport soxr\n\nrs = soxr.ResampleStream(\n    44100,              # input samplerate\n    16000,              # target samplerate\n    1,                  # channel(s)\n    dtype='float32'     # data type (default = 'float32')\n)\n\neof = False\nwhile not eof:\n    # Get chunk\n    ...\n\n    y_chunk = rs.resample_chunk(\n        x,              # input aray ‚Äì mono(1D) or multi-channel(2D of [frame, channel])\n        last=eof        # Set True at end of input\n    )\n```\n\nOutput frame count may not be consistent. This is normal operation.  \n(ex. [0, 0, 0, 186, 186, 166, 186, 186, 168, ...])\n\nüìù [More code examples](https://dofuuz.github.io/dsp/2024/05/26/sample-rate-conversion-in-python.html)\n\n\n## Benchmark\n\nSweep, impulse, speed compairsion with other resamplers for Python.\n\nhttps://colab.research.google.com/drive/1_xYUs00VWYOAXShB85W1MFWaUjGHfO4K?usp=sharing\n\n\n### Speed comparison summary\n\nDownsampling 10 sec of 48000 Hz to 44100 Hz.  \nRan on Google Colab.\n\nLibrary                  | Time on CPU (ms)\n------------------------ | ----------------\nsoxr (HQ)                | 10.8\ntorchaudio               | 13.8\nsoxr (VHQ)               | 14.5\nscipy.signal.resample    | 21.3\nlilfilter                | 24.7\njulius                   | 31\nresampy (kaiser_fast)    | 108\nsamplerate (sinc_medium) | 223\nresampy (kaiser_best)    | 310\nsamplerate (sinc_best)   | 794\n\n\n## Technical detail\n\nFor technical details behind resampler, see libsoxr docs.\n- https://sourceforge.net/p/soxr/wiki/Home/\n- http://sox.sourceforge.net/SoX/Resampling ([archive](https://web.archive.org/web/20230626144127/https://sox.sourceforge.net/SoX/Resampling))\n- https://sourceforge.net/p/soxr/code/ci/master/tree/src/soxr.h\n\nPython-SoXR package comes with [modified version](https://github.com/dofuuz/soxr) of libsoxr. [See changes here](https://github.com/dofuuz/soxr/compare/0.1.3...master).  \nThese changes do not apply to dynamic-linked builds (e.g. conda-forge build).  \nTo check the version of libsoxr, use `soxr.__libsoxr_version__`.\n\n\n## Credit and License\n\nPython-SoXR is LGPL v2.1+ licensed, following libsoxr's license.\n\n### OSS libraries used\n\n#### libsoxr (LGPLv2.1+)\nThe SoX Resampler library  \nhttps://sourceforge.net/projects/soxr/\n\nPython-SoXR is a Python wrapper of libsoxr.\n\n#### PFFFT (BSD-like)\nPFFFT: a pretty fast FFT.  \nhttps://bitbucket.org/jpommier/pffft/  \n\nlibsoxr dependency.\n"
      }
    },
    {
      "download_info": {
        "url": "https://pypi.tuna.tsinghua.edu.cn/packages/32/d5/f9a850d79b0851d1d4ef6456097579a9005b31fea68726a4ae5f2d82ddd9/threadpoolctl-3.6.0-py3-none-any.whl",
        "archive_info": {
          "hash": "sha256=43a0b8fd5a2928500110039e43a5eed8480b918967083ea48dc3ab9f13c4a7fb",
          "hashes": {
            "sha256": "43a0b8fd5a2928500110039e43a5eed8480b918967083ea48dc3ab9f13c4a7fb"
          }
        }
      },
      "is_direct": false,
      "is_yanked": false,
      "requested": false,
      "metadata": {
        "metadata_version": "2.4",
        "name": "threadpoolctl",
        "version": "3.6.0",
        "summary": "threadpoolctl",
        "description_content_type": "text/markdown",
        "home_page": "https://github.com/joblib/threadpoolctl",
        "author": "Thomas Moreau",
        "author_email": "thomas.moreau.2010@gmail.com",
        "license": "BSD-3-Clause",
        "license_file": [
          "LICENSE"
        ],
        "classifier": [
          "Intended Audience :: Developers",
          "License :: OSI Approved :: BSD License",
          "Programming Language :: Python :: 3",
          "Programming Language :: Python :: 3.9",
          "Programming Language :: Python :: 3.10",
          "Programming Language :: Python :: 3.11",
          "Programming Language :: Python :: 3.12",
          "Programming Language :: Python :: 3.13",
          "Topic :: Software Development :: Libraries :: Python Modules"
        ],
        "requires_python": ">=3.9",
        "description": "# Thread-pool Controls [![Build Status](https://github.com/joblib/threadpoolctl/actions/workflows/test.yml/badge.svg?branch=master)](https://github.com/joblib/threadpoolctl/actions?query=branch%3Amaster) [![codecov](https://codecov.io/gh/joblib/threadpoolctl/branch/master/graph/badge.svg)](https://codecov.io/gh/joblib/threadpoolctl)\n\nPython helpers to limit the number of threads used in the\nthreadpool-backed of common native libraries used for scientific\ncomputing and data science (e.g. BLAS and OpenMP).\n\nFine control of the underlying thread-pool size can be useful in\nworkloads that involve nested parallelism so as to mitigate\noversubscription issues.\n\n## Installation\n\n- For users, install the last published version from PyPI:\n\n  ```bash\n  pip install threadpoolctl\n  ```\n\n- For contributors, install from the source repository in developer\n  mode:\n\n  ```bash\n  pip install -r dev-requirements.txt\n  flit install --symlink\n  ```\n\n  then you run the tests with pytest:\n\n  ```bash\n  pytest\n  ```\n\n## Usage\n\n### Command Line Interface\n\nGet a JSON description of thread-pools initialized when importing python\npackages such as numpy or scipy for instance:\n\n```\npython -m threadpoolctl -i numpy scipy.linalg\n[\n  {\n    \"filepath\": \"/home/ogrisel/miniconda3/envs/tmp/lib/libmkl_rt.so\",\n    \"prefix\": \"libmkl_rt\",\n    \"user_api\": \"blas\",\n    \"internal_api\": \"mkl\",\n    \"version\": \"2019.0.4\",\n    \"num_threads\": 2,\n    \"threading_layer\": \"intel\"\n  },\n  {\n    \"filepath\": \"/home/ogrisel/miniconda3/envs/tmp/lib/libiomp5.so\",\n    \"prefix\": \"libiomp\",\n    \"user_api\": \"openmp\",\n    \"internal_api\": \"openmp\",\n    \"version\": null,\n    \"num_threads\": 4\n  }\n]\n```\n\nThe JSON information is written on STDOUT. If some of the packages are missing,\na warning message is displayed on STDERR.\n\n### Python Runtime Programmatic Introspection\n\nIntrospect the current state of the threadpool-enabled runtime libraries\nthat are loaded when importing Python packages:\n\n```python\n>>> from threadpoolctl import threadpool_info\n>>> from pprint import pprint\n>>> pprint(threadpool_info())\n[]\n\n>>> import numpy\n>>> pprint(threadpool_info())\n[{'filepath': '/home/ogrisel/miniconda3/envs/tmp/lib/libmkl_rt.so',\n  'internal_api': 'mkl',\n  'num_threads': 2,\n  'prefix': 'libmkl_rt',\n  'threading_layer': 'intel',\n  'user_api': 'blas',\n  'version': '2019.0.4'},\n {'filepath': '/home/ogrisel/miniconda3/envs/tmp/lib/libiomp5.so',\n  'internal_api': 'openmp',\n  'num_threads': 4,\n  'prefix': 'libiomp',\n  'user_api': 'openmp',\n  'version': None}]\n\n>>> import xgboost\n>>> pprint(threadpool_info())\n[{'filepath': '/home/ogrisel/miniconda3/envs/tmp/lib/libmkl_rt.so',\n  'internal_api': 'mkl',\n  'num_threads': 2,\n  'prefix': 'libmkl_rt',\n  'threading_layer': 'intel',\n  'user_api': 'blas',\n  'version': '2019.0.4'},\n {'filepath': '/home/ogrisel/miniconda3/envs/tmp/lib/libiomp5.so',\n  'internal_api': 'openmp',\n  'num_threads': 4,\n  'prefix': 'libiomp',\n  'user_api': 'openmp',\n  'version': None},\n {'filepath': '/home/ogrisel/miniconda3/envs/tmp/lib/libgomp.so.1.0.0',\n  'internal_api': 'openmp',\n  'num_threads': 4,\n  'prefix': 'libgomp',\n  'user_api': 'openmp',\n  'version': None}]\n```\n\nIn the above example, `numpy` was installed from the default anaconda channel and comes\nwith MKL and its Intel OpenMP (`libiomp5`) implementation while `xgboost` was installed\nfrom pypi.org and links against GNU OpenMP (`libgomp`) so both OpenMP runtimes are\nloaded in the same Python program.\n\nThe state of these libraries is also accessible through the object oriented API:\n\n```python\n>>> from threadpoolctl import ThreadpoolController, threadpool_info\n>>> from pprint import pprint\n>>> import numpy\n>>> controller = ThreadpoolController()\n>>> pprint(controller.info())\n[{'architecture': 'Haswell',\n  'filepath': '/home/jeremie/miniconda/envs/dev/lib/libopenblasp-r0.3.17.so',\n  'internal_api': 'openblas',\n  'num_threads': 4,\n  'prefix': 'libopenblas',\n  'threading_layer': 'pthreads',\n  'user_api': 'blas',\n  'version': '0.3.17'}]\n\n>>> controller.info() == threadpool_info()\nTrue\n```\n\n### Setting the Maximum Size of Thread-Pools\n\nControl the number of threads used by the underlying runtime libraries\nin specific sections of your Python program:\n\n```python\n>>> from threadpoolctl import threadpool_limits\n>>> import numpy as np\n\n>>> with threadpool_limits(limits=1, user_api='blas'):\n...     # In this block, calls to blas implementation (like openblas or MKL)\n...     # will be limited to use only one thread. They can thus be used jointly\n...     # with thread-parallelism.\n...     a = np.random.randn(1000, 1000)\n...     a_squared = a @ a\n```\n\nThe threadpools can also be controlled via the object oriented API, which is especially\nuseful to avoid searching through all the loaded shared libraries each time. It will\nhowever not act on libraries loaded after the instantiation of the\n`ThreadpoolController`:\n\n```python\n>>> from threadpoolctl import ThreadpoolController\n>>> import numpy as np\n>>> controller = ThreadpoolController()\n\n>>> with controller.limit(limits=1, user_api='blas'):\n...     a = np.random.randn(1000, 1000)\n...     a_squared = a @ a\n```\n\n### Restricting the limits to the scope of a function\n\n`threadpool_limits` and `ThreadpoolController` can also be used as decorators to set\nthe maximum number of threads used by the supported libraries at a function level. The\ndecorators are accessible through their `wrap` method:\n\n```python\n>>> from threadpoolctl import ThreadpoolController, threadpool_limits\n>>> import numpy as np\n>>> controller = ThreadpoolController()\n\n>>> @controller.wrap(limits=1, user_api='blas')\n... # or @threadpool_limits.wrap(limits=1, user_api='blas')\n... def my_func():\n...     # Inside this function, calls to blas implementation (like openblas or MKL)\n...     # will be limited to use only one thread.\n...     a = np.random.randn(1000, 1000)\n...     a_squared = a @ a\n...\n```\n\n### Switching the FlexiBLAS backend\n\n`FlexiBLAS` is a BLAS wrapper for which the BLAS backend can be switched at runtime.\n`threadpoolctl` exposes python bindings for this feature. Here's an example but note\nthat this part of the API is experimental and subject to change without deprecation:\n\n```python\n>>> from threadpoolctl import ThreadpoolController\n>>> import numpy as np\n>>> controller = ThreadpoolController()\n\n>>> controller.info()\n[{'user_api': 'blas',\n  'internal_api': 'flexiblas',\n  'num_threads': 1,\n  'prefix': 'libflexiblas',\n  'filepath': '/usr/local/lib/libflexiblas.so.3.3',\n  'version': '3.3.1',\n  'available_backends': ['NETLIB', 'OPENBLASPTHREAD', 'ATLAS'],\n  'loaded_backends': ['NETLIB'],\n  'current_backend': 'NETLIB'}]\n\n# Retrieve the flexiblas controller\n>>> flexiblas_ct = controller.select(internal_api=\"flexiblas\").lib_controllers[0]\n\n# Switch the backend with one predefined at build time (listed in \"available_backends\")\n>>> flexiblas_ct.switch_backend(\"OPENBLASPTHREAD\")\n>>> controller.info()\n[{'user_api': 'blas',\n  'internal_api': 'flexiblas',\n  'num_threads': 4,\n  'prefix': 'libflexiblas',\n  'filepath': '/usr/local/lib/libflexiblas.so.3.3',\n  'version': '3.3.1',\n  'available_backends': ['NETLIB', 'OPENBLASPTHREAD', 'ATLAS'],\n  'loaded_backends': ['NETLIB', 'OPENBLASPTHREAD'],\n  'current_backend': 'OPENBLASPTHREAD'},\n {'user_api': 'blas',\n  'internal_api': 'openblas',\n  'num_threads': 4,\n  'prefix': 'libopenblas',\n  'filepath': '/usr/lib/x86_64-linux-gnu/openblas-pthread/libopenblasp-r0.3.8.so',\n  'version': '0.3.8',\n  'threading_layer': 'pthreads',\n  'architecture': 'Haswell'}]\n\n# It's also possible to directly give the path to a shared library\n>>> flexiblas_controller.switch_backend(\"/home/jeremie/miniforge/envs/flexiblas_threadpoolctl/lib/libmkl_rt.so\")\n>>> controller.info()\n[{'user_api': 'blas',\n  'internal_api': 'flexiblas',\n  'num_threads': 2,\n  'prefix': 'libflexiblas',\n  'filepath': '/usr/local/lib/libflexiblas.so.3.3',\n  'version': '3.3.1',\n  'available_backends': ['NETLIB', 'OPENBLASPTHREAD', 'ATLAS'],\n  'loaded_backends': ['NETLIB',\n   'OPENBLASPTHREAD',\n   '/home/jeremie/miniforge/envs/flexiblas_threadpoolctl/lib/libmkl_rt.so'],\n  'current_backend': '/home/jeremie/miniforge/envs/flexiblas_threadpoolctl/lib/libmkl_rt.so'},\n {'user_api': 'openmp',\n  'internal_api': 'openmp',\n  'num_threads': 4,\n  'prefix': 'libomp',\n  'filepath': '/home/jeremie/miniforge/envs/flexiblas_threadpoolctl/lib/libomp.so',\n  'version': None},\n {'user_api': 'blas',\n  'internal_api': 'openblas',\n  'num_threads': 4,\n  'prefix': 'libopenblas',\n  'filepath': '/usr/lib/x86_64-linux-gnu/openblas-pthread/libopenblasp-r0.3.8.so',\n  'version': '0.3.8',\n  'threading_layer': 'pthreads',\n  'architecture': 'Haswell'},\n {'user_api': 'blas',\n  'internal_api': 'mkl',\n  'num_threads': 2,\n  'prefix': 'libmkl_rt',\n  'filepath': '/home/jeremie/miniforge/envs/flexiblas_threadpoolctl/lib/libmkl_rt.so.2',\n  'version': '2024.0-Product',\n  'threading_layer': 'gnu'}]\n```\n\nYou can observe that the previously linked OpenBLAS shared object stays loaded by\nthe Python program indefinitely, but FlexiBLAS itself no longer delegates BLAS calls\nto OpenBLAS as indicated by the `current_backend` attribute.\n### Writing a custom library controller\n\nCurrently, `threadpoolctl` has support for `OpenMP` and the main `BLAS` libraries.\nHowever it can also be used to control the threadpool of other native libraries,\nprovided that they expose an API to get and set the limit on the number of threads.\nFor that, one must implement a controller for this library and register it to\n`threadpoolctl`.\n\nA custom controller must be a subclass of the `LibController` class and implement\nthe attributes and methods described in the docstring of `LibController`. Then this\nnew controller class must be registered using the `threadpoolctl.register` function.\nAn complete example can be found [here](\n  https://github.com/joblib/threadpoolctl/blob/master/tests/_pyMylib/__init__.py).\n\n### Sequential BLAS within OpenMP parallel region\n\nWhen one wants to have sequential BLAS calls within an OpenMP parallel region, it's\nsafer to set `limits=\"sequential_blas_under_openmp\"` since setting `limits=1` and\n`user_api=\"blas\"` might not lead to the expected behavior in some configurations\n(e.g. OpenBLAS with the OpenMP threading layer\nhttps://github.com/xianyi/OpenBLAS/issues/2985).\n\n### Known Limitations\n\n- `threadpool_limits` can fail to limit the number of inner threads when nesting\n  parallel loops managed by distinct OpenMP runtime implementations (for instance\n  libgomp from GCC and libomp from clang/llvm or libiomp from ICC).\n\n  See the `test_openmp_nesting` function in [tests/test_threadpoolctl.py](\n  https://github.com/joblib/threadpoolctl/blob/master/tests/test_threadpoolctl.py)\n  for an example. More information can be found at:\n  https://github.com/jeremiedbb/Nested_OpenMP\n\n  Note however that this problem does not happen when `threadpool_limits` is\n  used to limit the number of threads used internally by BLAS calls that are\n  themselves nested under OpenMP parallel loops. `threadpool_limits` works as\n  expected, even if the inner BLAS implementation relies on a distinct OpenMP\n  implementation.\n\n- Using Intel OpenMP (ICC) and LLVM OpenMP (clang) in the same Python program\n  under Linux is known to cause problems. See the following guide for more details\n  and workarounds:\n  https://github.com/joblib/threadpoolctl/blob/master/multiple_openmp.md\n\n- Setting the maximum number of threads of the OpenMP and BLAS libraries has a global\n  effect and impacts the whole Python process. There is no thread level isolation as\n  these libraries do not offer thread-local APIs to configure the number of threads to\n  use in nested parallel calls.\n\n\n## Maintainers\n\nTo make a release:\n\n- Bump the version number (`__version__`) in `threadpoolctl.py` and update the\n  release date in `CHANGES.md`.\n\n- Build the distribution archives:\n\n```bash\npip install flit\nflit build\n```\n\nand check the contents of `dist/`.\n\n- If everything is fine, make a commit for the release, tag it and push the\ntag to github:\n\n```bash\ngit tag -a X.Y.Z\ngit push git@github.com:joblib/threadpoolctl.git X.Y.Z\n```\n\n- Upload the wheels and source distribution to PyPI using flit. Since PyPI doesn't\n  allow password authentication anymore, the username needs to be changed to the\n  generic name `__token__`:\n\n```bash\nFLIT_USERNAME=__token__ flit publish\n```\n\n  and a PyPI token has to be passed in place of the password.\n\n- Create a PR for the release on the [conda-forge feedstock](https://github.com/conda-forge/threadpoolctl-feedstock) (or wait for the bot to make it).\n\n- Publish the release on github.\n\n### Credits\n\nThe initial dynamic library introspection code was written by @anton-malakhov\nfor the smp package available at https://github.com/IntelPython/smp .\n\nthreadpoolctl extends this for other operating systems. Contrary to smp,\nthreadpoolctl does not attempt to limit the size of Python multiprocessing\npools (threads or processes) or set operating system-level CPU affinity\nconstraints: threadpoolctl only interacts with native libraries via their\npublic runtime APIs.\n\n"
      }
    },
    {
      "download_info": {
        "url": "https://pypi.tuna.tsinghua.edu.cn/packages/86/05/63f01821681b2be5d1739b4aad7b186c28d4ead2c5c99a9fc4aa53c13c19/modelscope-1.33.0-py3-none-any.whl",
        "archive_info": {
          "hash": "sha256=d9bdd566303f813d762e133410007eaf1b78f065c871228ab38640919b707489",
          "hashes": {
            "sha256": "d9bdd566303f813d762e133410007eaf1b78f065c871228ab38640919b707489"
          }
        }
      },
      "is_direct": false,
      "is_yanked": false,
      "requested": false,
      "metadata": {
        "metadata_version": "2.4",
        "name": "modelscope",
        "version": "1.33.0",
        "dynamic": [
          "license-file"
        ],
        "summary": "ModelScope: bring the notion of Model-as-a-Service to life.",
        "description_content_type": "text/markdown",
        "keywords": [
          "python",
          "nlp",
          "science",
          "cv",
          "speech",
          "multi-modal"
        ],
        "author": "ModelScope team",
        "author_email": "contact@modelscope.cn",
        "license_expression": "Apache-2.0",
        "license_file": [
          "LICENSE"
        ],
        "classifier": [
          "Development Status :: 4 - Beta",
          "Operating System :: OS Independent",
          "Programming Language :: Python :: 3",
          "Programming Language :: Python :: 3.9",
          "Programming Language :: Python :: 3.10",
          "Programming Language :: Python :: 3.11"
        ],
        "requires_dist": [
          "filelock",
          "requests>=2.25",
          "setuptools",
          "tqdm>=4.64.0",
          "urllib3>=1.26",
          "filelock; extra == \"hub\"",
          "requests>=2.25; extra == \"hub\"",
          "setuptools; extra == \"hub\"",
          "tqdm>=4.64.0; extra == \"hub\"",
          "urllib3>=1.26; extra == \"hub\"",
          "addict; extra == \"datasets\"",
          "attrs; extra == \"datasets\"",
          "datasets<=3.6.0,>=3.0.0; extra == \"datasets\"",
          "einops; extra == \"datasets\"",
          "oss2; extra == \"datasets\"",
          "Pillow; extra == \"datasets\"",
          "python-dateutil>=2.1; extra == \"datasets\"",
          "scipy; extra == \"datasets\"",
          "setuptools; extra == \"datasets\"",
          "simplejson>=3.3.0; extra == \"datasets\"",
          "sortedcontainers>=1.5.9; extra == \"datasets\"",
          "urllib3>=1.26; extra == \"datasets\"",
          "addict; extra == \"framework\"",
          "attrs; extra == \"framework\"",
          "datasets<=3.6.0,>=3.0.0; extra == \"framework\"",
          "einops; extra == \"framework\"",
          "Pillow; extra == \"framework\"",
          "python-dateutil>=2.1; extra == \"framework\"",
          "PyYAML>=5.4; extra == \"framework\"",
          "scipy; extra == \"framework\"",
          "setuptools; extra == \"framework\"",
          "simplejson>=3.3.0; extra == \"framework\"",
          "sortedcontainers>=1.5.9; extra == \"framework\"",
          "transformers; extra == \"framework\"",
          "urllib3>=1.26; extra == \"framework\"",
          "fastapi; extra == \"server\"",
          "sse-starlette; extra == \"server\"",
          "uvicorn; extra == \"server\"",
          "docutils>=0.16.0; extra == \"docs\"",
          "myst_parser; extra == \"docs\"",
          "recommonmark; extra == \"docs\"",
          "sphinx>=5.3.0; extra == \"docs\"",
          "sphinx-book-theme; extra == \"docs\"",
          "sphinx-copybutton; extra == \"docs\"",
          "sphinx_markdown_tables; extra == \"docs\"",
          "expecttest; extra == \"tests\"",
          "flake8; extra == \"tests\"",
          "isort>=4.3.21; extra == \"tests\"",
          "pre-commit; extra == \"tests\"",
          "yapf==0.30.0; extra == \"tests\"",
          "addict; extra == \"cv\"",
          "attrs; extra == \"cv\"",
          "datasets<=3.6.0,>=3.0.0; extra == \"cv\"",
          "einops; extra == \"cv\"",
          "Pillow; extra == \"cv\"",
          "python-dateutil>=2.1; extra == \"cv\"",
          "PyYAML>=5.4; extra == \"cv\"",
          "scipy; extra == \"cv\"",
          "setuptools; extra == \"cv\"",
          "simplejson>=3.3.0; extra == \"cv\"",
          "sortedcontainers>=1.5.9; extra == \"cv\"",
          "transformers; extra == \"cv\"",
          "urllib3>=1.26; extra == \"cv\"",
          "accelerate; extra == \"cv\"",
          "albumentations>=1.0.3; extra == \"cv\"",
          "av>=9.2.0; extra == \"cv\"",
          "bmt_clipit>=1.0; extra == \"cv\"",
          "chumpy; extra == \"cv\"",
          "clip>=1.0; extra == \"cv\"",
          "control_ldm; extra == \"cv\"",
          "ddpm_guided_diffusion==0.0.0; extra == \"cv\"",
          "diffusers; extra == \"cv\"",
          "easydict; extra == \"cv\"",
          "edit_distance; extra == \"cv\"",
          "face_alignment>=1.3.5; extra == \"cv\"",
          "fairscale>=0.4.1; extra == \"cv\"",
          "fastai>=1.0.51; extra == \"cv\"",
          "ffmpeg>=1.4; extra == \"cv\"",
          "ffmpeg-python>=0.2.0; extra == \"cv\"",
          "ftfy; extra == \"cv\"",
          "fvcore; extra == \"cv\"",
          "imageio>=2.9.0; extra == \"cv\"",
          "imageio-ffmpeg>=0.4.2; extra == \"cv\"",
          "imgaug>=0.4.0; extra == \"cv\"",
          "kornia>=0.5.0; extra == \"cv\"",
          "lmdb; extra == \"cv\"",
          "lpips; extra == \"cv\"",
          "matplotlib>=3.8.0; extra == \"cv\"",
          "ml_collections; extra == \"cv\"",
          "mmcls>=0.21.0; extra == \"cv\"",
          "mmdet<=2.28.2,>=2.25.0; extra == \"cv\"",
          "mmdet3d==1.0.0a1; extra == \"cv\"",
          "mmsegmentation<=0.30.0; extra == \"cv\"",
          "moviepy==1.0.3; extra == \"cv\"",
          "nerfacc==0.2.2; extra == \"cv\"",
          "networkx; extra == \"cv\"",
          "numba; extra == \"cv\"",
          "omegaconf; extra == \"cv\"",
          "onnx; extra == \"cv\"",
          "onnxruntime>=1.10; extra == \"cv\"",
          "onnxsim; extra == \"cv\"",
          "open-clip-torch>=2.7.0; extra == \"cv\"",
          "opencv-python; extra == \"cv\"",
          "paint_ldm; extra == \"cv\"",
          "pandas; extra == \"cv\"",
          "panopticapi; extra == \"cv\"",
          "Pillow>=6.2.0; extra == \"cv\"",
          "plyfile>=0.7.4; extra == \"cv\"",
          "psutil; extra == \"cv\"",
          "pyclipper; extra == \"cv\"",
          "PyMCubes<=0.1.4; extra == \"cv\"",
          "pytorch-lightning; extra == \"cv\"",
          "regex; extra == \"cv\"",
          "scikit-image; extra == \"cv\"",
          "scikit-learn; extra == \"cv\"",
          "shapely; extra == \"cv\"",
          "shotdetect_scenedetect_lgss==0.0.4; extra == \"cv\"",
          "smplx; extra == \"cv\"",
          "tensorflow-estimator>=1.15.1; extra == \"cv\"",
          "tf_slim; extra == \"cv\"",
          "thop; extra == \"cv\"",
          "timm>=0.4.9; extra == \"cv\"",
          "torch-scatter; extra == \"cv\"",
          "torchmetrics>=0.6.2; extra == \"cv\"",
          "torchsummary>=1.5.1; extra == \"cv\"",
          "torchvision; extra == \"cv\"",
          "tqdm; extra == \"cv\"",
          "transformers>=4.26.0; extra == \"cv\"",
          "trimesh; extra == \"cv\"",
          "ujson; extra == \"cv\"",
          "utils; extra == \"cv\"",
          "videofeatures_clipit>=1.0; extra == \"cv\"",
          "yacs; extra == \"cv\"",
          "addict; extra == \"nlp\"",
          "attrs; extra == \"nlp\"",
          "datasets<=3.6.0,>=3.0.0; extra == \"nlp\"",
          "einops; extra == \"nlp\"",
          "Pillow; extra == \"nlp\"",
          "python-dateutil>=2.1; extra == \"nlp\"",
          "PyYAML>=5.4; extra == \"nlp\"",
          "scipy; extra == \"nlp\"",
          "setuptools; extra == \"nlp\"",
          "simplejson>=3.3.0; extra == \"nlp\"",
          "sortedcontainers>=1.5.9; extra == \"nlp\"",
          "transformers; extra == \"nlp\"",
          "urllib3>=1.26; extra == \"nlp\"",
          "boto3; extra == \"nlp\"",
          "embeddings; extra == \"nlp\"",
          "filelock; extra == \"nlp\"",
          "ftfy; extra == \"nlp\"",
          "jieba>=0.42.1; extra == \"nlp\"",
          "matplotlib; extra == \"nlp\"",
          "megatron_util; extra == \"nlp\"",
          "nltk; extra == \"nlp\"",
          "pandas; extra == \"nlp\"",
          "protobuf<3.21.0,>=3.19.0; extra == \"nlp\"",
          "pythainlp; extra == \"nlp\"",
          "pyvi; extra == \"nlp\"",
          "regex; extra == \"nlp\"",
          "rouge; extra == \"nlp\"",
          "sacremoses>=0.0.41; extra == \"nlp\"",
          "scikit_learn; extra == \"nlp\"",
          "sentencepiece; extra == \"nlp\"",
          "seqeval; extra == \"nlp\"",
          "spacy<=3.7.0,>=2.3.5; extra == \"nlp\"",
          "stanza; extra == \"nlp\"",
          "subword_nmt>=0.3.8; extra == \"nlp\"",
          "termcolor; extra == \"nlp\"",
          "tokenizers; extra == \"nlp\"",
          "transformers>=4.12.0; extra == \"nlp\"",
          "zhconv; extra == \"nlp\"",
          "addict; extra == \"multi-modal\"",
          "attrs; extra == \"multi-modal\"",
          "datasets<=3.6.0,>=3.0.0; extra == \"multi-modal\"",
          "einops; extra == \"multi-modal\"",
          "Pillow; extra == \"multi-modal\"",
          "python-dateutil>=2.1; extra == \"multi-modal\"",
          "PyYAML>=5.4; extra == \"multi-modal\"",
          "scipy; extra == \"multi-modal\"",
          "setuptools; extra == \"multi-modal\"",
          "simplejson>=3.3.0; extra == \"multi-modal\"",
          "sortedcontainers>=1.5.9; extra == \"multi-modal\"",
          "transformers; extra == \"multi-modal\"",
          "urllib3>=1.26; extra == \"multi-modal\"",
          "accelerate; extra == \"multi-modal\"",
          "cloudpickle; extra == \"multi-modal\"",
          "decord>=0.6.0; extra == \"multi-modal\"",
          "diffusers>=0.25.0; extra == \"multi-modal\"",
          "fairseq-fixed==0.12.3.1; extra == \"multi-modal\"",
          "ftfy>=6.0.3; extra == \"multi-modal\"",
          "librosa==0.10.1; extra == \"multi-modal\"",
          "opencv-python; extra == \"multi-modal\"",
          "pycocoevalcap>=1.2; extra == \"multi-modal\"",
          "pycocotools>=2.0.4; extra == \"multi-modal\"",
          "pydot; extra == \"multi-modal\"",
          "pytorch_lightning<=1.7.7; extra == \"multi-modal\"",
          "rapidfuzz; extra == \"multi-modal\"",
          "rouge_score<=0.0.4; extra == \"multi-modal\"",
          "sacrebleu; extra == \"multi-modal\"",
          "safetensors; extra == \"multi-modal\"",
          "soundfile; extra == \"multi-modal\"",
          "taming-transformers-rom1504; extra == \"multi-modal\"",
          "timm; extra == \"multi-modal\"",
          "tokenizers; extra == \"multi-modal\"",
          "torchvision; extra == \"multi-modal\"",
          "transformers>=4.27.1; extra == \"multi-modal\"",
          "unicodedata2; extra == \"multi-modal\"",
          "zhconv; extra == \"multi-modal\"",
          "addict; extra == \"science\"",
          "attrs; extra == \"science\"",
          "datasets<=3.6.0,>=3.0.0; extra == \"science\"",
          "einops; extra == \"science\"",
          "Pillow; extra == \"science\"",
          "python-dateutil>=2.1; extra == \"science\"",
          "PyYAML>=5.4; extra == \"science\"",
          "scipy; extra == \"science\"",
          "setuptools; extra == \"science\"",
          "simplejson>=3.3.0; extra == \"science\"",
          "sortedcontainers>=1.5.9; extra == \"science\"",
          "transformers; extra == \"science\"",
          "urllib3>=1.26; extra == \"science\"",
          "biopython; extra == \"science\"",
          "iopath; extra == \"science\"",
          "ipdb; extra == \"science\"",
          "lmdb; extra == \"science\"",
          "ml_collections; extra == \"science\"",
          "scipy; extra == \"science\"",
          "tensorboardX; extra == \"science\"",
          "tokenizers; extra == \"science\"",
          "addict; extra == \"audio-asr\"",
          "attrs; extra == \"audio-asr\"",
          "datasets<=3.6.0,>=3.0.0; extra == \"audio-asr\"",
          "einops; extra == \"audio-asr\"",
          "Pillow; extra == \"audio-asr\"",
          "python-dateutil>=2.1; extra == \"audio-asr\"",
          "PyYAML>=5.4; extra == \"audio-asr\"",
          "scipy; extra == \"audio-asr\"",
          "setuptools; extra == \"audio-asr\"",
          "simplejson>=3.3.0; extra == \"audio-asr\"",
          "sortedcontainers>=1.5.9; extra == \"audio-asr\"",
          "transformers; extra == \"audio-asr\"",
          "urllib3>=1.26; extra == \"audio-asr\"",
          "funasr>=1.0.0; extra == \"audio-asr\"",
          "addict; extra == \"audio-codec\"",
          "attrs; extra == \"audio-codec\"",
          "datasets<=3.6.0,>=3.0.0; extra == \"audio-codec\"",
          "einops; extra == \"audio-codec\"",
          "Pillow; extra == \"audio-codec\"",
          "python-dateutil>=2.1; extra == \"audio-codec\"",
          "PyYAML>=5.4; extra == \"audio-codec\"",
          "scipy; extra == \"audio-codec\"",
          "setuptools; extra == \"audio-codec\"",
          "simplejson>=3.3.0; extra == \"audio-codec\"",
          "sortedcontainers>=1.5.9; extra == \"audio-codec\"",
          "transformers; extra == \"audio-codec\"",
          "urllib3>=1.26; extra == \"audio-codec\"",
          "ms-funcodec>=0.2.0; extra == \"audio-codec\"",
          "addict; extra == \"audio-tts\"",
          "attrs; extra == \"audio-tts\"",
          "datasets<=3.6.0,>=3.0.0; extra == \"audio-tts\"",
          "einops; extra == \"audio-tts\"",
          "Pillow; extra == \"audio-tts\"",
          "python-dateutil>=2.1; extra == \"audio-tts\"",
          "PyYAML>=5.4; extra == \"audio-tts\"",
          "scipy; extra == \"audio-tts\"",
          "setuptools; extra == \"audio-tts\"",
          "simplejson>=3.3.0; extra == \"audio-tts\"",
          "sortedcontainers>=1.5.9; extra == \"audio-tts\"",
          "transformers; extra == \"audio-tts\"",
          "urllib3>=1.26; extra == \"audio-tts\"",
          "bitstring; extra == \"audio-tts\"",
          "greenlet>=1.1.2; extra == \"audio-tts\"",
          "inflect; extra == \"audio-tts\"",
          "jedi>=0.18.1; extra == \"audio-tts\"",
          "kantts; extra == \"audio-tts\"",
          "librosa==0.10.1; extra == \"audio-tts\"",
          "lxml; extra == \"audio-tts\"",
          "matplotlib; extra == \"audio-tts\"",
          "msgpack>=1.0.4; extra == \"audio-tts\"",
          "parso>=0.8.3; extra == \"audio-tts\"",
          "pexpect>=4.8.0; extra == \"audio-tts\"",
          "pickleshare>=0.7.5; extra == \"audio-tts\"",
          "prompt-toolkit>=3.0.30; extra == \"audio-tts\"",
          "protobuf; extra == \"audio-tts\"",
          "ptflops; extra == \"audio-tts\"",
          "ptyprocess>=0.7.0; extra == \"audio-tts\"",
          "pygments>=2.12.0; extra == \"audio-tts\"",
          "pytorch_wavelets; extra == \"audio-tts\"",
          "PyWavelets>=1.0.0; extra == \"audio-tts\"",
          "scikit-learn; extra == \"audio-tts\"",
          "sox; extra == \"audio-tts\"",
          "tensorboardx; extra == \"audio-tts\"",
          "tqdm; extra == \"audio-tts\"",
          "traitlets>=5.3.0; extra == \"audio-tts\"",
          "unidecode; extra == \"audio-tts\"",
          "wcwidth>=0.2.5; extra == \"audio-tts\"",
          "addict; extra == \"audio-kws\"",
          "attrs; extra == \"audio-kws\"",
          "datasets<=3.6.0,>=3.0.0; extra == \"audio-kws\"",
          "einops; extra == \"audio-kws\"",
          "Pillow; extra == \"audio-kws\"",
          "python-dateutil>=2.1; extra == \"audio-kws\"",
          "PyYAML>=5.4; extra == \"audio-kws\"",
          "scipy; extra == \"audio-kws\"",
          "setuptools; extra == \"audio-kws\"",
          "simplejson>=3.3.0; extra == \"audio-kws\"",
          "sortedcontainers>=1.5.9; extra == \"audio-kws\"",
          "transformers; extra == \"audio-kws\"",
          "urllib3>=1.26; extra == \"audio-kws\"",
          "kaldiio; extra == \"audio-kws\"",
          "matplotlib; extra == \"audio-kws\"",
          "py_sound_connect>=0.1; extra == \"audio-kws\"",
          "scipy; extra == \"audio-kws\"",
          "SoundFile>0.10; extra == \"audio-kws\"",
          "tensorboardX; extra == \"audio-kws\"",
          "addict; extra == \"audio-signal\"",
          "attrs; extra == \"audio-signal\"",
          "datasets<=3.6.0,>=3.0.0; extra == \"audio-signal\"",
          "einops; extra == \"audio-signal\"",
          "Pillow; extra == \"audio-signal\"",
          "python-dateutil>=2.1; extra == \"audio-signal\"",
          "PyYAML>=5.4; extra == \"audio-signal\"",
          "scipy; extra == \"audio-signal\"",
          "setuptools; extra == \"audio-signal\"",
          "simplejson>=3.3.0; extra == \"audio-signal\"",
          "sortedcontainers>=1.5.9; extra == \"audio-signal\"",
          "transformers; extra == \"audio-signal\"",
          "urllib3>=1.26; extra == \"audio-signal\"",
          "hdbscan; extra == \"audio-signal\"",
          "hyperpyyaml; extra == \"audio-signal\"",
          "librosa==0.10.1; extra == \"audio-signal\"",
          "MinDAEC==0.0.2; extra == \"audio-signal\"",
          "mir_eval>=0.7; extra == \"audio-signal\"",
          "rotary_embedding_torch>=0.1.5; extra == \"audio-signal\"",
          "scipy; extra == \"audio-signal\"",
          "SoundFile>0.10; extra == \"audio-signal\"",
          "speechbrain>=0.5.12; extra == \"audio-signal\"",
          "torchaudio; extra == \"audio-signal\"",
          "tqdm; extra == \"audio-signal\"",
          "umap-learn; extra == \"audio-signal\"",
          "addict; extra == \"audio\"",
          "attrs; extra == \"audio\"",
          "datasets<=3.6.0,>=3.0.0; extra == \"audio\"",
          "einops; extra == \"audio\"",
          "Pillow; extra == \"audio\"",
          "python-dateutil>=2.1; extra == \"audio\"",
          "PyYAML>=5.4; extra == \"audio\"",
          "scipy; extra == \"audio\"",
          "setuptools; extra == \"audio\"",
          "simplejson>=3.3.0; extra == \"audio\"",
          "sortedcontainers>=1.5.9; extra == \"audio\"",
          "transformers; extra == \"audio\"",
          "urllib3>=1.26; extra == \"audio\"",
          "funasr>=1.0.0; extra == \"audio\"",
          "ms-funcodec>=0.2.0; extra == \"audio\"",
          "bitstring; extra == \"audio\"",
          "greenlet>=1.1.2; extra == \"audio\"",
          "inflect; extra == \"audio\"",
          "jedi>=0.18.1; extra == \"audio\"",
          "kantts; extra == \"audio\"",
          "librosa==0.10.1; extra == \"audio\"",
          "lxml; extra == \"audio\"",
          "matplotlib; extra == \"audio\"",
          "msgpack>=1.0.4; extra == \"audio\"",
          "parso>=0.8.3; extra == \"audio\"",
          "pexpect>=4.8.0; extra == \"audio\"",
          "pickleshare>=0.7.5; extra == \"audio\"",
          "prompt-toolkit>=3.0.30; extra == \"audio\"",
          "protobuf; extra == \"audio\"",
          "ptflops; extra == \"audio\"",
          "ptyprocess>=0.7.0; extra == \"audio\"",
          "pygments>=2.12.0; extra == \"audio\"",
          "pytorch_wavelets; extra == \"audio\"",
          "PyWavelets>=1.0.0; extra == \"audio\"",
          "scikit-learn; extra == \"audio\"",
          "sox; extra == \"audio\"",
          "tensorboardx; extra == \"audio\"",
          "tqdm; extra == \"audio\"",
          "traitlets>=5.3.0; extra == \"audio\"",
          "unidecode; extra == \"audio\"",
          "wcwidth>=0.2.5; extra == \"audio\"",
          "kaldiio; extra == \"audio\"",
          "matplotlib; extra == \"audio\"",
          "py_sound_connect>=0.1; extra == \"audio\"",
          "scipy; extra == \"audio\"",
          "SoundFile>0.10; extra == \"audio\"",
          "tensorboardX; extra == \"audio\"",
          "hdbscan; extra == \"audio\"",
          "hyperpyyaml; extra == \"audio\"",
          "librosa==0.10.1; extra == \"audio\"",
          "MinDAEC==0.0.2; extra == \"audio\"",
          "mir_eval>=0.7; extra == \"audio\"",
          "rotary_embedding_torch>=0.1.5; extra == \"audio\"",
          "scipy; extra == \"audio\"",
          "SoundFile>0.10; extra == \"audio\"",
          "speechbrain>=0.5.12; extra == \"audio\"",
          "torchaudio; extra == \"audio\"",
          "tqdm; extra == \"audio\"",
          "umap-learn; extra == \"audio\"",
          "filelock; extra == \"all\"",
          "requests>=2.25; extra == \"all\"",
          "setuptools; extra == \"all\"",
          "tqdm>=4.64.0; extra == \"all\"",
          "urllib3>=1.26; extra == \"all\"",
          "addict; extra == \"all\"",
          "attrs; extra == \"all\"",
          "datasets<=3.6.0,>=3.0.0; extra == \"all\"",
          "einops; extra == \"all\"",
          "oss2; extra == \"all\"",
          "Pillow; extra == \"all\"",
          "python-dateutil>=2.1; extra == \"all\"",
          "scipy; extra == \"all\"",
          "setuptools; extra == \"all\"",
          "simplejson>=3.3.0; extra == \"all\"",
          "sortedcontainers>=1.5.9; extra == \"all\"",
          "urllib3>=1.26; extra == \"all\"",
          "addict; extra == \"all\"",
          "attrs; extra == \"all\"",
          "datasets<=3.6.0,>=3.0.0; extra == \"all\"",
          "einops; extra == \"all\"",
          "Pillow; extra == \"all\"",
          "python-dateutil>=2.1; extra == \"all\"",
          "PyYAML>=5.4; extra == \"all\"",
          "scipy; extra == \"all\"",
          "setuptools; extra == \"all\"",
          "simplejson>=3.3.0; extra == \"all\"",
          "sortedcontainers>=1.5.9; extra == \"all\"",
          "transformers; extra == \"all\"",
          "urllib3>=1.26; extra == \"all\"",
          "accelerate; extra == \"all\"",
          "albumentations>=1.0.3; extra == \"all\"",
          "av>=9.2.0; extra == \"all\"",
          "bmt_clipit>=1.0; extra == \"all\"",
          "chumpy; extra == \"all\"",
          "clip>=1.0; extra == \"all\"",
          "control_ldm; extra == \"all\"",
          "ddpm_guided_diffusion==0.0.0; extra == \"all\"",
          "diffusers; extra == \"all\"",
          "easydict; extra == \"all\"",
          "edit_distance; extra == \"all\"",
          "face_alignment>=1.3.5; extra == \"all\"",
          "fairscale>=0.4.1; extra == \"all\"",
          "fastai>=1.0.51; extra == \"all\"",
          "ffmpeg>=1.4; extra == \"all\"",
          "ffmpeg-python>=0.2.0; extra == \"all\"",
          "ftfy; extra == \"all\"",
          "fvcore; extra == \"all\"",
          "imageio>=2.9.0; extra == \"all\"",
          "imageio-ffmpeg>=0.4.2; extra == \"all\"",
          "imgaug>=0.4.0; extra == \"all\"",
          "kornia>=0.5.0; extra == \"all\"",
          "lmdb; extra == \"all\"",
          "lpips; extra == \"all\"",
          "matplotlib>=3.8.0; extra == \"all\"",
          "ml_collections; extra == \"all\"",
          "mmcls>=0.21.0; extra == \"all\"",
          "mmdet<=2.28.2,>=2.25.0; extra == \"all\"",
          "mmdet3d==1.0.0a1; extra == \"all\"",
          "mmsegmentation<=0.30.0; extra == \"all\"",
          "moviepy==1.0.3; extra == \"all\"",
          "nerfacc==0.2.2; extra == \"all\"",
          "networkx; extra == \"all\"",
          "numba; extra == \"all\"",
          "omegaconf; extra == \"all\"",
          "onnx; extra == \"all\"",
          "onnxruntime>=1.10; extra == \"all\"",
          "onnxsim; extra == \"all\"",
          "open-clip-torch>=2.7.0; extra == \"all\"",
          "opencv-python; extra == \"all\"",
          "paint_ldm; extra == \"all\"",
          "pandas; extra == \"all\"",
          "panopticapi; extra == \"all\"",
          "Pillow>=6.2.0; extra == \"all\"",
          "plyfile>=0.7.4; extra == \"all\"",
          "psutil; extra == \"all\"",
          "pyclipper; extra == \"all\"",
          "PyMCubes<=0.1.4; extra == \"all\"",
          "pytorch-lightning; extra == \"all\"",
          "regex; extra == \"all\"",
          "scikit-image; extra == \"all\"",
          "scikit-learn; extra == \"all\"",
          "shapely; extra == \"all\"",
          "shotdetect_scenedetect_lgss==0.0.4; extra == \"all\"",
          "smplx; extra == \"all\"",
          "tensorflow-estimator>=1.15.1; extra == \"all\"",
          "tf_slim; extra == \"all\"",
          "thop; extra == \"all\"",
          "timm>=0.4.9; extra == \"all\"",
          "torch-scatter; extra == \"all\"",
          "torchmetrics>=0.6.2; extra == \"all\"",
          "torchsummary>=1.5.1; extra == \"all\"",
          "torchvision; extra == \"all\"",
          "tqdm; extra == \"all\"",
          "transformers>=4.26.0; extra == \"all\"",
          "trimesh; extra == \"all\"",
          "ujson; extra == \"all\"",
          "utils; extra == \"all\"",
          "videofeatures_clipit>=1.0; extra == \"all\"",
          "yacs; extra == \"all\"",
          "boto3; extra == \"all\"",
          "embeddings; extra == \"all\"",
          "filelock; extra == \"all\"",
          "ftfy; extra == \"all\"",
          "jieba>=0.42.1; extra == \"all\"",
          "matplotlib; extra == \"all\"",
          "megatron_util; extra == \"all\"",
          "nltk; extra == \"all\"",
          "pandas; extra == \"all\"",
          "protobuf<3.21.0,>=3.19.0; extra == \"all\"",
          "pythainlp; extra == \"all\"",
          "pyvi; extra == \"all\"",
          "regex; extra == \"all\"",
          "rouge; extra == \"all\"",
          "sacremoses>=0.0.41; extra == \"all\"",
          "scikit_learn; extra == \"all\"",
          "sentencepiece; extra == \"all\"",
          "seqeval; extra == \"all\"",
          "spacy<=3.7.0,>=2.3.5; extra == \"all\"",
          "stanza; extra == \"all\"",
          "subword_nmt>=0.3.8; extra == \"all\"",
          "termcolor; extra == \"all\"",
          "tokenizers; extra == \"all\"",
          "transformers>=4.12.0; extra == \"all\"",
          "zhconv; extra == \"all\"",
          "accelerate; extra == \"all\"",
          "cloudpickle; extra == \"all\"",
          "decord>=0.6.0; extra == \"all\"",
          "diffusers>=0.25.0; extra == \"all\"",
          "fairseq-fixed==0.12.3.1; extra == \"all\"",
          "ftfy>=6.0.3; extra == \"all\"",
          "librosa==0.10.1; extra == \"all\"",
          "opencv-python; extra == \"all\"",
          "pycocoevalcap>=1.2; extra == \"all\"",
          "pycocotools>=2.0.4; extra == \"all\"",
          "pydot; extra == \"all\"",
          "pytorch_lightning<=1.7.7; extra == \"all\"",
          "rapidfuzz; extra == \"all\"",
          "rouge_score<=0.0.4; extra == \"all\"",
          "sacrebleu; extra == \"all\"",
          "safetensors; extra == \"all\"",
          "soundfile; extra == \"all\"",
          "taming-transformers-rom1504; extra == \"all\"",
          "timm; extra == \"all\"",
          "tokenizers; extra == \"all\"",
          "torchvision; extra == \"all\"",
          "transformers>=4.27.1; extra == \"all\"",
          "unicodedata2; extra == \"all\"",
          "zhconv; extra == \"all\"",
          "biopython; extra == \"all\"",
          "iopath; extra == \"all\"",
          "ipdb; extra == \"all\"",
          "lmdb; extra == \"all\"",
          "ml_collections; extra == \"all\"",
          "scipy; extra == \"all\"",
          "tensorboardX; extra == \"all\"",
          "tokenizers; extra == \"all\"",
          "fastapi; extra == \"all\"",
          "sse-starlette; extra == \"all\"",
          "uvicorn; extra == \"all\""
        ],
        "requires_python": ">=3.9",
        "project_url": [
          "Homepage, https://github.com/modelscope/modelscope"
        ],
        "provides_extra": [
          "hub",
          "datasets",
          "framework",
          "server",
          "docs",
          "tests",
          "cv",
          "nlp",
          "multi-modal",
          "science",
          "audio-asr",
          "audio-codec",
          "audio-tts",
          "audio-kws",
          "audio-signal",
          "audio",
          "all"
        ],
        "description": "\n<p align=\"center\">\n    <br>\n    <img src=\"https://modelscope.oss-cn-beijing.aliyuncs.com/modelscope.gif\" width=\"400\"/>\n    <br>\n<p>\n\n<div align=\"center\">\n\n[![PyPI](https://img.shields.io/pypi/v/modelscope)](https://pypi.org/project/modelscope/)\n<!-- [![Documentation Status](https://readthedocs.org/projects/easy-cv/badge/?version=latest)](https://easy-cv.readthedocs.io/en/latest/) -->\n[![license](https://img.shields.io/github/license/modelscope/modelscope.svg)](https://github.com/modelscope/modelscope/blob/master/LICENSE)\n[![open issues](https://isitmaintained.com/badge/open/modelscope/modelscope.svg)](https://github.com/modelscope/modelscope/issues)\n[![GitHub pull-requests](https://img.shields.io/github/issues-pr/modelscope/modelscope.svg)](https://GitHub.com/modelscope/modelscope/pull/)\n[![GitHub latest commit](https://badgen.net/github/last-commit/modelscope/modelscope)](https://GitHub.com/modelscope/modelscope/commit/)\n[![Leaderboard](https://img.shields.io/badge/ModelScope-Check%20Your%20Contribution-orange)](https://opensource.alibaba.com/contribution_leaderboard/details?projectValue=modelscope)\n\n<!-- [![GitHub contributors](https://img.shields.io/github/contributors/modelscope/modelscope.svg)](https://GitHub.com/modelscope/modelscope/graphs/contributors/) -->\n<!-- [![PRs Welcome](https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=flat-square)](http://makeapullrequest.com) -->\n[Discord](https://discord.gg/FMupRv4jUR)\n\n<h4 align=\"center\">\n<a href=\"https://trendshift.io/repositories/4784\" target=\"_blank\"><img src=\"https://trendshift.io/api/badge/repositories/4784\" alt=\"modelscope%2Fmodelscope | Trendshift\" style=\"width: 250px; height: 55px;\" width=\"250\" height=\"55\"/></a>\n</h4>\n\n<h4 align=\"center\">\n    <p>\n        <b>English</b> |\n        <a href=\"https://github.com/modelscope/modelscope/blob/master/README_zh.md\">‰∏≠Êñá</a> |\n        <a href=\"https://github.com/modelscope/modelscope/blob/master/README_ja.md\">Êó•Êú¨Ë™û</a>\n    <p>\n</h4>\n\n\n</div>\n\n# Introduction\n\n[ModelScope]( https://www.modelscope.cn) is built upon the notion of ‚ÄúModel-as-a-Service‚Äù (MaaS). It seeks to bring together most advanced machine learning models from the AI community, and streamlines the process of leveraging AI models in real-world applications. The core ModelScope library open-sourced in this repository provides the interfaces and implementations that allow developers to perform  model inference, training and evaluation.\n\n\nIn particular, with rich layers of API-abstraction, the ModelScope library offers unified experience to explore state-of-the-art models spanning across domains such as CV, NLP, Speech, Multi-Modality, and Scientific-computation. Model contributors of different areas can integrate models into the ModelScope ecosystem through the layered-APIs, allowing easy and unified access to their models. Once integrated, model inference, fine-tuning, and evaluations can be done with only a few lines of codes. In the meantime, flexibilities are also provided so that different components in the model applications can be customized wherever necessary.\n\nApart from harboring implementations of a wide range of different models, ModelScope library also enables the necessary interactions with ModelScope backend services, particularly with the Model-Hub and Dataset-Hub. Such interactions facilitate management of  various entities (models and datasets) to be performed seamlessly under-the-hood, including entity lookup, version control, cache management, and many others.\n\n# Models and Online Accessibility\n\nHundreds of models are made publicly available on [ModelScope]( https://www.modelscope.cn)  (700+ and counting), covering the latest development in areas such as NLP, CV, Audio, Multi-modality, and AI for Science, etc. Many of these models represent the SOTA in their specific fields, and made their open-sourced debut on ModelScope. Users can visit ModelScope([modelscope.cn](http://www.modelscope.cn)) and experience first-hand how these models perform via online experience, with just a few clicks. Immediate developer-experience is also possible through the ModelScope Notebook, which is backed by ready-to-use CPU/GPU development environment in the cloud - only one click away on [ModelScope](https://www.modelscope.cn).\n\n\n<p align=\"center\">\n    <br>\n    <img src=\"data/resource/inference.gif\" width=\"1024\"/>\n    <br>\n<p>\n\nSome representative examples include:\n\nLLM:\n\n* [Yi-1.5-34B-Chat](https://modelscope.cn/models/01ai/Yi-1.5-34B-Chat/summary)\n\n* [Qwen1.5-110B-Chat](https://modelscope.cn/models/qwen/Qwen1.5-110B-Chat/summary)\n\n* [DeepSeek-V2-Chat](https://modelscope.cn/models/deepseek-ai/DeepSeek-V2-Chat/summary)\n\n* [Ziya2-13B-Chat](https://modelscope.cn/models/Fengshenbang/Ziya2-13B-Chat/summary)\n\n* [Meta-Llama-3-8B-Instruct](https://modelscope.cn/models/LLM-Research/Meta-Llama-3-8B-Instruct/summary)\n\n* [Phi-3-mini-128k-instruct](https://modelscope.cn/models/LLM-Research/Phi-3-mini-128k-instruct/summary)\n\n\nMulti-Modal:\n\n* [Qwen-VL-Chat](https://modelscope.cn/models/qwen/Qwen-VL-Chat/summary)\n\n* [Yi-VL-6B](https://modelscope.cn/models/01ai/Yi-VL-6B/summary)\n\n* [InternVL-Chat-V1-5](https://modelscope.cn/models/AI-ModelScope/InternVL-Chat-V1-5/summary)\n\n* [deepseek-vl-7b-chat](https://modelscope.cn/models/deepseek-ai/deepseek-vl-7b-chat/summary)\n\n* [OpenSoraPlan](https://modelscope.cn/models/AI-ModelScope/Open-Sora-Plan-v1.0.0/summary)\n\n* [OpenSora](https://modelscope.cn/models/luchentech/OpenSora-STDiT-v1-HQ-16x512x512/summary)\n\n* [I2VGen-XL](https://modelscope.cn/models/iic/i2vgen-xl/summary)\n\nCV:\n\n* [DamoFD Face Detection Key Point Model - 0.5G](https://modelscope.cn/models/damo/cv_ddsar_face-detection_iclr23-damofd/summary)\n\n* [BSHM Portrait Matting](https://modelscope.cn/models/damo/cv_unet_image-matting/summary)\n\n* [DCT-Net Portrait Cartoonization - 3D](https://modelscope.cn/models/damo/cv_unet_person-image-cartoon-3d_compound-models/summary)\n\n* [DCT-Net Portrait Cartoonization Model - 3D](https://modelscope.cn/models/damo/face_chain_control_model/summary)\n\n* [DuGuang - Text Recognition - Line Recognition Model - Chinese and English - General Domain](https://modelscope.cn/models/damo/cv_convnextTiny_ocr-recognition-general_damo/summary)\n\n* [DuGuang - Text Recognition - Line Recognition Model - Chinese and English - General Domain](https://modelscope.cn/models/damo/cv_resnet18_ocr-detection-line-level_damo/summary)\n\n* [LaMa Image Inpainting](https://modelscope.cn/models/damo/cv_fft_inpainting_lama/summary)\n\n\nAudio:\n\n* [Paraformer Speech Recognition - Chinese - General - 16k - Offline - Large - Long Audio Version](https://modelscope.cn/models/damo/speech_paraformer-large-vad-punc_asr_nat-zh-cn-16k-common-vocab8404-pytorch/summary)\n\n* [FSMN Voice Endpoint Detection - Chinese - General - 16k - onnx](https://modelscope.cn/models/damo/speech_fsmn_vad_zh-cn-16k-common-onnx/summary)\n\n* [Monotonic-Aligner Speech Timestamp Prediction - 16k - Offline](https://modelscope.cn/models/damo/speech_timestamp_prediction-v1-16k-offline/summary)\n\n* [CT-Transformer Punctuation - Chinese - General - onnx](https://modelscope.cn/models/damo/punc_ct-transformer_zh-cn-common-vocab272727-onnx/summary)\n\n* [Speech Synthesis - Chinese - Multiple Emotions Domain - 16k - Multiple Speakers](https://modelscope.cn/models/damo/speech_sambert-hifigan_tts_zh-cn_16k/summary)\n\n* [CAM++ Speaker Verification - Chinese - General - 200k-Spkrs](https://modelscope.cn/models/damo/speech_campplus_sv_zh-cn_16k-common/summary)\n\n\n\nAI for Science:\n\n* [uni-fold-monomer](https://modelscope.cn/models/DPTech/uni-fold-monomer/summary)\n\n* [uni-fold-multimer](https://modelscope.cn/models/DPTech/uni-fold-multimer/summary)\n\n**Note:** Most models on ModelScope are public and can be downloaded directly from the [website](https://modelscope.cn/), please refer to instructions for [model download](https://modelscope.cn/docs/%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%B8%8B%E8%BD%BD), for downloading models with api provided by modelscope library or git.\n\n# QuickTour\n\nWe provide unified interface for inference using `pipeline`, fine-tuning and evaluation using `Trainer` for different tasks.\n\nFor any given task with any type of input (image, text, audio, video...), inference pipeline can be implemented with only a few lines of code, which will automatically load the underlying model to get inference result, as is exemplified below:\n\n```python\n>>> from modelscope.pipelines import pipeline\n>>> word_segmentation = pipeline('word-segmentation',model='damo/nlp_structbert_word-segmentation_chinese-base')\n>>> word_segmentation('‰ªäÂ§©Â§©Ê∞î‰∏çÈîôÔºåÈÄÇÂêàÂá∫ÂéªÊ∏∏Áé©')\n{'output': '‰ªäÂ§© Â§©Ê∞î ‰∏çÈîô Ôºå ÈÄÇÂêà Âá∫Âéª Ê∏∏Áé©'}\n```\n\nGiven an image, portrait matting (aka. background-removal) can be accomplished with the following code snippet:\n\n![image](data/resource/portrait_input.png)\n\n```python\n>>> import cv2\n>>> from modelscope.pipelines import pipeline\n\n>>> portrait_matting = pipeline('portrait-matting')\n>>> result = portrait_matting('https://modelscope.oss-cn-beijing.aliyuncs.com/test/images/image_matting.png')\n>>> cv2.imwrite('result.png', result['output_img'])\n```\n\nThe output image with the background removed is:\n![image](data/resource/portrait_output.png)\n\n\nFine-tuning and evaluation can also be done with a few more lines of code to set up training dataset and trainer, with the heavy-lifting work of training and evaluation a model encapsulated in the implementation of  `trainer.train()` and\n`trainer.evaluate()`  interfaces.\n\nFor example, the gpt3 base model (1.3B) can be fine-tuned with the chinese-poetry dataset, resulting in a model that can be used for chinese-poetry generation.\n\n```python\n>>> from modelscope.metainfo import Trainers\n>>> from modelscope.msdatasets import MsDataset\n>>> from modelscope.trainers import build_trainer\n\n>>> train_dataset = MsDataset.load('chinese-poetry-collection', split='train'). remap_columns({'text1': 'src_txt'})\n>>> eval_dataset = MsDataset.load('chinese-poetry-collection', split='test').remap_columns({'text1': 'src_txt'})\n>>> max_epochs = 10\n>>> tmp_dir = './gpt3_poetry'\n\n>>> kwargs = dict(\n     model='damo/nlp_gpt3_text-generation_1.3B',\n     train_dataset=train_dataset,\n     eval_dataset=eval_dataset,\n     max_epochs=max_epochs,\n     work_dir=tmp_dir)\n\n>>> trainer = build_trainer(name=Trainers.gpt3_trainer, default_args=kwargs)\n>>> trainer.train()\n```\n\n# Why should I use ModelScope library\n\n1. A unified and concise user interface is abstracted for different tasks and different models. Model inferences and training can be implemented by as few as 3 and 10 lines of code, respectively. It is convenient for users to explore models in different fields in the ModelScope community. All models integrated into ModelScope are ready to use, which makes it easy to get started with AI, in both educational and industrial settings.\n\n2. ModelScope offers a model-centric development and application experience. It streamlines the support for model training, inference, export and deployment, and facilitates users to build their own MLOps based on the ModelScope ecosystem.\n\n3. For the model inference and training process, a modular design is put in place, and a wealth of functional module implementations are provided, which is convenient for users to customize their own model inference, training and other processes.\n\n4. For distributed model training, especially for large models, it provides rich training strategy support, including data parallel, model parallel, hybrid parallel and so on.\n\n# Installation\n\n## Docker\n\nModelScope Library currently supports popular deep learning framework for model training and inference, including PyTorch, TensorFlow and ONNX. All releases are tested and run on Python 3.7+, Pytorch 1.8+, Tensorflow1.15 or Tensorflow2.0+.\n\nTo allow out-of-box usage for all the models on ModelScope, official docker images are provided for all releases. Based on the docker image, developers can skip all environment installation and configuration and use it directly. Currently, the latest version of the CPU image and GPU image can be obtained from:\n\nCPU docker image\n```shell\n# py37\nregistry.cn-hangzhou.aliyuncs.com/modelscope-repo/modelscope:ubuntu20.04-py37-torch1.11.0-tf1.15.5-1.6.1\n\n# py38\nregistry.cn-hangzhou.aliyuncs.com/modelscope-repo/modelscope:ubuntu20.04-py38-torch2.0.1-tf2.13.0-1.9.5\n```\n\nGPU docker image\n```shell\n# py37\nregistry.cn-hangzhou.aliyuncs.com/modelscope-repo/modelscope:ubuntu20.04-cuda11.3.0-py37-torch1.11.0-tf1.15.5-1.6.1\n\n# py38\nregistry.cn-hangzhou.aliyuncs.com/modelscope-repo/modelscope:ubuntu20.04-cuda11.8.0-py38-torch2.0.1-tf2.13.0-1.9.5\n```\n\n## Setup Local Python Environment\n\nOne can also set up local ModelScope environment using pip and conda.  ModelScope supports python3.7 and above.\nWe suggest [anaconda](https://docs.anaconda.com/anaconda/install/) for creating local python environment:\n\n```shell\nconda create -n modelscope python=3.8\nconda activate modelscope\n```\n\nPyTorch or TensorFlow can be installed separately according to each model's requirements.\n* Install pytorch [doc](https://pytorch.org/get-started/locally/)\n* Install tensorflow [doc](https://www.tensorflow.org/install/pip)\n\nAfter installing the necessary machine-learning framework, you can install modelscope library as follows:\n\nIf you only want to play around with the modelscope framework, of trying out model/dataset download, you can install the core modelscope components:\n```shell\npip install modelscope\n```\n\nIf you want to use multi-modal models:\n```shell\npip install modelscope[multi-modal]\n```\n\nIf you want to use nlp models:\n```shell\npip install modelscope[nlp] -f https://modelscope.oss-cn-beijing.aliyuncs.com/releases/repo.html\n```\n\nIf you want to use cv models:\n```shell\npip install modelscope[cv] -f https://modelscope.oss-cn-beijing.aliyuncs.com/releases/repo.html\n```\n\nIf you want to use audio models:\n```shell\npip install modelscope[audio] -f https://modelscope.oss-cn-beijing.aliyuncs.com/releases/repo.html\n```\n\nIf you want to use science models:\n```shell\npip install modelscope[science] -f https://modelscope.oss-cn-beijing.aliyuncs.com/releases/repo.html\n```\n\n`Notes`:\n1. Currently, some audio-task models only support python3.7, tensorflow1.15.4 Linux environments. Most other models can be installed and used on Windows and Mac (x86).\n\n2. Some models in the audio field use the third-party library SoundFile for wav file processing. On the Linux system, users need to manually install libsndfile of SoundFile([doc link](https://github.com/bastibe/python-soundfile#installation)). On Windows and MacOS, it will be installed automatically without user operation. For example, on Ubuntu, you can use following commands:\n    ```shell\n    sudo apt-get update\n    sudo apt-get install libsndfile1\n    ```\n\n3. Some models in computer vision need mmcv-full, you can refer to mmcv [installation guide](https://github.com/open-mmlab/mmcv#installation), a minimal installation is as follows:\n\n    ```shell\n    pip uninstall mmcv # if you have installed mmcv, uninstall it\n    pip install -U openmim\n    mim install mmcv-full\n    ```\n\n\n\n# Learn More\n\nWe  provide additional documentations including:\n* [More detailed Installation Guide](https://modelscope.cn/docs/%E7%8E%AF%E5%A2%83%E5%AE%89%E8%A3%85)\n* [Introduction to tasks](https://modelscope.cn/docs/%E4%BB%BB%E5%8A%A1%E7%9A%84%E4%BB%8B%E7%BB%8D)\n* [Use pipeline for model inference](https://modelscope.cn/docs/%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%8E%A8%E7%90%86Pipeline)\n* [Finetuning example](https://modelscope.cn/docs/%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%AE%AD%E7%BB%83Train)\n* [Preprocessing of data](https://modelscope.cn/docs/%E6%95%B0%E6%8D%AE%E7%9A%84%E9%A2%84%E5%A4%84%E7%90%86)\n* [Evaluation](https://modelscope.cn/docs/%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%AF%84%E4%BC%B0)\n* [Contribute your own model to ModelScope](https://modelscope.cn/docs/ModelScope%E6%A8%A1%E5%9E%8B%E6%8E%A5%E5%85%A5%E6%B5%81%E7%A8%8B%E6%A6%82%E8%A7%88)\n\n# License\n\nThis project is licensed under the [Apache License (Version 2.0)](https://github.com/modelscope/modelscope/blob/master/LICENSE).\n\n# Citation\n```\n@Misc{modelscope,\n  title = {ModelScope: bring the notion of Model-as-a-Service to life.},\n  author = {The ModelScope Team},\n  howpublished = {\\url{https://github.com/modelscope/modelscope}},\n  year = {2023}\n}\n```\n"
      }
    },
    {
      "download_info": {
        "url": "https://pypi.tuna.tsinghua.edu.cn/packages/d0/30/dc54f88dd4a2b5dc8a0279bdd7270e735851848b762aeb1c1184ed1f6b14/tqdm-4.67.1-py3-none-any.whl",
        "archive_info": {
          "hash": "sha256=26445eca388f82e72884e0d580d5464cd801a3ea01e63e5601bdff9ba6a48de2",
          "hashes": {
            "sha256": "26445eca388f82e72884e0d580d5464cd801a3ea01e63e5601bdff9ba6a48de2"
          }
        }
      },
      "is_direct": false,
      "is_yanked": false,
      "requested": false,
      "metadata": {
        "metadata_version": "2.1",
        "name": "tqdm",
        "version": "4.67.1",
        "summary": "Fast, Extensible Progress Meter",
        "description_content_type": "text/x-rst",
        "keywords": [
          "progressbar",
          "progressmeter",
          "progress",
          "bar",
          "meter",
          "rate",
          "eta",
          "console",
          "terminal",
          "time"
        ],
        "maintainer_email": "tqdm developers <devs@tqdm.ml>",
        "license": "MPL-2.0 AND MIT",
        "license_file": [
          "LICENCE"
        ],
        "classifier": [
          "Development Status :: 5 - Production/Stable",
          "Environment :: Console",
          "Environment :: MacOS X",
          "Environment :: Other Environment",
          "Environment :: Win32 (MS Windows)",
          "Environment :: X11 Applications",
          "Framework :: IPython",
          "Framework :: Jupyter",
          "Intended Audience :: Developers",
          "Intended Audience :: Education",
          "Intended Audience :: End Users/Desktop",
          "Intended Audience :: Other Audience",
          "Intended Audience :: System Administrators",
          "License :: OSI Approved :: MIT License",
          "License :: OSI Approved :: Mozilla Public License 2.0 (MPL 2.0)",
          "Operating System :: MacOS",
          "Operating System :: MacOS :: MacOS X",
          "Operating System :: Microsoft",
          "Operating System :: Microsoft :: MS-DOS",
          "Operating System :: Microsoft :: Windows",
          "Operating System :: POSIX",
          "Operating System :: POSIX :: BSD",
          "Operating System :: POSIX :: BSD :: FreeBSD",
          "Operating System :: POSIX :: Linux",
          "Operating System :: POSIX :: SunOS/Solaris",
          "Operating System :: Unix",
          "Programming Language :: Python",
          "Programming Language :: Python :: 3",
          "Programming Language :: Python :: 3.7",
          "Programming Language :: Python :: 3.8",
          "Programming Language :: Python :: 3.9",
          "Programming Language :: Python :: 3.10",
          "Programming Language :: Python :: 3.11",
          "Programming Language :: Python :: 3.12",
          "Programming Language :: Python :: 3 :: Only",
          "Programming Language :: Python :: Implementation",
          "Programming Language :: Python :: Implementation :: IronPython",
          "Programming Language :: Python :: Implementation :: PyPy",
          "Programming Language :: Unix Shell",
          "Topic :: Desktop Environment",
          "Topic :: Education :: Computer Aided Instruction (CAI)",
          "Topic :: Education :: Testing",
          "Topic :: Office/Business",
          "Topic :: Other/Nonlisted Topic",
          "Topic :: Software Development :: Build Tools",
          "Topic :: Software Development :: Libraries",
          "Topic :: Software Development :: Libraries :: Python Modules",
          "Topic :: Software Development :: Pre-processors",
          "Topic :: Software Development :: User Interfaces",
          "Topic :: System :: Installation/Setup",
          "Topic :: System :: Logging",
          "Topic :: System :: Monitoring",
          "Topic :: System :: Shells",
          "Topic :: Terminals",
          "Topic :: Utilities"
        ],
        "requires_dist": [
          "colorama; platform_system == \"Windows\"",
          "pytest>=6; extra == \"dev\"",
          "pytest-cov; extra == \"dev\"",
          "pytest-timeout; extra == \"dev\"",
          "pytest-asyncio>=0.24; extra == \"dev\"",
          "nbval; extra == \"dev\"",
          "requests; extra == \"discord\"",
          "slack-sdk; extra == \"slack\"",
          "requests; extra == \"telegram\"",
          "ipywidgets>=6; extra == \"notebook\""
        ],
        "requires_python": ">=3.7",
        "project_url": [
          "homepage, https://tqdm.github.io",
          "repository, https://github.com/tqdm/tqdm",
          "changelog, https://tqdm.github.io/releases",
          "wiki, https://github.com/tqdm/tqdm/wiki"
        ],
        "provides_extra": [
          "dev",
          "discord",
          "slack",
          "telegram",
          "notebook"
        ],
        "description": "|Logo|\n\ntqdm\n====\n\n|Py-Versions| |Versions| |Conda-Forge-Status| |Docker| |Snapcraft|\n\n|Build-Status| |Coverage-Status| |Branch-Coverage-Status| |Codacy-Grade| |Libraries-Rank| |PyPI-Downloads|\n\n|LICENCE| |OpenHub-Status| |binder-demo| |awesome-python|\n\n``tqdm`` derives from the Arabic word *taqaddum* (ÿ™ŸÇÿØŸëŸÖ) which can mean \"progress,\"\nand is an abbreviation for \"I love you so much\" in Spanish (*te quiero demasiado*).\n\nInstantly make your loops show a smart progress meter - just wrap any\niterable with ``tqdm(iterable)``, and you're done!\n\n.. code:: python\n\n    from tqdm import tqdm\n    for i in tqdm(range(10000)):\n        ...\n\n``76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà ¬†¬†¬†¬†¬†¬† | 7568/10000 [00:33<00:10, 229.00it/s]``\n\n``trange(N)`` can be also used as a convenient shortcut for\n``tqdm(range(N))``.\n\n|Screenshot|\n    |Video| |Slides| |Merch|\n\nIt can also be executed as a module with pipes:\n\n.. code:: sh\n\n    $ seq 9999999 | tqdm --bytes | wc -l\n    75.2MB [00:00, 217MB/s]\n    9999999\n\n    $ tar -zcf - docs/ | tqdm --bytes --total `du -sb docs/ | cut -f1` \\\n        > backup.tgz\n     32%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                      | 8.89G/27.9G [00:42<01:31, 223MB/s]\n\nOverhead is low -- about 60ns per iteration (80ns with ``tqdm.gui``), and is\nunit tested against performance regression.\nBy comparison, the well-established\n`ProgressBar <https://github.com/niltonvolpato/python-progressbar>`__ has\nan 800ns/iter overhead.\n\nIn addition to its low overhead, ``tqdm`` uses smart algorithms to predict\nthe remaining time and to skip unnecessary iteration displays, which allows\nfor a negligible overhead in most cases.\n\n``tqdm`` works on any platform\n(Linux, Windows, Mac, FreeBSD, NetBSD, Solaris/SunOS),\nin any console or in a GUI, and is also friendly with IPython/Jupyter notebooks.\n\n``tqdm`` does not require any dependencies (not even ``curses``!), just\nPython and an environment supporting ``carriage return \\r`` and\n``line feed \\n`` control characters.\n\n------------------------------------------\n\n.. contents:: Table of contents\n   :backlinks: top\n   :local:\n\n\nInstallation\n------------\n\nLatest PyPI stable release\n~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n|Versions| |PyPI-Downloads| |Libraries-Dependents|\n\n.. code:: sh\n\n    pip install tqdm\n\nLatest development release on GitHub\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n|GitHub-Status| |GitHub-Stars| |GitHub-Commits| |GitHub-Forks| |GitHub-Updated|\n\nPull and install pre-release ``devel`` branch:\n\n.. code:: sh\n\n    pip install \"git+https://github.com/tqdm/tqdm.git@devel#egg=tqdm\"\n\nLatest Conda release\n~~~~~~~~~~~~~~~~~~~~\n\n|Conda-Forge-Status|\n\n.. code:: sh\n\n    conda install -c conda-forge tqdm\n\nLatest Snapcraft release\n~~~~~~~~~~~~~~~~~~~~~~~~\n\n|Snapcraft|\n\nThere are 3 channels to choose from:\n\n.. code:: sh\n\n    snap install tqdm  # implies --stable, i.e. latest tagged release\n    snap install tqdm  --candidate  # master branch\n    snap install tqdm  --edge  # devel branch\n\nNote that ``snap`` binaries are purely for CLI use (not ``import``-able), and\nautomatically set up ``bash`` tab-completion.\n\nLatest Docker release\n~~~~~~~~~~~~~~~~~~~~~\n\n|Docker|\n\n.. code:: sh\n\n    docker pull tqdm/tqdm\n    docker run -i --rm tqdm/tqdm --help\n\nOther\n~~~~~\n\nThere are other (unofficial) places where ``tqdm`` may be downloaded, particularly for CLI use:\n\n|Repology|\n\n.. |Repology| image:: https://repology.org/badge/tiny-repos/python:tqdm.svg\n   :target: https://repology.org/project/python:tqdm/versions\n\nChangelog\n---------\n\nThe list of all changes is available either on GitHub's Releases:\n|GitHub-Status|, on the\n`wiki <https://github.com/tqdm/tqdm/wiki/Releases>`__, or on the\n`website <https://tqdm.github.io/releases>`__.\n\n\nUsage\n-----\n\n``tqdm`` is very versatile and can be used in a number of ways.\nThe three main ones are given below.\n\nIterable-based\n~~~~~~~~~~~~~~\n\nWrap ``tqdm()`` around any iterable:\n\n.. code:: python\n\n    from tqdm import tqdm\n    from time import sleep\n\n    text = \"\"\n    for char in tqdm([\"a\", \"b\", \"c\", \"d\"]):\n        sleep(0.25)\n        text = text + char\n\n``trange(i)`` is a special optimised instance of ``tqdm(range(i))``:\n\n.. code:: python\n\n    from tqdm import trange\n\n    for i in trange(100):\n        sleep(0.01)\n\nInstantiation outside of the loop allows for manual control over ``tqdm()``:\n\n.. code:: python\n\n    pbar = tqdm([\"a\", \"b\", \"c\", \"d\"])\n    for char in pbar:\n        sleep(0.25)\n        pbar.set_description(\"Processing %s\" % char)\n\nManual\n~~~~~~\n\nManual control of ``tqdm()`` updates using a ``with`` statement:\n\n.. code:: python\n\n    with tqdm(total=100) as pbar:\n        for i in range(10):\n            sleep(0.1)\n            pbar.update(10)\n\nIf the optional variable ``total`` (or an iterable with ``len()``) is\nprovided, predictive stats are displayed.\n\n``with`` is also optional (you can just assign ``tqdm()`` to a variable,\nbut in this case don't forget to ``del`` or ``close()`` at the end:\n\n.. code:: python\n\n    pbar = tqdm(total=100)\n    for i in range(10):\n        sleep(0.1)\n        pbar.update(10)\n    pbar.close()\n\nModule\n~~~~~~\n\nPerhaps the most wonderful use of ``tqdm`` is in a script or on the command\nline. Simply inserting ``tqdm`` (or ``python -m tqdm``) between pipes will pass\nthrough all ``stdin`` to ``stdout`` while printing progress to ``stderr``.\n\nThe example below demonstrate counting the number of lines in all Python files\nin the current directory, with timing information included.\n\n.. code:: sh\n\n    $ time find . -name '*.py' -type f -exec cat \\{} \\; | wc -l\n    857365\n\n    real    0m3.458s\n    user    0m0.274s\n    sys     0m3.325s\n\n    $ time find . -name '*.py' -type f -exec cat \\{} \\; | tqdm | wc -l\n    857366it [00:03, 246471.31it/s]\n    857365\n\n    real    0m3.585s\n    user    0m0.862s\n    sys     0m3.358s\n\nNote that the usual arguments for ``tqdm`` can also be specified.\n\n.. code:: sh\n\n    $ find . -name '*.py' -type f -exec cat \\{} \\; |\n        tqdm --unit loc --unit_scale --total 857366 >> /dev/null\n    100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 857K/857K [00:04<00:00, 246Kloc/s]\n\nBacking up a large directory?\n\n.. code:: sh\n\n    $ tar -zcf - docs/ | tqdm --bytes --total `du -sb docs/ | cut -f1` \\\n      > backup.tgz\n     44%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                   | 153M/352M [00:14<00:18, 11.0MB/s]\n\nThis can be beautified further:\n\n.. code:: sh\n\n    $ BYTES=$(du -sb docs/ | cut -f1)\n    $ tar -cf - docs/ \\\n      | tqdm --bytes --total \"$BYTES\" --desc Processing | gzip \\\n      | tqdm --bytes --total \"$BYTES\" --desc Compressed --position 1 \\\n      > ~/backup.tgz\n    Processing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 352M/352M [00:14<00:00, 30.2MB/s]\n    Compressed:  42%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé            | 148M/352M [00:14<00:19, 10.9MB/s]\n\nOr done on a file level using 7-zip:\n\n.. code:: sh\n\n    $ 7z a -bd -r backup.7z docs/ | grep Compressing \\\n      | tqdm --total $(find docs/ -type f | wc -l) --unit files \\\n      | grep -v Compressing\n    100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 15327/15327 [01:00<00:00, 712.96files/s]\n\nPre-existing CLI programs already outputting basic progress information will\nbenefit from ``tqdm``'s ``--update`` and ``--update_to`` flags:\n\n.. code:: sh\n\n    $ seq 3 0.1 5 | tqdm --total 5 --update_to --null\n    100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5.0/5 [00:00<00:00, 9673.21it/s]\n    $ seq 10 | tqdm --update --null  # 1 + 2 + ... + 10 = 55 iterations\n    55it [00:00, 90006.52it/s]\n\nFAQ and Known Issues\n--------------------\n\n|GitHub-Issues|\n\nThe most common issues relate to excessive output on multiple lines, instead\nof a neat one-line progress bar.\n\n- Consoles in general: require support for carriage return (``CR``, ``\\r``).\n\n  * Some cloud logging consoles which don't support ``\\r`` properly\n    (`cloudwatch <https://github.com/tqdm/tqdm/issues/966>`__,\n    `K8s <https://github.com/tqdm/tqdm/issues/1319>`__) may benefit from\n    ``export TQDM_POSITION=-1``.\n\n- Nested progress bars:\n\n  * Consoles in general: require support for moving cursors up to the\n    previous line. For example,\n    `IDLE <https://github.com/tqdm/tqdm/issues/191#issuecomment-230168030>`__,\n    `ConEmu <https://github.com/tqdm/tqdm/issues/254>`__ and\n    `PyCharm <https://github.com/tqdm/tqdm/issues/203>`__ (also\n    `here <https://github.com/tqdm/tqdm/issues/208>`__,\n    `here <https://github.com/tqdm/tqdm/issues/307>`__, and\n    `here <https://github.com/tqdm/tqdm/issues/454#issuecomment-335416815>`__)\n    lack full support.\n  * Windows: additionally may require the Python module ``colorama``\n    to ensure nested bars stay within their respective lines.\n\n- Unicode:\n\n  * Environments which report that they support unicode will have solid smooth\n    progressbars. The fallback is an ``ascii``-only bar.\n  * Windows consoles often only partially support unicode and thus\n    `often require explicit ascii=True <https://github.com/tqdm/tqdm/issues/454#issuecomment-335416815>`__\n    (also `here <https://github.com/tqdm/tqdm/issues/499>`__). This is due to\n    either normal-width unicode characters being incorrectly displayed as\n    \"wide\", or some unicode characters not rendering.\n\n- Wrapping generators:\n\n  * Generator wrapper functions tend to hide the length of iterables.\n    ``tqdm`` does not.\n  * Replace ``tqdm(enumerate(...))`` with ``enumerate(tqdm(...))`` or\n    ``tqdm(enumerate(x), total=len(x), ...)``.\n    The same applies to ``numpy.ndenumerate``.\n  * Replace ``tqdm(zip(a, b))`` with ``zip(tqdm(a), b)`` or even\n    ``zip(tqdm(a), tqdm(b))``.\n  * The same applies to ``itertools``.\n  * Some useful convenience functions can be found under ``tqdm.contrib``.\n\n- `No intermediate output in docker-compose <https://github.com/tqdm/tqdm/issues/771>`__:\n  use ``docker-compose run`` instead of ``docker-compose up`` and ``tty: true``.\n\n- Overriding defaults via environment variables:\n  e.g. in CI/cloud jobs, ``export TQDM_MININTERVAL=5`` to avoid log spam.\n  This override logic is handled by the ``tqdm.utils.envwrap`` decorator\n  (useful independent of ``tqdm``).\n\nIf you come across any other difficulties, browse and file |GitHub-Issues|.\n\nDocumentation\n-------------\n\n|Py-Versions| |README-Hits| (Since 19 May 2016)\n\n.. code:: python\n\n    class tqdm():\n      \"\"\"\n      Decorate an iterable object, returning an iterator which acts exactly\n      like the original iterable, but prints a dynamically updating\n      progressbar every time a value is requested.\n      \"\"\"\n\n      @envwrap(\"TQDM_\")  # override defaults via env vars\n      def __init__(self, iterable=None, desc=None, total=None, leave=True,\n                   file=None, ncols=None, mininterval=0.1,\n                   maxinterval=10.0, miniters=None, ascii=None, disable=False,\n                   unit='it', unit_scale=False, dynamic_ncols=False,\n                   smoothing=0.3, bar_format=None, initial=0, position=None,\n                   postfix=None, unit_divisor=1000, write_bytes=False,\n                   lock_args=None, nrows=None, colour=None, delay=0):\n\nParameters\n~~~~~~~~~~\n\n* iterable  : iterable, optional  \n    Iterable to decorate with a progressbar.\n    Leave blank to manually manage the updates.\n* desc  : str, optional  \n    Prefix for the progressbar.\n* total  : int or float, optional  \n    The number of expected iterations. If unspecified,\n    len(iterable) is used if possible. If float(\"inf\") or as a last\n    resort, only basic progress statistics are displayed\n    (no ETA, no progressbar).\n    If ``gui`` is True and this parameter needs subsequent updating,\n    specify an initial arbitrary large positive number,\n    e.g. 9e9.\n* leave  : bool, optional  \n    If [default: True], keeps all traces of the progressbar\n    upon termination of iteration.\n    If ``None``, will leave only if ``position`` is ``0``.\n* file  : ``io.TextIOWrapper`` or ``io.StringIO``, optional  \n    Specifies where to output the progress messages\n    (default: sys.stderr). Uses ``file.write(str)`` and ``file.flush()``\n    methods.  For encoding, see ``write_bytes``.\n* ncols  : int, optional  \n    The width of the entire output message. If specified,\n    dynamically resizes the progressbar to stay within this bound.\n    If unspecified, attempts to use environment width. The\n    fallback is a meter width of 10 and no limit for the counter and\n    statistics. If 0, will not print any meter (only stats).\n* mininterval  : float, optional  \n    Minimum progress display update interval [default: 0.1] seconds.\n* maxinterval  : float, optional  \n    Maximum progress display update interval [default: 10] seconds.\n    Automatically adjusts ``miniters`` to correspond to ``mininterval``\n    after long display update lag. Only works if ``dynamic_miniters``\n    or monitor thread is enabled.\n* miniters  : int or float, optional  \n    Minimum progress display update interval, in iterations.\n    If 0 and ``dynamic_miniters``, will automatically adjust to equal\n    ``mininterval`` (more CPU efficient, good for tight loops).\n    If > 0, will skip display of specified number of iterations.\n    Tweak this and ``mininterval`` to get very efficient loops.\n    If your progress is erratic with both fast and slow iterations\n    (network, skipping items, etc) you should set miniters=1.\n* ascii  : bool or str, optional  \n    If unspecified or False, use unicode (smooth blocks) to fill\n    the meter. The fallback is to use ASCII characters \" 123456789#\".\n* disable  : bool, optional  \n    Whether to disable the entire progressbar wrapper\n    [default: False]. If set to None, disable on non-TTY.\n* unit  : str, optional  \n    String that will be used to define the unit of each iteration\n    [default: it].\n* unit_scale  : bool or int or float, optional  \n    If 1 or True, the number of iterations will be reduced/scaled\n    automatically and a metric prefix following the\n    International System of Units standard will be added\n    (kilo, mega, etc.) [default: False]. If any other non-zero\n    number, will scale ``total`` and ``n``.\n* dynamic_ncols  : bool, optional  \n    If set, constantly alters ``ncols`` and ``nrows`` to the\n    environment (allowing for window resizes) [default: False].\n* smoothing  : float, optional  \n    Exponential moving average smoothing factor for speed estimates\n    (ignored in GUI mode). Ranges from 0 (average speed) to 1\n    (current/instantaneous speed) [default: 0.3].\n* bar_format  : str, optional  \n    Specify a custom bar string formatting. May impact performance.\n    [default: '{l_bar}{bar}{r_bar}'], where\n    l_bar='{desc}: {percentage:3.0f}%|' and\n    r_bar='| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, '\n    '{rate_fmt}{postfix}]'\n    Possible vars: l_bar, bar, r_bar, n, n_fmt, total, total_fmt,\n    percentage, elapsed, elapsed_s, ncols, nrows, desc, unit,\n    rate, rate_fmt, rate_noinv, rate_noinv_fmt,\n    rate_inv, rate_inv_fmt, postfix, unit_divisor,\n    remaining, remaining_s, eta.\n    Note that a trailing \": \" is automatically removed after {desc}\n    if the latter is empty.\n* initial  : int or float, optional  \n    The initial counter value. Useful when restarting a progress\n    bar [default: 0]. If using float, consider specifying ``{n:.3f}``\n    or similar in ``bar_format``, or specifying ``unit_scale``.\n* position  : int, optional  \n    Specify the line offset to print this bar (starting from 0)\n    Automatic if unspecified.\n    Useful to manage multiple bars at once (eg, from threads).\n* postfix  : dict or ``*``, optional  \n    Specify additional stats to display at the end of the bar.\n    Calls ``set_postfix(**postfix)`` if possible (dict).\n* unit_divisor  : float, optional  \n    [default: 1000], ignored unless ``unit_scale`` is True.\n* write_bytes  : bool, optional  \n    Whether to write bytes. If (default: False) will write unicode.\n* lock_args  : tuple, optional  \n    Passed to ``refresh`` for intermediate output\n    (initialisation, iterating, and updating).\n* nrows  : int, optional  \n    The screen height. If specified, hides nested bars outside this\n    bound. If unspecified, attempts to use environment height.\n    The fallback is 20.\n* colour  : str, optional  \n    Bar colour (e.g. 'green', '#00ff00').\n* delay  : float, optional  \n    Don't display until [default: 0] seconds have elapsed.\n\nExtra CLI Options\n~~~~~~~~~~~~~~~~~\n\n* delim  : chr, optional  \n    Delimiting character [default: '\\n']. Use '\\0' for null.\n    N.B.: on Windows systems, Python converts '\\n' to '\\r\\n'.\n* buf_size  : int, optional  \n    String buffer size in bytes [default: 256]\n    used when ``delim`` is specified.\n* bytes  : bool, optional  \n    If true, will count bytes, ignore ``delim``, and default\n    ``unit_scale`` to True, ``unit_divisor`` to 1024, and ``unit`` to 'B'.\n* tee  : bool, optional  \n    If true, passes ``stdin`` to both ``stderr`` and ``stdout``.\n* update  : bool, optional  \n    If true, will treat input as newly elapsed iterations,\n    i.e. numbers to pass to ``update()``. Note that this is slow\n    (~2e5 it/s) since every input must be decoded as a number.\n* update_to  : bool, optional  \n    If true, will treat input as total elapsed iterations,\n    i.e. numbers to assign to ``self.n``. Note that this is slow\n    (~2e5 it/s) since every input must be decoded as a number.\n* null  : bool, optional  \n    If true, will discard input (no stdout).\n* manpath  : str, optional  \n    Directory in which to install tqdm man pages.\n* comppath  : str, optional  \n    Directory in which to place tqdm completion.\n* log  : str, optional  \n    CRITICAL|FATAL|ERROR|WARN(ING)|[default: 'INFO']|DEBUG|NOTSET.\n\nReturns\n~~~~~~~\n\n* out  : decorated iterator.  \n\n.. code:: python\n\n    class tqdm():\n      def update(self, n=1):\n          \"\"\"\n          Manually update the progress bar, useful for streams\n          such as reading files.\n          E.g.:\n          >>> t = tqdm(total=filesize) # Initialise\n          >>> for current_buffer in stream:\n          ...    ...\n          ...    t.update(len(current_buffer))\n          >>> t.close()\n          The last line is highly recommended, but possibly not necessary if\n          ``t.update()`` will be called in such a way that ``filesize`` will be\n          exactly reached and printed.\n\n          Parameters\n          ----------\n          n  : int or float, optional\n              Increment to add to the internal counter of iterations\n              [default: 1]. If using float, consider specifying ``{n:.3f}``\n              or similar in ``bar_format``, or specifying ``unit_scale``.\n\n          Returns\n          -------\n          out  : bool or None\n              True if a ``display()`` was triggered.\n          \"\"\"\n\n      def close(self):\n          \"\"\"Cleanup and (if leave=False) close the progressbar.\"\"\"\n\n      def clear(self, nomove=False):\n          \"\"\"Clear current bar display.\"\"\"\n\n      def refresh(self):\n          \"\"\"\n          Force refresh the display of this bar.\n\n          Parameters\n          ----------\n          nolock  : bool, optional\n              If ``True``, does not lock.\n              If [default: ``False``]: calls ``acquire()`` on internal lock.\n          lock_args  : tuple, optional\n              Passed to internal lock's ``acquire()``.\n              If specified, will only ``display()`` if ``acquire()`` returns ``True``.\n          \"\"\"\n\n      def unpause(self):\n          \"\"\"Restart tqdm timer from last print time.\"\"\"\n\n      def reset(self, total=None):\n          \"\"\"\n          Resets to 0 iterations for repeated use.\n\n          Consider combining with ``leave=True``.\n\n          Parameters\n          ----------\n          total  : int or float, optional. Total to use for the new bar.\n          \"\"\"\n\n      def set_description(self, desc=None, refresh=True):\n          \"\"\"\n          Set/modify description of the progress bar.\n\n          Parameters\n          ----------\n          desc  : str, optional\n          refresh  : bool, optional\n              Forces refresh [default: True].\n          \"\"\"\n\n      def set_postfix(self, ordered_dict=None, refresh=True, **tqdm_kwargs):\n          \"\"\"\n          Set/modify postfix (additional stats)\n          with automatic formatting based on datatype.\n\n          Parameters\n          ----------\n          ordered_dict  : dict or OrderedDict, optional\n          refresh  : bool, optional\n              Forces refresh [default: True].\n          kwargs  : dict, optional\n          \"\"\"\n\n      @classmethod\n      def write(cls, s, file=sys.stdout, end=\"\\n\"):\n          \"\"\"Print a message via tqdm (without overlap with bars).\"\"\"\n\n      @property\n      def format_dict(self):\n          \"\"\"Public API for read-only member access.\"\"\"\n\n      def display(self, msg=None, pos=None):\n          \"\"\"\n          Use ``self.sp`` to display ``msg`` in the specified ``pos``.\n\n          Consider overloading this function when inheriting to use e.g.:\n          ``self.some_frontend(**self.format_dict)`` instead of ``self.sp``.\n\n          Parameters\n          ----------\n          msg  : str, optional. What to display (default: ``repr(self)``).\n          pos  : int, optional. Position to ``moveto``\n            (default: ``abs(self.pos)``).\n          \"\"\"\n\n      @classmethod\n      @contextmanager\n      def wrapattr(cls, stream, method, total=None, bytes=True, **tqdm_kwargs):\n          \"\"\"\n          stream  : file-like object.\n          method  : str, \"read\" or \"write\". The result of ``read()`` and\n              the first argument of ``write()`` should have a ``len()``.\n\n          >>> with tqdm.wrapattr(file_obj, \"read\", total=file_obj.size) as fobj:\n          ...     while True:\n          ...         chunk = fobj.read(chunk_size)\n          ...         if not chunk:\n          ...             break\n          \"\"\"\n\n      @classmethod\n      def pandas(cls, *targs, **tqdm_kwargs):\n          \"\"\"Registers the current `tqdm` class with `pandas`.\"\"\"\n\n    def trange(*args, **tqdm_kwargs):\n        \"\"\"Shortcut for `tqdm(range(*args), **tqdm_kwargs)`.\"\"\"\n\nConvenience Functions\n~~~~~~~~~~~~~~~~~~~~~\n\n.. code:: python\n\n    def tqdm.contrib.tenumerate(iterable, start=0, total=None,\n                                tqdm_class=tqdm.auto.tqdm, **tqdm_kwargs):\n        \"\"\"Equivalent of `numpy.ndenumerate` or builtin `enumerate`.\"\"\"\n\n    def tqdm.contrib.tzip(iter1, *iter2plus, **tqdm_kwargs):\n        \"\"\"Equivalent of builtin `zip`.\"\"\"\n\n    def tqdm.contrib.tmap(function, *sequences, **tqdm_kwargs):\n        \"\"\"Equivalent of builtin `map`.\"\"\"\n\nSubmodules\n~~~~~~~~~~\n\n.. code:: python\n\n    class tqdm.notebook.tqdm(tqdm.tqdm):\n        \"\"\"IPython/Jupyter Notebook widget.\"\"\"\n\n    class tqdm.auto.tqdm(tqdm.tqdm):\n        \"\"\"Automatically chooses beween `tqdm.notebook` and `tqdm.tqdm`.\"\"\"\n\n    class tqdm.asyncio.tqdm(tqdm.tqdm):\n      \"\"\"Asynchronous version.\"\"\"\n      @classmethod\n      def as_completed(cls, fs, *, loop=None, timeout=None, total=None,\n                       **tqdm_kwargs):\n          \"\"\"Wrapper for `asyncio.as_completed`.\"\"\"\n\n    class tqdm.gui.tqdm(tqdm.tqdm):\n        \"\"\"Matplotlib GUI version.\"\"\"\n\n    class tqdm.tk.tqdm(tqdm.tqdm):\n        \"\"\"Tkinter GUI version.\"\"\"\n\n    class tqdm.rich.tqdm(tqdm.tqdm):\n        \"\"\"`rich.progress` version.\"\"\"\n\n    class tqdm.keras.TqdmCallback(keras.callbacks.Callback):\n        \"\"\"Keras callback for epoch and batch progress.\"\"\"\n\n    class tqdm.dask.TqdmCallback(dask.callbacks.Callback):\n        \"\"\"Dask callback for task progress.\"\"\"\n\n\n``contrib``\n+++++++++++\n\nThe ``tqdm.contrib`` package also contains experimental modules:\n\n- ``tqdm.contrib.itertools``: Thin wrappers around ``itertools``\n- ``tqdm.contrib.concurrent``: Thin wrappers around ``concurrent.futures``\n- ``tqdm.contrib.slack``: Posts to `Slack <https://slack.com>`__ bots\n- ``tqdm.contrib.discord``: Posts to `Discord <https://discord.com>`__ bots\n- ``tqdm.contrib.telegram``: Posts to `Telegram <https://telegram.org>`__ bots\n- ``tqdm.contrib.bells``: Automagically enables all optional features\n\n  * ``auto``, ``pandas``, ``slack``, ``discord``, ``telegram``\n\nExamples and Advanced Usage\n---------------------------\n\n- See the `examples <https://github.com/tqdm/tqdm/tree/master/examples>`__\n  folder;\n- import the module and run ``help()``;\n- consult the `wiki <https://github.com/tqdm/tqdm/wiki>`__;\n\n  * this has an\n    `excellent article <https://github.com/tqdm/tqdm/wiki/How-to-make-a-great-Progress-Bar>`__\n    on how to make a **great** progressbar;\n\n- check out the `slides from PyData London <https://tqdm.github.io/PyData2019/slides.html>`__, or\n- run the |binder-demo|.\n\nDescription and additional stats\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nCustom information can be displayed and updated dynamically on ``tqdm`` bars\nwith the ``desc`` and ``postfix`` arguments:\n\n.. code:: python\n\n    from tqdm import tqdm, trange\n    from random import random, randint\n    from time import sleep\n\n    with trange(10) as t:\n        for i in t:\n            # Description will be displayed on the left\n            t.set_description('GEN %i' % i)\n            # Postfix will be displayed on the right,\n            # formatted automatically based on argument's datatype\n            t.set_postfix(loss=random(), gen=randint(1,999), str='h',\n                          lst=[1, 2])\n            sleep(0.1)\n\n    with tqdm(total=10, bar_format=\"{postfix[0]} {postfix[1][value]:>8.2g}\",\n              postfix=[\"Batch\", {\"value\": 0}]) as t:\n        for i in range(10):\n            sleep(0.1)\n            t.postfix[1][\"value\"] = i / 2\n            t.update()\n\nPoints to remember when using ``{postfix[...]}`` in the ``bar_format`` string:\n\n- ``postfix`` also needs to be passed as an initial argument in a compatible\n  format, and\n- ``postfix`` will be auto-converted to a string if it is a ``dict``-like\n  object. To prevent this behaviour, insert an extra item into the dictionary\n  where the key is not a string.\n\nAdditional ``bar_format`` parameters may also be defined by overriding\n``format_dict``, and the bar itself may be modified using ``ascii``:\n\n.. code:: python\n\n    from tqdm import tqdm\n    class TqdmExtraFormat(tqdm):\n        \"\"\"Provides a `total_time` format parameter\"\"\"\n        @property\n        def format_dict(self):\n            d = super().format_dict\n            total_time = d[\"elapsed\"] * (d[\"total\"] or 0) / max(d[\"n\"], 1)\n            d.update(total_time=self.format_interval(total_time) + \" in total\")\n            return d\n\n    for i in TqdmExtraFormat(\n          range(9), ascii=\" .oO0\",\n          bar_format=\"{total_time}: {percentage:.0f}%|{bar}{r_bar}\"):\n        if i == 4:\n            break\n\n.. code::\n\n    00:00 in total: 44%|0000.     | 4/9 [00:00<00:00, 962.93it/s]\n\nNote that ``{bar}`` also supports a format specifier ``[width][type]``.\n\n- ``width``\n\n  * unspecified (default): automatic to fill ``ncols``\n  * ``int >= 0``: fixed width overriding ``ncols`` logic\n  * ``int < 0``: subtract from the automatic default\n\n- ``type``\n\n  * ``a``: ascii (``ascii=True`` override)\n  * ``u``: unicode (``ascii=False`` override)\n  * ``b``: blank (``ascii=\"  \"`` override)\n\nThis means a fixed bar with right-justified text may be created by using:\n``bar_format=\"{l_bar}{bar:10}|{bar:-10b}right-justified\"``\n\nNested progress bars\n~~~~~~~~~~~~~~~~~~~~\n\n``tqdm`` supports nested progress bars. Here's an example:\n\n.. code:: python\n\n    from tqdm.auto import trange\n    from time import sleep\n\n    for i in trange(4, desc='1st loop'):\n        for j in trange(5, desc='2nd loop'):\n            for k in trange(50, desc='3rd loop', leave=False):\n                sleep(0.01)\n\nFor manual control over positioning (e.g. for multi-processing use),\nyou may specify ``position=n`` where ``n=0`` for the outermost bar,\n``n=1`` for the next, and so on.\nHowever, it's best to check if ``tqdm`` can work without manual ``position``\nfirst.\n\n.. code:: python\n\n    from time import sleep\n    from tqdm import trange, tqdm\n    from multiprocessing import Pool, RLock, freeze_support\n\n    L = list(range(9))\n\n    def progresser(n):\n        interval = 0.001 / (n + 2)\n        total = 5000\n        text = f\"#{n}, est. {interval * total:<04.2}s\"\n        for _ in trange(total, desc=text, position=n):\n            sleep(interval)\n\n    if __name__ == '__main__':\n        freeze_support()  # for Windows support\n        tqdm.set_lock(RLock())  # for managing output contention\n        p = Pool(initializer=tqdm.set_lock, initargs=(tqdm.get_lock(),))\n        p.map(progresser, L)\n\nNote that in Python 3, ``tqdm.write`` is thread-safe:\n\n.. code:: python\n\n    from time import sleep\n    from tqdm import tqdm, trange\n    from concurrent.futures import ThreadPoolExecutor\n\n    L = list(range(9))\n\n    def progresser(n):\n        interval = 0.001 / (n + 2)\n        total = 5000\n        text = f\"#{n}, est. {interval * total:<04.2}s\"\n        for _ in trange(total, desc=text):\n            sleep(interval)\n        if n == 6:\n            tqdm.write(\"n == 6 completed.\")\n            tqdm.write(\"`tqdm.write()` is thread-safe in py3!\")\n\n    if __name__ == '__main__':\n        with ThreadPoolExecutor() as p:\n            p.map(progresser, L)\n\nHooks and callbacks\n~~~~~~~~~~~~~~~~~~~\n\n``tqdm`` can easily support callbacks/hooks and manual updates.\nHere's an example with ``urllib``:\n\n**``urllib.urlretrieve`` documentation**\n\n    | [...]\n    | If present, the hook function will be called once\n    | on establishment of the network connection and once after each block read\n    | thereafter. The hook will be passed three arguments; a count of blocks\n    | transferred so far, a block size in bytes, and the total size of the file.\n    | [...]\n\n.. code:: python\n\n    import urllib, os\n    from tqdm import tqdm\n    urllib = getattr(urllib, 'request', urllib)\n\n    class TqdmUpTo(tqdm):\n        \"\"\"Provides `update_to(n)` which uses `tqdm.update(delta_n)`.\"\"\"\n        def update_to(self, b=1, bsize=1, tsize=None):\n            \"\"\"\n            b  : int, optional\n                Number of blocks transferred so far [default: 1].\n            bsize  : int, optional\n                Size of each block (in tqdm units) [default: 1].\n            tsize  : int, optional\n                Total size (in tqdm units). If [default: None] remains unchanged.\n            \"\"\"\n            if tsize is not None:\n                self.total = tsize\n            return self.update(b * bsize - self.n)  # also sets self.n = b * bsize\n\n    eg_link = \"https://caspersci.uk.to/matryoshka.zip\"\n    with TqdmUpTo(unit='B', unit_scale=True, unit_divisor=1024, miniters=1,\n                  desc=eg_link.split('/')[-1]) as t:  # all optional kwargs\n        urllib.urlretrieve(eg_link, filename=os.devnull,\n                           reporthook=t.update_to, data=None)\n        t.total = t.n\n\nInspired by `twine#242 <https://github.com/pypa/twine/pull/242>`__.\nFunctional alternative in\n`examples/tqdm_wget.py <https://github.com/tqdm/tqdm/blob/master/examples/tqdm_wget.py>`__.\n\nIt is recommend to use ``miniters=1`` whenever there is potentially\nlarge differences in iteration speed (e.g. downloading a file over\na patchy connection).\n\n**Wrapping read/write methods**\n\nTo measure throughput through a file-like object's ``read`` or ``write``\nmethods, use ``CallbackIOWrapper``:\n\n.. code:: python\n\n    from tqdm.auto import tqdm\n    from tqdm.utils import CallbackIOWrapper\n\n    with tqdm(total=file_obj.size,\n              unit='B', unit_scale=True, unit_divisor=1024) as t:\n        fobj = CallbackIOWrapper(t.update, file_obj, \"read\")\n        while True:\n            chunk = fobj.read(chunk_size)\n            if not chunk:\n                break\n        t.reset()\n        # ... continue to use `t` for something else\n\nAlternatively, use the even simpler ``wrapattr`` convenience function,\nwhich would condense both the ``urllib`` and ``CallbackIOWrapper`` examples\ndown to:\n\n.. code:: python\n\n    import urllib, os\n    from tqdm import tqdm\n\n    eg_link = \"https://caspersci.uk.to/matryoshka.zip\"\n    response = getattr(urllib, 'request', urllib).urlopen(eg_link)\n    with tqdm.wrapattr(open(os.devnull, \"wb\"), \"write\",\n                       miniters=1, desc=eg_link.split('/')[-1],\n                       total=getattr(response, 'length', None)) as fout:\n        for chunk in response:\n            fout.write(chunk)\n\nThe ``requests`` equivalent is nearly identical:\n\n.. code:: python\n\n    import requests, os\n    from tqdm import tqdm\n\n    eg_link = \"https://caspersci.uk.to/matryoshka.zip\"\n    response = requests.get(eg_link, stream=True)\n    with tqdm.wrapattr(open(os.devnull, \"wb\"), \"write\",\n                       miniters=1, desc=eg_link.split('/')[-1],\n                       total=int(response.headers.get('content-length', 0))) as fout:\n        for chunk in response.iter_content(chunk_size=4096):\n            fout.write(chunk)\n\n**Custom callback**\n\n``tqdm`` is known for intelligently skipping unnecessary displays. To make a\ncustom callback take advantage of this, simply use the return value of\n``update()``. This is set to ``True`` if a ``display()`` was triggered.\n\n.. code:: python\n\n    from tqdm.auto import tqdm as std_tqdm\n\n    def external_callback(*args, **kwargs):\n        ...\n\n    class TqdmExt(std_tqdm):\n        def update(self, n=1):\n            displayed = super().update(n)\n            if displayed:\n                external_callback(**self.format_dict)\n            return displayed\n\n``asyncio``\n~~~~~~~~~~~\n\nNote that ``break`` isn't currently caught by asynchronous iterators.\nThis means that ``tqdm`` cannot clean up after itself in this case:\n\n.. code:: python\n\n    from tqdm.asyncio import tqdm\n\n    async for i in tqdm(range(9)):\n        if i == 2:\n            break\n\nInstead, either call ``pbar.close()`` manually or use the context manager syntax:\n\n.. code:: python\n\n    from tqdm.asyncio import tqdm\n\n    with tqdm(range(9)) as pbar:\n        async for i in pbar:\n            if i == 2:\n                break\n\nPandas Integration\n~~~~~~~~~~~~~~~~~~\n\nDue to popular demand we've added support for ``pandas`` -- here's an example\nfor ``DataFrame.progress_apply`` and ``DataFrameGroupBy.progress_apply``:\n\n.. code:: python\n\n    import pandas as pd\n    import numpy as np\n    from tqdm import tqdm\n\n    df = pd.DataFrame(np.random.randint(0, 100, (100000, 6)))\n\n    # Register `pandas.progress_apply` and `pandas.Series.map_apply` with `tqdm`\n    # (can use `tqdm.gui.tqdm`, `tqdm.notebook.tqdm`, optional kwargs, etc.)\n    tqdm.pandas(desc=\"my bar!\")\n\n    # Now you can use `progress_apply` instead of `apply`\n    # and `progress_map` instead of `map`\n    df.progress_apply(lambda x: x**2)\n    # can also groupby:\n    # df.groupby(0).progress_apply(lambda x: x**2)\n\nIn case you're interested in how this works (and how to modify it for your\nown callbacks), see the\n`examples <https://github.com/tqdm/tqdm/tree/master/examples>`__\nfolder or import the module and run ``help()``.\n\nKeras Integration\n~~~~~~~~~~~~~~~~~\n\nA ``keras`` callback is also available:\n\n.. code:: python\n\n    from tqdm.keras import TqdmCallback\n\n    ...\n\n    model.fit(..., verbose=0, callbacks=[TqdmCallback()])\n\nDask Integration\n~~~~~~~~~~~~~~~~\n\nA ``dask`` callback is also available:\n\n.. code:: python\n\n    from tqdm.dask import TqdmCallback\n\n    with TqdmCallback(desc=\"compute\"):\n        ...\n        arr.compute()\n\n    # or use callback globally\n    cb = TqdmCallback(desc=\"global\")\n    cb.register()\n    arr.compute()\n\nIPython/Jupyter Integration\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nIPython/Jupyter is supported via the ``tqdm.notebook`` submodule:\n\n.. code:: python\n\n    from tqdm.notebook import trange, tqdm\n    from time import sleep\n\n    for i in trange(3, desc='1st loop'):\n        for j in tqdm(range(100), desc='2nd loop'):\n            sleep(0.01)\n\nIn addition to ``tqdm`` features, the submodule provides a native Jupyter\nwidget (compatible with IPython v1-v4 and Jupyter), fully working nested bars\nand colour hints (blue: normal, green: completed, red: error/interrupt,\nlight blue: no ETA); as demonstrated below.\n\n|Screenshot-Jupyter1|\n|Screenshot-Jupyter2|\n|Screenshot-Jupyter3|\n\nThe ``notebook`` version supports percentage or pixels for overall width\n(e.g.: ``ncols='100%'`` or ``ncols='480px'``).\n\nIt is also possible to let ``tqdm`` automatically choose between\nconsole or notebook versions by using the ``autonotebook`` submodule:\n\n.. code:: python\n\n    from tqdm.autonotebook import tqdm\n    tqdm.pandas()\n\nNote that this will issue a ``TqdmExperimentalWarning`` if run in a notebook\nsince it is not meant to be possible to distinguish between ``jupyter notebook``\nand ``jupyter console``. Use ``auto`` instead of ``autonotebook`` to suppress\nthis warning.\n\nNote that notebooks will display the bar in the cell where it was created.\nThis may be a different cell from the one where it is used.\nIf this is not desired, either\n\n- delay the creation of the bar to the cell where it must be displayed, or\n- create the bar with ``display=False``, and in a later cell call\n  ``display(bar.container)``:\n\n.. code:: python\n\n    from tqdm.notebook import tqdm\n    pbar = tqdm(..., display=False)\n\n.. code:: python\n\n    # different cell\n    display(pbar.container)\n\nThe ``keras`` callback has a ``display()`` method which can be used likewise:\n\n.. code:: python\n\n    from tqdm.keras import TqdmCallback\n    cbk = TqdmCallback(display=False)\n\n.. code:: python\n\n    # different cell\n    cbk.display()\n    model.fit(..., verbose=0, callbacks=[cbk])\n\nAnother possibility is to have a single bar (near the top of the notebook)\nwhich is constantly re-used (using ``reset()`` rather than ``close()``).\nFor this reason, the notebook version (unlike the CLI version) does not\nautomatically call ``close()`` upon ``Exception``.\n\n.. code:: python\n\n    from tqdm.notebook import tqdm\n    pbar = tqdm()\n\n.. code:: python\n\n    # different cell\n    iterable = range(100)\n    pbar.reset(total=len(iterable))  # initialise with new `total`\n    for i in iterable:\n        pbar.update()\n    pbar.refresh()  # force print final status but don't `close()`\n\nCustom Integration\n~~~~~~~~~~~~~~~~~~\n\nTo change the default arguments (such as making ``dynamic_ncols=True``),\nsimply use built-in Python magic:\n\n.. code:: python\n\n    from functools import partial\n    from tqdm import tqdm as std_tqdm\n    tqdm = partial(std_tqdm, dynamic_ncols=True)\n\nFor further customisation,\n``tqdm`` may be inherited from to create custom callbacks (as with the\n``TqdmUpTo`` example `above <#hooks-and-callbacks>`__) or for custom frontends\n(e.g. GUIs such as notebook or plotting packages). In the latter case:\n\n1. ``def __init__()`` to call ``super().__init__(..., gui=True)`` to disable\n   terminal ``status_printer`` creation.\n2. Redefine: ``close()``, ``clear()``, ``display()``.\n\nConsider overloading ``display()`` to use e.g.\n``self.frontend(**self.format_dict)`` instead of ``self.sp(repr(self))``.\n\nSome submodule examples of inheritance:\n\n- `tqdm/notebook.py <https://github.com/tqdm/tqdm/blob/master/tqdm/notebook.py>`__\n- `tqdm/gui.py <https://github.com/tqdm/tqdm/blob/master/tqdm/gui.py>`__\n- `tqdm/tk.py <https://github.com/tqdm/tqdm/blob/master/tqdm/tk.py>`__\n- `tqdm/contrib/slack.py <https://github.com/tqdm/tqdm/blob/master/tqdm/contrib/slack.py>`__\n- `tqdm/contrib/discord.py <https://github.com/tqdm/tqdm/blob/master/tqdm/contrib/discord.py>`__\n- `tqdm/contrib/telegram.py <https://github.com/tqdm/tqdm/blob/master/tqdm/contrib/telegram.py>`__\n\nDynamic Monitor/Meter\n~~~~~~~~~~~~~~~~~~~~~\n\nYou can use a ``tqdm`` as a meter which is not monotonically increasing.\nThis could be because ``n`` decreases (e.g. a CPU usage monitor) or ``total``\nchanges.\n\nOne example would be recursively searching for files. The ``total`` is the\nnumber of objects found so far, while ``n`` is the number of those objects which\nare files (rather than folders):\n\n.. code:: python\n\n    from tqdm import tqdm\n    import os.path\n\n    def find_files_recursively(path, show_progress=True):\n        files = []\n        # total=1 assumes `path` is a file\n        t = tqdm(total=1, unit=\"file\", disable=not show_progress)\n        if not os.path.exists(path):\n            raise IOError(\"Cannot find:\" + path)\n\n        def append_found_file(f):\n            files.append(f)\n            t.update()\n\n        def list_found_dir(path):\n            \"\"\"returns os.listdir(path) assuming os.path.isdir(path)\"\"\"\n            listing = os.listdir(path)\n            # subtract 1 since a \"file\" we found was actually this directory\n            t.total += len(listing) - 1\n            # fancy way to give info without forcing a refresh\n            t.set_postfix(dir=path[-10:], refresh=False)\n            t.update(0)  # may trigger a refresh\n            return listing\n\n        def recursively_search(path):\n            if os.path.isdir(path):\n                for f in list_found_dir(path):\n                    recursively_search(os.path.join(path, f))\n            else:\n                append_found_file(path)\n\n        recursively_search(path)\n        t.set_postfix(dir=path)\n        t.close()\n        return files\n\nUsing ``update(0)`` is a handy way to let ``tqdm`` decide when to trigger a\ndisplay refresh to avoid console spamming.\n\nWriting messages\n~~~~~~~~~~~~~~~~\n\nThis is a work in progress (see\n`#737 <https://github.com/tqdm/tqdm/issues/737>`__).\n\nSince ``tqdm`` uses a simple printing mechanism to display progress bars,\nyou should not write any message in the terminal using ``print()`` while\na progressbar is open.\n\nTo write messages in the terminal without any collision with ``tqdm`` bar\ndisplay, a ``.write()`` method is provided:\n\n.. code:: python\n\n    from tqdm.auto import tqdm, trange\n    from time import sleep\n\n    bar = trange(10)\n    for i in bar:\n        # Print using tqdm class method .write()\n        sleep(0.1)\n        if not (i % 3):\n            tqdm.write(\"Done task %i\" % i)\n        # Can also use bar.write()\n\nBy default, this will print to standard output ``sys.stdout``. but you can\nspecify any file-like object using the ``file`` argument. For example, this\ncan be used to redirect the messages writing to a log file or class.\n\nRedirecting writing\n~~~~~~~~~~~~~~~~~~~\n\nIf using a library that can print messages to the console, editing the library\nby  replacing ``print()`` with ``tqdm.write()`` may not be desirable.\nIn that case, redirecting ``sys.stdout`` to ``tqdm.write()`` is an option.\n\nTo redirect ``sys.stdout``, create a file-like class that will write\nany input string to ``tqdm.write()``, and supply the arguments\n``file=sys.stdout, dynamic_ncols=True``.\n\nA reusable canonical example is given below:\n\n.. code:: python\n\n    from time import sleep\n    import contextlib\n    import sys\n    from tqdm import tqdm\n    from tqdm.contrib import DummyTqdmFile\n\n\n    @contextlib.contextmanager\n    def std_out_err_redirect_tqdm():\n        orig_out_err = sys.stdout, sys.stderr\n        try:\n            sys.stdout, sys.stderr = map(DummyTqdmFile, orig_out_err)\n            yield orig_out_err[0]\n        # Relay exceptions\n        except Exception as exc:\n            raise exc\n        # Always restore sys.stdout/err if necessary\n        finally:\n            sys.stdout, sys.stderr = orig_out_err\n\n    def some_fun(i):\n        print(\"Fee, fi, fo,\".split()[i])\n\n    # Redirect stdout to tqdm.write() (don't forget the `as save_stdout`)\n    with std_out_err_redirect_tqdm() as orig_stdout:\n        # tqdm needs the original stdout\n        # and dynamic_ncols=True to autodetect console width\n        for i in tqdm(range(3), file=orig_stdout, dynamic_ncols=True):\n            sleep(.5)\n            some_fun(i)\n\n    # After the `with`, printing is restored\n    print(\"Done!\")\n\nRedirecting ``logging``\n~~~~~~~~~~~~~~~~~~~~~~~\n\nSimilar to ``sys.stdout``/``sys.stderr`` as detailed above, console ``logging``\nmay also be redirected to ``tqdm.write()``.\n\nWarning: if also redirecting ``sys.stdout``/``sys.stderr``, make sure to\nredirect ``logging`` first if needed.\n\nHelper methods are available in ``tqdm.contrib.logging``. For example:\n\n.. code:: python\n\n    import logging\n    from tqdm import trange\n    from tqdm.contrib.logging import logging_redirect_tqdm\n\n    LOG = logging.getLogger(__name__)\n\n    if __name__ == '__main__':\n        logging.basicConfig(level=logging.INFO)\n        with logging_redirect_tqdm():\n            for i in trange(9):\n                if i == 4:\n                    LOG.info(\"console logging redirected to `tqdm.write()`\")\n        # logging restored\n\nMonitoring thread, intervals and miniters\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n``tqdm`` implements a few tricks to increase efficiency and reduce overhead.\n\n- Avoid unnecessary frequent bar refreshing: ``mininterval`` defines how long\n  to wait between each refresh. ``tqdm`` always gets updated in the background,\n  but it will display only every ``mininterval``.\n- Reduce number of calls to check system clock/time.\n- ``mininterval`` is more intuitive to configure than ``miniters``.\n  A clever adjustment system ``dynamic_miniters`` will automatically adjust\n  ``miniters`` to the amount of iterations that fit into time ``mininterval``.\n  Essentially, ``tqdm`` will check if it's time to print without actually\n  checking time. This behaviour can be still be bypassed by manually setting\n  ``miniters``.\n\nHowever, consider a case with a combination of fast and slow iterations.\nAfter a few fast iterations, ``dynamic_miniters`` will set ``miniters`` to a\nlarge number. When iteration rate subsequently slows, ``miniters`` will\nremain large and thus reduce display update frequency. To address this:\n\n- ``maxinterval`` defines the maximum time between display refreshes.\n  A concurrent monitoring thread checks for overdue updates and forces one\n  where necessary.\n\nThe monitoring thread should not have a noticeable overhead, and guarantees\nupdates at least every 10 seconds by default.\nThis value can be directly changed by setting the ``monitor_interval`` of\nany ``tqdm`` instance (i.e. ``t = tqdm.tqdm(...); t.monitor_interval = 2``).\nThe monitor thread may be disabled application-wide by setting\n``tqdm.tqdm.monitor_interval = 0`` before instantiation of any ``tqdm`` bar.\n\n\nMerch\n-----\n\nYou can buy `tqdm branded merch <https://tqdm.github.io/merch>`__ now!\n\nContributions\n-------------\n\n|GitHub-Commits| |GitHub-Issues| |GitHub-PRs| |OpenHub-Status| |GitHub-Contributions| |CII Best Practices|\n\nAll source code is hosted on `GitHub <https://github.com/tqdm/tqdm>`__.\nContributions are welcome.\n\nSee the\n`CONTRIBUTING <https://github.com/tqdm/tqdm/blob/master/CONTRIBUTING.md>`__\nfile for more information.\n\nDevelopers who have made significant contributions, ranked by *SLoC*\n(surviving lines of code,\n`git fame <https://github.com/casperdcl/git-fame>`__ ``-wMC --excl '\\.(png|gif|jpg)$'``),\nare:\n\n==================== ======================================================== ==== ================================\nName                 ID                                                       SLoC Notes\n==================== ======================================================== ==== ================================\nCasper da Costa-Luis `casperdcl <https://github.com/casperdcl>`__             ~80% primary maintainer |Gift-Casper|\nStephen Larroque     `lrq3000 <https://github.com/lrq3000>`__                 ~9%  team member\nMartin Zugnoni       `martinzugnoni <https://github.com/martinzugnoni>`__     ~3%\nDaniel Ecer          `de-code <https://github.com/de-code>`__                 ~2%\nRichard Sheridan     `richardsheridan <https://github.com/richardsheridan>`__ ~1%\nGuangshuo Chen       `chengs <https://github.com/chengs>`__                   ~1%\nHelio Machado        `0x2b3bfa0 <https://github.com/0x2b3bfa0>`__             ~1%\nKyle Altendorf       `altendky <https://github.com/altendky>`__               <1%\nNoam Yorav-Raphael   `noamraph <https://github.com/noamraph>`__               <1%  original author\nMatthew Stevens      `mjstevens777 <https://github.com/mjstevens777>`__       <1%\nHadrien Mary         `hadim <https://github.com/hadim>`__                     <1%  team member\nMikhail Korobov      `kmike <https://github.com/kmike>`__                     <1%  team member\n==================== ======================================================== ==== ================================\n\nPorts to Other Languages\n~~~~~~~~~~~~~~~~~~~~~~~~\n\nA list is available on\n`this wiki page <https://github.com/tqdm/tqdm/wiki/tqdm-ports>`__.\n\n\nLICENCE\n-------\n\nOpen Source (OSI approved): |LICENCE|\n\nCitation information: |DOI|\n\n|README-Hits| (Since 19 May 2016)\n\n.. |Logo| image:: https://tqdm.github.io/img/logo.gif\n.. |Screenshot| image:: https://tqdm.github.io/img/tqdm.gif\n.. |Video| image:: https://tqdm.github.io/img/video.jpg\n   :target: https://tqdm.github.io/video\n.. |Slides| image:: https://tqdm.github.io/img/slides.jpg\n   :target: https://tqdm.github.io/PyData2019/slides.html\n.. |Merch| image:: https://tqdm.github.io/img/merch.jpg\n   :target: https://tqdm.github.io/merch\n.. |Build-Status| image:: https://img.shields.io/github/actions/workflow/status/tqdm/tqdm/test.yml?branch=master&label=tqdm&logo=GitHub\n   :target: https://github.com/tqdm/tqdm/actions/workflows/test.yml\n.. |Coverage-Status| image:: https://img.shields.io/coveralls/github/tqdm/tqdm/master?logo=coveralls\n   :target: https://coveralls.io/github/tqdm/tqdm\n.. |Branch-Coverage-Status| image:: https://codecov.io/gh/tqdm/tqdm/branch/master/graph/badge.svg\n   :target: https://codecov.io/gh/tqdm/tqdm\n.. |Codacy-Grade| image:: https://app.codacy.com/project/badge/Grade/3f965571598f44549c7818f29cdcf177\n   :target: https://www.codacy.com/gh/tqdm/tqdm/dashboard\n.. |CII Best Practices| image:: https://bestpractices.coreinfrastructure.org/projects/3264/badge\n   :target: https://bestpractices.coreinfrastructure.org/projects/3264\n.. |GitHub-Status| image:: https://img.shields.io/github/tag/tqdm/tqdm.svg?maxAge=86400&logo=github&logoColor=white\n   :target: https://github.com/tqdm/tqdm/releases\n.. |GitHub-Forks| image:: https://img.shields.io/github/forks/tqdm/tqdm.svg?logo=github&logoColor=white\n   :target: https://github.com/tqdm/tqdm/network\n.. |GitHub-Stars| image:: https://img.shields.io/github/stars/tqdm/tqdm.svg?logo=github&logoColor=white\n   :target: https://github.com/tqdm/tqdm/stargazers\n.. |GitHub-Commits| image:: https://img.shields.io/github/commit-activity/y/tqdm/tqdm.svg?logo=git&logoColor=white\n   :target: https://github.com/tqdm/tqdm/graphs/commit-activity\n.. |GitHub-Issues| image:: https://img.shields.io/github/issues-closed/tqdm/tqdm.svg?logo=github&logoColor=white\n   :target: https://github.com/tqdm/tqdm/issues?q=\n.. |GitHub-PRs| image:: https://img.shields.io/github/issues-pr-closed/tqdm/tqdm.svg?logo=github&logoColor=white\n   :target: https://github.com/tqdm/tqdm/pulls\n.. |GitHub-Contributions| image:: https://img.shields.io/github/contributors/tqdm/tqdm.svg?logo=github&logoColor=white\n   :target: https://github.com/tqdm/tqdm/graphs/contributors\n.. |GitHub-Updated| image:: https://img.shields.io/github/last-commit/tqdm/tqdm/master.svg?logo=github&logoColor=white&label=pushed\n   :target: https://github.com/tqdm/tqdm/pulse\n.. |Gift-Casper| image:: https://img.shields.io/badge/dynamic/json.svg?color=ff69b4&label=gifts%20received&prefix=%C2%A3&query=%24..sum&url=https%3A%2F%2Fcaspersci.uk.to%2Fgifts.json\n   :target: https://cdcl.ml/sponsor\n.. |Versions| image:: https://img.shields.io/pypi/v/tqdm.svg\n   :target: https://tqdm.github.io/releases\n.. |PyPI-Downloads| image:: https://img.shields.io/pypi/dm/tqdm.svg?label=pypi%20downloads&logo=PyPI&logoColor=white\n   :target: https://pepy.tech/project/tqdm\n.. |Py-Versions| image:: https://img.shields.io/pypi/pyversions/tqdm.svg?logo=python&logoColor=white\n   :target: https://pypi.org/project/tqdm\n.. |Conda-Forge-Status| image:: https://img.shields.io/conda/v/conda-forge/tqdm.svg?label=conda-forge&logo=conda-forge\n   :target: https://anaconda.org/conda-forge/tqdm\n.. |Snapcraft| image:: https://img.shields.io/badge/snap-install-82BEA0.svg?logo=snapcraft\n   :target: https://snapcraft.io/tqdm\n.. |Docker| image:: https://img.shields.io/badge/docker-pull-blue.svg?logo=docker&logoColor=white\n   :target: https://hub.docker.com/r/tqdm/tqdm\n.. |Libraries-Rank| image:: https://img.shields.io/librariesio/sourcerank/pypi/tqdm.svg?logo=koding&logoColor=white\n   :target: https://libraries.io/pypi/tqdm\n.. |Libraries-Dependents| image:: https://img.shields.io/librariesio/dependent-repos/pypi/tqdm.svg?logo=koding&logoColor=white\n    :target: https://github.com/tqdm/tqdm/network/dependents\n.. |OpenHub-Status| image:: https://www.openhub.net/p/tqdm/widgets/project_thin_badge?format=gif\n   :target: https://www.openhub.net/p/tqdm?ref=Thin+badge\n.. |awesome-python| image:: https://awesome.re/mentioned-badge.svg\n   :target: https://github.com/vinta/awesome-python\n.. |LICENCE| image:: https://img.shields.io/pypi/l/tqdm.svg\n   :target: https://raw.githubusercontent.com/tqdm/tqdm/master/LICENCE\n.. |DOI| image:: https://img.shields.io/badge/DOI-10.5281/zenodo.595120-blue.svg\n   :target: https://doi.org/10.5281/zenodo.595120\n.. |binder-demo| image:: https://mybinder.org/badge_logo.svg\n   :target: https://mybinder.org/v2/gh/tqdm/tqdm/master?filepath=DEMO.ipynb\n.. |Screenshot-Jupyter1| image:: https://tqdm.github.io/img/jupyter-1.gif\n.. |Screenshot-Jupyter2| image:: https://tqdm.github.io/img/jupyter-2.gif\n.. |Screenshot-Jupyter3| image:: https://tqdm.github.io/img/jupyter-3.gif\n.. |README-Hits| image:: https://cgi.cdcl.ml/hits?q=tqdm&style=social&r=https://github.com/tqdm/tqdm&l=https://tqdm.github.io/img/favicon.png&f=https://tqdm.github.io/img/logo.gif\n   :target: https://cgi.cdcl.ml/hits?q=tqdm&a=plot&r=https://github.com/tqdm/tqdm&l=https://tqdm.github.io/img/favicon.png&f=https://tqdm.github.io/img/logo.gif&style=social\n"
      }
    },
    {
      "download_info": {
        "url": "https://pypi.tuna.tsinghua.edu.cn/packages/df/b5/f2cb1950dda46ac2284d6c950489fdacd0e743c2d79a347924d3cc44b86f/oss2-2.19.1.tar.gz",
        "archive_info": {
          "hash": "sha256=a8ab9ee7eb99e88a7e1382edc6ea641d219d585a7e074e3776e9dec9473e59c1",
          "hashes": {
            "sha256": "a8ab9ee7eb99e88a7e1382edc6ea641d219d585a7e074e3776e9dec9473e59c1"
          }
        }
      },
      "is_direct": false,
      "is_yanked": false,
      "requested": false,
      "metadata": {
        "metadata_version": "2.4",
        "name": "oss2",
        "version": "2.19.1",
        "dynamic": [
          "classifier",
          "description",
          "home-page",
          "license-file",
          "requires-dist",
          "summary"
        ],
        "summary": "Aliyun OSS (Object Storage Service) SDK",
        "home_page": "http://oss.aliyun.com",
        "license_file": [
          "LICENSE"
        ],
        "classifier": [
          "Development Status :: 5 - Production/Stable",
          "Intended Audience :: Developers",
          "License :: OSI Approved :: MIT License",
          "Operating System :: OS Independent",
          "Programming Language :: Python",
          "Programming Language :: Python :: 2",
          "Programming Language :: Python :: 2.6",
          "Programming Language :: Python :: 2.7",
          "Programming Language :: Python :: 3",
          "Programming Language :: Python :: 3.3",
          "Programming Language :: Python :: 3.4",
          "Programming Language :: Python :: 3.5",
          "Programming Language :: Python :: 3.6",
          "Programming Language :: Python :: 3.7",
          "Programming Language :: Python :: 3.8"
        ],
        "requires_dist": [
          "requests!=2.9.0",
          "crcmod>=1.7",
          "pycryptodome>=3.4.7",
          "aliyun-python-sdk-kms>=2.4.1",
          "aliyun-python-sdk-core>=2.13.12",
          "six"
        ],
        "description": "Alibaba Cloud OSS SDK for Python\r\n================================\r\n\r\n.. image:: https://badge.fury.io/py/oss2.svg\r\n    :target: https://badge.fury.io/py/oss2\r\n.. image:: https://travis-ci.org/aliyun/aliyun-oss-python-sdk.svg?branch=master\r\n    :target: https://travis-ci.org/aliyun/aliyun-oss-python-sdk\r\n.. image:: https://coveralls.io/repos/github/aliyun/aliyun-oss-python-sdk/badge.svg?branch=master\r\n    :target: https://coveralls.io/github/aliyun/aliyun-oss-python-sdk?branch=master\r\n\r\n`README of Chinese <https://github.com/aliyun/aliyun-oss-python-sdk/blob/master/README-CN.rst>`\r\n\r\nOverview\r\n--------\r\n\r\nAlibaba Cloud Object Storage Python SDK 2.x. This version is not compatible with the previous version (Version 0.x). The package name is `oss2` to avoid conflict with previous versions. \r\n\r\n\r\nThe SDK of this version is dependent on the third-party HTTP library `requests <https://github.com/kennethreitz/requests>`_ and `crcmod`. Install the SDK following the methods below. \r\n\r\nNote:\r\n\r\n    This version does not contain the `osscmd` command line tool. \r\n\r\nRunning environment\r\n-------------------\r\n\r\nPython 2.6(not recommended)Ôºå2.7Ôºå3.3(not recommended)Ôºå3.4Ôºå3.5Ôºå3.6\r\n\r\n\r\nNote:\r\n    Python 2.6 is not recommended because it is no longer supported by the Python core team. \r\n    Do not use Python 3.3.0 or 3.3.1. Refer to `Python Issue 16658 <https://bugs.python.org/issue16658>`_.\r\n\r\nInstalling\r\n----------\r\n\r\nInstall the official release version through PIP (taking Linux as an example): \r\n\r\n.. code-block:: bash\r\n\r\n    $ pip install oss2\r\n\r\nYou can also install the unzipped installer package directly: \r\n\r\n.. code-block:: bash\r\n\r\n    $ sudo python setup.py install\r\n\r\n\r\nGetting started\r\n---------------\r\n\r\n.. code-block:: python\r\n\r\n    # -*- coding: utf-8 -*-\r\n\r\n    import oss2\r\n\r\n    endpoint = 'http://oss-cn-hangzhou.aliyuncs.com' # Suppose that your bucket is in the Hangzhou region. \r\n\r\n    auth = oss2.Auth('<Your AccessKeyID>', '<Your AccessKeySecret>')\r\n    bucket = oss2.Bucket(auth, endpoint, '<your bucket name>')\r\n\r\n    # The object key in the bucket is story.txt\r\n    key = 'story.txt'\r\n\r\n    # Upload\r\n    bucket.put_object(key, 'Ali Baba is a happy youth.')\r\n\r\n    # Download\r\n    bucket.get_object(key).read()\r\n\r\n    # Delete\r\n    bucket.delete_object(key)\r\n\r\n    # Traverse all objects in the bucket\r\n    for object_info in oss2.ObjectIterator(bucket):\r\n        print(object_info.key)\r\n\r\nFor more examples, refer to the code under the \"examples\" directory. \r\n\r\nHandling errors\r\n---------------\r\n\r\nThe Python SDK interface will throw an exception in case of an error (see oss2.exceptions sub-module) unless otherwise specified. An example is provided below:\r\n\r\n.. code-block:: python\r\n\r\n    try:\r\n        result = bucket.get_object(key)\r\n        print(result.read())\r\n    except oss2.exceptions.NoSuchKey as e:\r\n        print('{0} not found: http_status={1}, request_id={2}'.format(key, e.status, e.request_id))\r\n\r\nSetup Logging\r\n---------------\r\n\r\nThe following code can set the logging level of 'oss2'.\r\n .. code-block:: python\r\n\r\n    import logging\r\n    logging.getLogger('oss2').setLevel(logging.WARNING)\r\n\r\nTesting\r\n-------\r\n\r\nFirst set the required AccessKeyId, AccessKeySecret, endpoint and bucket information for the test through environment variables (**Do not use the bucket for the production environment**). \r\nTake the Linux system for example: \r\n\r\n.. code-block:: bash\r\n\r\n    $ export OSS_TEST_ACCESS_KEY_ID=<AccessKeyId>\r\n    $ export OSS_TEST_ACCESS_KEY_SECRET=<AccessKeySecret>\r\n    $ export OSS_TEST_ENDPOINT=<endpoint>\r\n    $ export OSS_TEST_BUCKET=<bucket>\r\n\r\n    $ export OSS_TEST_STS_ID=<AccessKeyId for testing STS>\r\n    $ export OSS_TEST_STS_KEY=<AccessKeySecret for testing STS>\r\n    $ export OSS_TEST_STS_ARN=<Role ARN for testing STS>\r\n\r\n\r\nRun the test in the following method: \r\n\r\n.. code-block:: bash\r\n\r\n    $ nosetests                          # First install nose\r\n\r\n\r\nYou can set environment variable to test auth v2:\r\n\r\n.. code-block:: bash\r\n\r\n    $ export OSS_TEST_AUTH_VERSION=v2\r\n\r\nMore resources\r\n--------------\r\n- `More examples <https://github.com/aliyun/aliyun-oss-python-sdk/tree/master/examples>`_. \r\n- `Python SDK API documentation <http://aliyun-oss-python-sdk.readthedocs.org/en/latest>`_. \r\n- `Official Python SDK documentation <https://help.aliyun.com/document_detail/32026.html>`_.\r\n\r\nContacting us\r\n-------------\r\n- `Alibaba Cloud OSS official website <http://oss.aliyun.com>`_.\r\n- `Alibaba Cloud OSS official forum <http://bbs.aliyun.com>`_.\r\n- `Alibaba Cloud OSS official documentation center <https://help.aliyun.com/document_detail/32026.html>`_.\r\n- Alibaba Cloud official technical support: `Submit a ticket <https://workorder.console.aliyun.com/#/ticket/createIndex>`_.\r\n\r\nLicense\r\n-------\r\n- `MIT <https://github.com/aliyun/aliyun-oss-python-sdk/blob/master/LICENSE>`_.\r\n"
      }
    },
    {
      "download_info": {
        "url": "https://pypi.tuna.tsinghua.edu.cn/packages/3e/09/da9f58eb38b4fdb97ba6523274fbf445ef6a06be64b433693da8307b4bec/aliyun-python-sdk-core-2.16.0.tar.gz",
        "archive_info": {
          "hash": "sha256=651caad597eb39d4fad6cf85133dffe92837d53bdf62db9d8f37dab6508bb8f9",
          "hashes": {
            "sha256": "651caad597eb39d4fad6cf85133dffe92837d53bdf62db9d8f37dab6508bb8f9"
          }
        }
      },
      "is_direct": false,
      "is_yanked": false,
      "requested": false,
      "metadata": {
        "metadata_version": "2.4",
        "name": "aliyun-python-sdk-core",
        "version": "2.16.0",
        "dynamic": [
          "author",
          "author-email",
          "classifier",
          "description",
          "home-page",
          "keywords",
          "license",
          "license-file",
          "platform",
          "requires-dist",
          "requires-python",
          "summary"
        ],
        "platform": [
          "any"
        ],
        "summary": "The core module of Aliyun Python SDK.",
        "keywords": [
          "aliyun",
          "sdk",
          "core"
        ],
        "home_page": "https://github.com/aliyun/aliyun-openapi-python-sdk",
        "author": "Alibaba Cloud",
        "author_email": "alibaba-cloud-sdk-dev-team@list.alibaba-inc.com",
        "license": "Apache License 2.0",
        "license_file": [
          "LICENSE"
        ],
        "classifier": [
          "Development Status :: 5 - Production/Stable",
          "Intended Audience :: Developers",
          "License :: OSI Approved :: Apache Software License",
          "Programming Language :: Python",
          "Programming Language :: Python :: 3",
          "Programming Language :: Python :: 3.7",
          "Programming Language :: Python :: 3.8",
          "Programming Language :: Python :: 3.9",
          "Programming Language :: Python :: 3.10",
          "Programming Language :: Python :: 3.11",
          "Programming Language :: Python :: 3.12",
          "Topic :: Software Development"
        ],
        "requires_dist": [
          "jmespath<1.0.0,>=0.9.3",
          "cryptography>=3.0.0"
        ],
        "requires_python": ">=3.7",
        "description": "======================\naliyun-python-sdk-core\n======================\n\n\nThis is the core module of Aliyun Python SDK.\n\nAliyun Python SDK is the official software development kit. It makes things easy to integrate your Python application,\nlibrary, or script with Aliyun services.\n\nThis module works on Python versions:\n\n   * 2.7 and greater\n\n\nDocumentation:\n\nPlease visit http://develop.aliyun.com/sdk/python\n"
      }
    },
    {
      "download_info": {
        "url": "https://pypi.tuna.tsinghua.edu.cn/packages/07/cb/5f001272b6faeb23c1c9e0acc04d48eaaf5c862c17709d20e3469c6e0139/jmespath-0.10.0-py2.py3-none-any.whl",
        "archive_info": {
          "hash": "sha256=cdf6525904cc597730141d61b36f2e4b8ecc257c420fa2f4549bac2c2d0cb72f",
          "hashes": {
            "sha256": "cdf6525904cc597730141d61b36f2e4b8ecc257c420fa2f4549bac2c2d0cb72f"
          }
        }
      },
      "is_direct": false,
      "is_yanked": false,
      "requested": false,
      "metadata": {
        "metadata_version": "2.0",
        "name": "jmespath",
        "version": "0.10.0",
        "platform": [
          "UNKNOWN"
        ],
        "summary": "JSON Matching Expressions",
        "home_page": "https://github.com/jmespath/jmespath.py",
        "author": "James Saryerwinnie",
        "author_email": "js@jamesls.com",
        "license": "MIT",
        "classifier": [
          "Development Status :: 5 - Production/Stable",
          "Intended Audience :: Developers",
          "Natural Language :: English",
          "License :: OSI Approved :: MIT License",
          "Programming Language :: Python",
          "Programming Language :: Python :: 2",
          "Programming Language :: Python :: 2.6",
          "Programming Language :: Python :: 2.7",
          "Programming Language :: Python :: 3",
          "Programming Language :: Python :: 3.3",
          "Programming Language :: Python :: 3.4",
          "Programming Language :: Python :: 3.5",
          "Programming Language :: Python :: 3.6",
          "Programming Language :: Python :: 3.7",
          "Programming Language :: Python :: Implementation :: CPython",
          "Programming Language :: Python :: Implementation :: PyPy"
        ],
        "requires_python": ">=2.6, !=3.0.*, !=3.1.*, !=3.2.*",
        "description": "JMESPath\n========\n\n\n.. image:: https://badges.gitter.im/Join Chat.svg\n   :target: https://gitter.im/jmespath/chat\n\n\n.. image:: https://travis-ci.org/jmespath/jmespath.py.svg?branch=develop\n    :target: https://travis-ci.org/jmespath/jmespath.py\n\n\n.. image:: https://codecov.io/github/jmespath/jmespath.py/coverage.svg?branch=develop\n    :target: https://codecov.io/github/jmespath/jmespath.py?branch=develop\n\n\nJMESPath (pronounced \"james path\") allows you to declaratively specify how to\nextract elements from a JSON document.\n\nFor example, given this document::\n\n    {\"foo\": {\"bar\": \"baz\"}}\n\nThe jmespath expression ``foo.bar`` will return \"baz\".\n\nJMESPath also supports:\n\nReferencing elements in a list.  Given the data::\n\n    {\"foo\": {\"bar\": [\"one\", \"two\"]}}\n\nThe expression: ``foo.bar[0]`` will return \"one\".\nYou can also reference all the items in a list using the ``*``\nsyntax::\n\n   {\"foo\": {\"bar\": [{\"name\": \"one\"}, {\"name\": \"two\"}]}}\n\nThe expression: ``foo.bar[*].name`` will return [\"one\", \"two\"].\nNegative indexing is also supported (-1 refers to the last element\nin the list).  Given the data above, the expression\n``foo.bar[-1].name`` will return \"two\".\n\nThe ``*`` can also be used for hash types::\n\n   {\"foo\": {\"bar\": {\"name\": \"one\"}, \"baz\": {\"name\": \"two\"}}}\n\nThe expression: ``foo.*.name`` will return [\"one\", \"two\"].\n\n\nInstallation\n============\n\nYou can install JMESPath from pypi with:\n\n.. code:: bash\n\n    pip install jmespath\n\n\nAPI\n===\n\nThe ``jmespath.py`` library has two functions\nthat operate on python data structures.  You can use ``search``\nand give it the jmespath expression and the data:\n\n.. code:: python\n\n    >>> import jmespath\n    >>> path = jmespath.search('foo.bar', {'foo': {'bar': 'baz'}})\n    'baz'\n\nSimilar to the ``re`` module, you can use the ``compile`` function\nto compile the JMESPath expression and use this parsed expression\nto perform repeated searches:\n\n.. code:: python\n\n    >>> import jmespath\n    >>> expression = jmespath.compile('foo.bar')\n    >>> expression.search({'foo': {'bar': 'baz'}})\n    'baz'\n    >>> expression.search({'foo': {'bar': 'other'}})\n    'other'\n\nThis is useful if you're going to use the same jmespath expression to\nsearch multiple documents.  This avoids having to reparse the\nJMESPath expression each time you search a new document.\n\nOptions\n-------\n\nYou can provide an instance of ``jmespath.Options`` to control how\na JMESPath expression is evaluated.  The most common scenario for\nusing an ``Options`` instance is if you want to have ordered output\nof your dict keys.  To do this you can use either of these options:\n\n.. code:: python\n\n    >>> import jmespath\n    >>> jmespath.search('{a: a, b: b}',\n    ...                 mydata,\n    ...                 jmespath.Options(dict_cls=collections.OrderedDict))\n\n\n    >>> import jmespath\n    >>> parsed = jmespath.compile('{a: a, b: b}')\n    >>> parsed.search(mydata,\n    ...               jmespath.Options(dict_cls=collections.OrderedDict))\n\n\nCustom Functions\n~~~~~~~~~~~~~~~~\n\nThe JMESPath language has numerous\n`built-in functions\n<http://jmespath.org/specification.html#built-in-functions>`__, but it is\nalso possible to add your own custom functions.  Keep in mind that\ncustom function support in jmespath.py is experimental and the API may\nchange based on feedback.\n\n**If you have a custom function that you've found useful, consider submitting\nit to jmespath.site and propose that it be added to the JMESPath language.**\nYou can submit proposals\n`here <https://github.com/jmespath/jmespath.site/issues>`__.\n\nTo create custom functions:\n\n* Create a subclass of ``jmespath.functions.Functions``.\n* Create a method with the name ``_func_<your function name>``.\n* Apply the ``jmespath.functions.signature`` decorator that indicates\n  the expected types of the function arguments.\n* Provide an instance of your subclass in a ``jmespath.Options`` object.\n\nBelow are a few examples:\n\n.. code:: python\n\n    import jmespath\n    from jmespath import functions\n\n    # 1. Create a subclass of functions.Functions.\n    #    The function.Functions base class has logic\n    #    that introspects all of its methods and automatically\n    #    registers your custom functions in its function table.\n    class CustomFunctions(functions.Functions):\n\n        # 2 and 3.  Create a function that starts with _func_\n        # and decorate it with @signature which indicates its\n        # expected types.\n        # In this example, we're creating a jmespath function\n        # called \"unique_letters\" that accepts a single argument\n        # with an expected type \"string\".\n        @functions.signature({'types': ['string']})\n        def _func_unique_letters(self, s):\n            # Given a string s, return a sorted\n            # string of unique letters: 'ccbbadd' ->  'abcd'\n            return ''.join(sorted(set(s)))\n\n        # Here's another example.  This is creating\n        # a jmespath function called \"my_add\" that expects\n        # two arguments, both of which should be of type number.\n        @functions.signature({'types': ['number']}, {'types': ['number']})\n        def _func_my_add(self, x, y):\n            return x + y\n\n    # 4. Provide an instance of your subclass in a Options object.\n    options = jmespath.Options(custom_functions=CustomFunctions())\n\n    # Provide this value to jmespath.search:\n    # This will print 3\n    print(\n        jmespath.search(\n            'my_add(`1`, `2`)', {}, options=options)\n    )\n\n    # This will print \"abcd\"\n    print(\n        jmespath.search(\n            'foo.bar | unique_letters(@)',\n            {'foo': {'bar': 'ccbbadd'}},\n            options=options)\n    )\n\nAgain, if you come up with useful functions that you think make\nsense in the JMESPath language (and make sense to implement in all\nJMESPath libraries, not just python), please let us know at\n`jmespath.site <https://github.com/jmespath/jmespath.site/issues>`__.\n\n\nSpecification\n=============\n\nIf you'd like to learn more about the JMESPath language, you can check out\nthe `JMESPath tutorial <http://jmespath.org/tutorial.html>`__.  Also check\nout the `JMESPath examples page <http://jmespath.org/examples.html>`__ for\nexamples of more complex jmespath queries.\n\nThe grammar is specified using ABNF, as described in\n`RFC4234 <http://www.ietf.org/rfc/rfc4234.txt>`_.\nYou can find the most up to date\n`grammar for JMESPath here <http://jmespath.org/specification.html#grammar>`__.\n\nYou can read the full\n`JMESPath specification here <http://jmespath.org/specification.html>`__.\n\n\nTesting\n=======\n\nIn addition to the unit tests for the jmespath modules,\nthere is a ``tests/compliance`` directory that contains\n.json files with test cases.  This allows other implementations\nto verify they are producing the correct output.  Each json\nfile is grouped by feature.\n\n\nDiscuss\n=======\n\nJoin us on our `Gitter channel <https://gitter.im/jmespath/chat>`__\nif you want to chat or if you have any questions.\n\n\n"
      }
    },
    {
      "download_info": {
        "url": "https://pypi.tuna.tsinghua.edu.cn/packages/11/5c/0132193d7da2c735669a1ed103b142fd63c9455984d48c5a88a1a516efaa/aliyun_python_sdk_kms-2.16.5-py2.py3-none-any.whl",
        "archive_info": {
          "hash": "sha256=24b6cdc4fd161d2942619479c8d050c63ea9cd22b044fe33b60bbb60153786f0",
          "hashes": {
            "sha256": "24b6cdc4fd161d2942619479c8d050c63ea9cd22b044fe33b60bbb60153786f0"
          }
        }
      },
      "is_direct": false,
      "is_yanked": false,
      "requested": false,
      "metadata": {
        "metadata_version": "2.1",
        "name": "aliyun-python-sdk-kms",
        "version": "2.16.5",
        "platform": [
          "any"
        ],
        "summary": "The kms module of Aliyun Python sdk.",
        "keywords": [
          "aliyun",
          "sdk",
          "kms"
        ],
        "home_page": "http://develop.aliyun.com/sdk/python",
        "author": "Aliyun",
        "author_email": "aliyun-developers-efficiency@list.alibaba-inc.com",
        "license": "Apache",
        "classifier": [
          "Development Status :: 4 - Beta",
          "Intended Audience :: Developers",
          "License :: OSI Approved :: Apache Software License",
          "Programming Language :: Python",
          "Programming Language :: Python :: 2.6",
          "Programming Language :: Python :: 2.7",
          "Programming Language :: Python :: 3",
          "Programming Language :: Python :: 3.3",
          "Programming Language :: Python :: 3.4",
          "Programming Language :: Python :: 3.5",
          "Programming Language :: Python :: 3.6",
          "Topic :: Software Development"
        ],
        "requires_dist": [
          "aliyun-python-sdk-core >=2.11.5"
        ],
        "description": "=============================================================\naliyun-python-sdk-kms\n=============================================================\n\n.. This is the kms module of Aliyun Python SDK.\n\nAliyun Python SDK is the official software development kit. It makes things easy to integrate your Python application, library, or script with Aliyun services.\n\nThis module works on Python versions:\n\n2.6.5 and greater\n\n**Documentation:**\n\nPlease visit `http://develop.aliyun.com/sdk/python <http://develop.aliyun.com/sdk/python>`_\n\n\n"
      }
    },
    {
      "download_info": {
        "url": "https://pypi.tuna.tsinghua.edu.cn/packages/6b/b0/e595ce2a2527e169c3bcd6c33d2473c1918e0b7f6826a043ca1245dd4e5b/crcmod-1.7.tar.gz",
        "archive_info": {
          "hash": "sha256=dc7051a0db5f2bd48665a990d3ec1cc305a466a77358ca4492826f41f283601e",
          "hashes": {
            "sha256": "dc7051a0db5f2bd48665a990d3ec1cc305a466a77358ca4492826f41f283601e"
          }
        }
      },
      "is_direct": false,
      "is_yanked": false,
      "requested": false,
      "metadata": {
        "metadata_version": "2.4",
        "name": "crcmod",
        "version": "1.7",
        "dynamic": [
          "author",
          "author-email",
          "classifier",
          "description",
          "download-url",
          "home-page",
          "license",
          "license-file",
          "summary"
        ],
        "summary": "CRC Generator",
        "home_page": "http://crcmod.sourceforge.net/",
        "download_url": "http://sourceforge.net/projects/crcmod",
        "author": "Ray Buvel",
        "author_email": "rlbuvel@gmail.com",
        "license": "MIT",
        "license_file": [
          "LICENSE"
        ],
        "classifier": [
          "Development Status :: 5 - Production/Stable",
          "Intended Audience :: Developers",
          "Intended Audience :: Education",
          "Intended Audience :: End Users/Desktop",
          "Intended Audience :: Information Technology",
          "Intended Audience :: Science/Research",
          "License :: OSI Approved :: MIT License",
          "Operating System :: OS Independent",
          "Programming Language :: C",
          "Programming Language :: C++",
          "Programming Language :: Python",
          "Programming Language :: Python :: 2",
          "Programming Language :: Python :: 2.4",
          "Programming Language :: Python :: 2.5",
          "Programming Language :: Python :: 2.6",
          "Programming Language :: Python :: 2.7",
          "Programming Language :: Python :: 3",
          "Programming Language :: Python :: 3.1",
          "Topic :: Communications",
          "Topic :: Scientific/Engineering :: Interface Engine/Protocol Translator",
          "Topic :: Scientific/Engineering :: Mathematics",
          "Topic :: Utilities"
        ],
        "description": "===========================\ncrcmod for Calculating CRCs\n===========================\n\nThe software in this package is a Python module for generating objects that\ncompute the Cyclic Redundancy Check (CRC).  There is no attempt in this package\nto explain how the CRC works.  There are a number of resources on the web that\ngive a good explanation of the algorithms.  Just do a Google search for \"crc\ncalculation\" and browse till you find what you need.  Another resource can be\nfound in chapter 20 of the book \"Numerical Recipes in C\" by Press et. al.\n\nThis package allows the use of any 8, 16, 24, 32, or 64 bit CRC.  You can\ngenerate a Python function for the selected polynomial or an instance of the\nCrc class which provides the same interface as the ``md5`` and ``sha`` modules\nfrom the Python standard library.  A ``Crc`` class instance can also generate\nC/C++ source code that can be used in another application.\n\n----------\nGuidelines\n----------\n\nDocumentation is available from the doc strings.  It is up to you to decide\nwhat polynomials to use in your application.  If someone has not specified the\npolynomials to use, you will need to do some research to find one suitable for\nyour application.  Examples are available in the unit test script ``test.py``.\nYou may also use the ``predefined`` module to select one of the standard\npolynomials.\n\nIf you need to generate code for another language, I suggest you subclass the\n``Crc`` class and replace the method ``generateCode``.  Use ``generateCode`` as\na model for the new version.\n\n------------\nDependencies\n------------\n\nPython Version\n^^^^^^^^^^^^^^\n\nThe package has separate code to support the 2.x and 3.x Python series.\n\nFor the 2.x versions of Python, these versions have been tested:\n\n* 2.4\n* 2.5\n* 2.6\n* 2.7\n\nIt may still work on earlier versions of Python 2.x, but these have not been\nrecently tested.\n\nFor the 3.x versions of Python, these versions have been tested:\n\n* 3.1\n\nBuilding C extension\n^^^^^^^^^^^^^^^^^^^^\n\nTo build the C extension, the appropriate compiler tools for your platform must\nbe installed. Refer to the Python documentation for building C extensions for\ndetails.\n\n------------\nInstallation\n------------\n\nThe crcmod package is installed using ``distutils``.\nRun the following command::\n\n    python setup.py install\n\nIf the extension module builds, it will be installed.  Otherwise, the\ninstallation will include the pure Python version.  This will run significantly\nslower than the extension module but will allow the package to be used.\n\nFor Windows users who want to use the mingw32 compiler, run this command::\n\n    python setup.py build --compiler=mingw32 install\n\nFor Python 3.x, the install process is the same but you need to use the 3.x\ninterpreter.\n\n------------\nUnit Testing\n------------\n\nThe ``crcmod`` package has a module ``crcmod.test``, which contains unit\ntests for both ``crcmod`` and ``crcmod.predefined``.\n\nWhen you first install ``crcmod``, you should run the unit tests to make sure\neverything is installed properly.  The test script performs a number of tests\nincluding a comparison to the direct method which uses a class implementing\npolynomials over the integers mod 2.\n\nTo run the unit tests on Python >=2.5::\n\n    python -m crcmod.test\n\nAlternatively, in the ``test`` directory run::\n\n    python test_crcmod.py\n\n---------------\nCode Generation\n---------------\n\nThe crcmod package is capable of generating C functions that can be compiled\nwith a C or C++ compiler.  In the test directory, there is an examples.py\nscript that demonstrates how to use the code generator.  The result of this is\nwritten out to the file ``examples.c``.  The generated code was checked to make\nsure it compiles with the GCC compiler.\n\n-------\nLicense\n-------\n\nThe ``crcmod`` package is released under the MIT license. See the ``LICENSE``\nfile for details.\n\n------------\nContributors\n------------\n\nCraig McQueen\n"
      }
    },
    {
      "download_info": {
        "url": "https://pypi.tuna.tsinghua.edu.cn/packages/14/e5/fc82d72a58d41c393697aa18c9abe5ae1214ff6f2a5c18ac470f92777895/cryptography-46.0.3-cp38-abi3-manylinux_2_34_aarch64.whl",
        "archive_info": {
          "hash": "sha256=b02cf04496f6576afffef5ddd04a0cb7d49cf6be16a9059d793a30b035f6b6ac",
          "hashes": {
            "sha256": "b02cf04496f6576afffef5ddd04a0cb7d49cf6be16a9059d793a30b035f6b6ac"
          }
        }
      },
      "is_direct": false,
      "is_yanked": false,
      "requested": false,
      "metadata": {
        "metadata_version": "2.4",
        "name": "cryptography",
        "version": "46.0.3",
        "summary": "cryptography is a package which provides cryptographic recipes and primitives to Python developers.",
        "description_content_type": "text/x-rst; charset=UTF-8",
        "author_email": "The Python Cryptographic Authority and individual contributors <cryptography-dev@python.org>",
        "license_expression": "Apache-2.0 OR BSD-3-Clause",
        "license_file": [
          "LICENSE",
          "LICENSE.APACHE",
          "LICENSE.BSD"
        ],
        "classifier": [
          "Development Status :: 5 - Production/Stable",
          "Intended Audience :: Developers",
          "Natural Language :: English",
          "Operating System :: MacOS :: MacOS X",
          "Operating System :: POSIX",
          "Operating System :: POSIX :: BSD",
          "Operating System :: POSIX :: Linux",
          "Operating System :: Microsoft :: Windows",
          "Programming Language :: Python",
          "Programming Language :: Python :: 3",
          "Programming Language :: Python :: 3 :: Only",
          "Programming Language :: Python :: 3.8",
          "Programming Language :: Python :: 3.9",
          "Programming Language :: Python :: 3.10",
          "Programming Language :: Python :: 3.11",
          "Programming Language :: Python :: 3.12",
          "Programming Language :: Python :: 3.13",
          "Programming Language :: Python :: 3.14",
          "Programming Language :: Python :: Implementation :: CPython",
          "Programming Language :: Python :: Implementation :: PyPy",
          "Programming Language :: Python :: Free Threading :: 3 - Stable",
          "Topic :: Security :: Cryptography"
        ],
        "requires_dist": [
          "cffi>=1.14 ; python_full_version == '3.8.*' and platform_python_implementation != 'PyPy'",
          "cffi>=2.0.0 ; python_full_version >= '3.9' and platform_python_implementation != 'PyPy'",
          "typing-extensions>=4.13.2 ; python_full_version < '3.11'",
          "bcrypt>=3.1.5 ; extra == 'ssh'",
          "nox[uv]>=2024.4.15 ; extra == 'nox'",
          "cryptography-vectors==46.0.3 ; extra == 'test'",
          "pytest>=7.4.0 ; extra == 'test'",
          "pytest-benchmark>=4.0 ; extra == 'test'",
          "pytest-cov>=2.10.1 ; extra == 'test'",
          "pytest-xdist>=3.5.0 ; extra == 'test'",
          "pretend>=0.7 ; extra == 'test'",
          "certifi>=2024 ; extra == 'test'",
          "pytest-randomly ; extra == 'test-randomorder'",
          "sphinx>=5.3.0 ; extra == 'docs'",
          "sphinx-rtd-theme>=3.0.0 ; extra == 'docs'",
          "sphinx-inline-tabs ; extra == 'docs'",
          "pyenchant>=3 ; extra == 'docstest'",
          "readme-renderer>=30.0 ; extra == 'docstest'",
          "sphinxcontrib-spelling>=7.3.1 ; extra == 'docstest'",
          "build>=1.0.0 ; extra == 'sdist'",
          "ruff>=0.11.11 ; extra == 'pep8test'",
          "mypy>=1.14 ; extra == 'pep8test'",
          "check-sdist ; extra == 'pep8test'",
          "click>=8.0.1 ; extra == 'pep8test'"
        ],
        "requires_python": ">=3.8, !=3.9.0, !=3.9.1",
        "project_url": [
          "homepage, https://github.com/pyca/cryptography",
          "documentation, https://cryptography.io/",
          "source, https://github.com/pyca/cryptography/",
          "issues, https://github.com/pyca/cryptography/issues",
          "changelog, https://cryptography.io/en/latest/changelog/"
        ],
        "provides_extra": [
          "ssh",
          "nox",
          "test",
          "test-randomorder",
          "docs",
          "docstest",
          "sdist",
          "pep8test"
        ],
        "description": "pyca/cryptography\n=================\n\n.. image:: https://img.shields.io/pypi/v/cryptography.svg\n    :target: https://pypi.org/project/cryptography/\n    :alt: Latest Version\n\n.. image:: https://readthedocs.org/projects/cryptography/badge/?version=latest\n    :target: https://cryptography.io\n    :alt: Latest Docs\n\n.. image:: https://github.com/pyca/cryptography/actions/workflows/ci.yml/badge.svg\n    :target: https://github.com/pyca/cryptography/actions/workflows/ci.yml?query=branch%3Amain\n\n``cryptography`` is a package which provides cryptographic recipes and\nprimitives to Python developers. Our goal is for it to be your \"cryptographic\nstandard library\". It supports Python 3.8+ and PyPy3 7.3.11+.\n\n``cryptography`` includes both high level recipes and low level interfaces to\ncommon cryptographic algorithms such as symmetric ciphers, message digests, and\nkey derivation functions. For example, to encrypt something with\n``cryptography``'s high level symmetric encryption recipe:\n\n.. code-block:: pycon\n\n    >>> from cryptography.fernet import Fernet\n    >>> # Put this somewhere safe!\n    >>> key = Fernet.generate_key()\n    >>> f = Fernet(key)\n    >>> token = f.encrypt(b\"A really secret message. Not for prying eyes.\")\n    >>> token\n    b'...'\n    >>> f.decrypt(token)\n    b'A really secret message. Not for prying eyes.'\n\nYou can find more information in the `documentation`_.\n\nYou can install ``cryptography`` with:\n\n.. code-block:: console\n\n    $ pip install cryptography\n\nFor full details see `the installation documentation`_.\n\nDiscussion\n~~~~~~~~~~\n\nIf you run into bugs, you can file them in our `issue tracker`_.\n\nWe maintain a `cryptography-dev`_ mailing list for development discussion.\n\nYou can also join ``#pyca`` on ``irc.libera.chat`` to ask questions or get\ninvolved.\n\nSecurity\n~~~~~~~~\n\nNeed to report a security issue? Please consult our `security reporting`_\ndocumentation.\n\n\n.. _`documentation`: https://cryptography.io/\n.. _`the installation documentation`: https://cryptography.io/en/latest/installation/\n.. _`issue tracker`: https://github.com/pyca/cryptography/issues\n.. _`cryptography-dev`: https://mail.python.org/mailman/listinfo/cryptography-dev\n.. _`security reporting`: https://cryptography.io/en/latest/security/\n\n"
      }
    },
    {
      "download_info": {
        "url": "https://pypi.tuna.tsinghua.edu.cn/packages/50/52/adaf4c8c100a8c49d2bd058e5b551f73dfd8cb89eb4911e25a0c469b6b4e/pycryptodome-3.23.0-cp37-abi3-manylinux_2_17_aarch64.manylinux2014_aarch64.whl",
        "archive_info": {
          "hash": "sha256=67bd81fcbe34f43ad9422ee8fd4843c8e7198dd88dd3d40e6de42ee65fbe1490",
          "hashes": {
            "sha256": "67bd81fcbe34f43ad9422ee8fd4843c8e7198dd88dd3d40e6de42ee65fbe1490"
          }
        }
      },
      "is_direct": false,
      "is_yanked": false,
      "requested": false,
      "metadata": {
        "metadata_version": "2.1",
        "name": "pycryptodome",
        "version": "3.23.0",
        "platform": [
          "Posix; MacOS X; Windows"
        ],
        "summary": "Cryptographic library for Python",
        "home_page": "https://www.pycryptodome.org",
        "author": "Helder Eijs",
        "author_email": "helderijs@gmail.com",
        "license": "BSD, Public Domain",
        "license_file": [
          "LICENSE.rst",
          "AUTHORS.rst"
        ],
        "classifier": [
          "Development Status :: 5 - Production/Stable",
          "License :: OSI Approved :: BSD License",
          "License :: Public Domain",
          "Intended Audience :: Developers",
          "Operating System :: Unix",
          "Operating System :: Microsoft :: Windows",
          "Operating System :: MacOS :: MacOS X",
          "Topic :: Security :: Cryptography",
          "Programming Language :: Python :: 2",
          "Programming Language :: Python :: 2.7",
          "Programming Language :: Python :: 3",
          "Programming Language :: Python :: 3.7",
          "Programming Language :: Python :: 3.8",
          "Programming Language :: Python :: 3.9",
          "Programming Language :: Python :: 3.10",
          "Programming Language :: Python :: 3.11",
          "Programming Language :: Python :: 3.12",
          "Programming Language :: Python :: 3.13"
        ],
        "requires_python": ">=2.7, !=3.0.*, !=3.1.*, !=3.2.*, !=3.3.*, !=3.4.*, !=3.5.*, !=3.6.*",
        "project_url": [
          "Source, https://github.com/Legrandin/pycryptodome/",
          "Changelog, https://www.pycryptodome.org/src/changelog"
        ],
        "description": "\nPyCryptodome\n============\n\nPyCryptodome is a self-contained Python package of low-level\ncryptographic primitives.\n\nIt supports Python 2.7, Python 3.7 and newer, and PyPy.\n\nYou can install it with::\n\n    pip install pycryptodome\n\nAll modules are installed under the ``Crypto`` package.\n\nCheck the pycryptodomex_ project for the equivalent library that\nworks under the ``Cryptodome`` package.\n\nPyCryptodome is a fork of PyCrypto. It brings several enhancements\nwith respect to the last official version of PyCrypto (2.6.1),\nfor instance:\n\n* Authenticated encryption modes (GCM, CCM, EAX, SIV, OCB, KW, KWP)\n* Hybrid Public Key Encryption (HPKE)\n* Accelerated AES on Intel platforms via AES-NI\n* First class support for PyPy\n* Elliptic curves cryptography (NIST P-curves; Ed25519, Ed448, Curve25519)\n* Better and more compact API (`nonce` and `iv` attributes for ciphers,\n  automatic generation of random nonces and IVs, simplified CTR cipher mode,\n  and more)\n* SHA-3 (including SHAKE XOFs) and BLAKE2 hash algorithms\n* Salsa20 and ChaCha20 stream ciphers\n* scrypt and HKDF\n* Deterministic (EC)DSA and EdDSA\n* Password-protected PKCS#8 key containers\n* Shamir's Secret Sharing scheme\n* Random numbers get sourced directly from the OS (and not from a CSPRNG in userspace)\n* Simplified install process, including better support for Windows\n* Cleaner RSA and DSA key generation (largely based on FIPS 186-4)\n* Major clean ups and simplification of the code base\n\nPyCryptodome is not a wrapper to a separate C library like *OpenSSL*.\nTo the largest possible extent, algorithms are implemented in pure Python.\nOnly the pieces that are extremely critical to performance (e.g. block ciphers)\nare implemented as C extensions.\n\nFor more information, see the `homepage`_.\n\nAll the code can be downloaded from `GitHub`_.\n\n.. _pycryptodomex: https://pypi.python.org/pypi/pycryptodomex\n.. _`homepage`: http://www.pycryptodome.org\n.. _GitHub: https://github.com/Legrandin/pycryptodome\n"
      }
    },
    {
      "download_info": {
        "url": "https://pypi.tuna.tsinghua.edu.cn/packages/a0/e3/59cd50310fc9b59512193629e1984c1f95e5c8ae6e5d8c69532ccc65a7fe/pycparser-2.23-py3-none-any.whl",
        "archive_info": {
          "hash": "sha256=e5c6e8d3fbad53479cab09ac03729e0a9faf2bee3db8208a550daf5af81a5934",
          "hashes": {
            "sha256": "e5c6e8d3fbad53479cab09ac03729e0a9faf2bee3db8208a550daf5af81a5934"
          }
        }
      },
      "is_direct": false,
      "is_yanked": false,
      "requested": false,
      "metadata": {
        "metadata_version": "2.1",
        "name": "pycparser",
        "version": "2.23",
        "platform": [
          "Cross Platform"
        ],
        "summary": "C parser in Python",
        "home_page": "https://github.com/eliben/pycparser",
        "author": "Eli Bendersky",
        "author_email": "eliben@gmail.com",
        "maintainer": "Eli Bendersky",
        "license": "BSD-3-Clause",
        "license_file": [
          "LICENSE"
        ],
        "classifier": [
          "Development Status :: 5 - Production/Stable",
          "License :: OSI Approved :: BSD License",
          "Programming Language :: Python :: 3",
          "Programming Language :: Python :: 3.8",
          "Programming Language :: Python :: 3.9",
          "Programming Language :: Python :: 3.10",
          "Programming Language :: Python :: 3.11",
          "Programming Language :: Python :: 3.12",
          "Programming Language :: Python :: 3.13"
        ],
        "requires_python": ">=3.8",
        "description": "\n        pycparser is a complete parser of the C language, written in\n        pure Python using the PLY parsing library.\n        It parses C code into an AST and can serve as a front-end for\n        C compilers or analysis tools.\n    \n"
      }
    },
    {
      "download_info": {
        "url": "https://pypi.tuna.tsinghua.edu.cn/packages/23/de/c47967a11bfe68cb28d2f19e55c7027993c3721eba79813db65d245e4ced/pytorch_wpe-0.0.1-py3-none-any.whl",
        "archive_info": {
          "hash": "sha256=fa0dc9f818fba81b36c1a51a53331cf6ed975f29b33f23e07b0deb4bee82eaad",
          "hashes": {
            "sha256": "fa0dc9f818fba81b36c1a51a53331cf6ed975f29b33f23e07b0deb4bee82eaad"
          }
        }
      },
      "is_direct": false,
      "is_yanked": false,
      "requested": false,
      "metadata": {
        "metadata_version": "2.1",
        "name": "pytorch-wpe",
        "version": "0.0.1",
        "platform": [
          "UNKNOWN"
        ],
        "summary": "A pytorch implementation of Weighted Prediction Error",
        "home_page": "UNKNOWN",
        "author": "UNKNOWN",
        "author_email": "UNKNOWN",
        "license": "UNKNOWN",
        "requires_dist": [
          "numpy"
        ],
        "description": "UNKNOWN\n\n\n"
      }
    },
    {
      "download_info": {
        "url": "https://pypi.tuna.tsinghua.edu.cn/packages/46/a5/742c69b7bd144eb32b6e5fd50dbd8abbbc7a95fce2fe16e50156fa400e3b/sentencepiece-0.2.1-cp310-cp310-manylinux_2_27_aarch64.manylinux_2_28_aarch64.whl",
        "archive_info": {
          "hash": "sha256=d8b1d91545578852f128650b8cce4ec20f93d39b378ff554ebe66290f2dabb92",
          "hashes": {
            "sha256": "d8b1d91545578852f128650b8cce4ec20f93d39b378ff554ebe66290f2dabb92"
          }
        }
      },
      "is_direct": false,
      "is_yanked": false,
      "requested": false,
      "metadata": {
        "metadata_version": "2.4",
        "name": "sentencepiece",
        "version": "0.2.1",
        "summary": "Unsupervised text tokenizer and detokenizer.",
        "description_content_type": "text/markdown",
        "author_email": "Taku Kudo <taku@google.com>",
        "classifier": [
          "Programming Language :: Python :: 3",
          "Development Status :: 5 - Production/Stable",
          "Environment :: Console",
          "Intended Audience :: Developers",
          "Intended Audience :: Science/Research",
          "Operating System :: MacOS :: MacOS X",
          "Operating System :: Microsoft :: Windows",
          "Operating System :: POSIX :: Linux",
          "Programming Language :: Python",
          "Programming Language :: Python :: 3.9",
          "Programming Language :: Python :: 3.10",
          "Programming Language :: Python :: 3.11",
          "Programming Language :: Python :: 3.12",
          "Programming Language :: Python :: 3.13",
          "Programming Language :: Python :: 3.14",
          "Programming Language :: Python :: Free Threading :: 2 - Beta",
          "Topic :: Text Processing :: Linguistic",
          "Topic :: Software Development :: Libraries :: Python Modules"
        ],
        "requires_dist": [
          "pytest; extra == \"test\"",
          "test; extra == \"testpaths\""
        ],
        "requires_python": ">=3.9",
        "project_url": [
          "Homepage, https://github.com/google/sentencepiece"
        ],
        "provides_extra": [
          "test",
          "testpaths"
        ],
        "description": "# SentencePiece Python Wrapper\n\nPython wrapper for SentencePiece. This API will offer the encoding, decoding and training of Sentencepiece.\n\n## Build and Install SentencePiece\n\nFor Linux (x64/i686), macOS, and Windows(win32/x64/arm64) environment, you can simply use pip command to install SentencePiece python module.\n\n```\n% pip install sentencepiece\n```\n\nBefore building SentencePiece from source on Linux, ensure that the following dependencies are installed.\n\n```\n% sudo apt update\n% sudo apt install -y cmake pkg-config libsentencepiece-dev\n```\n\nTo build and install the Python wrapper from source, try the following commands to build and install wheel package.\n\n```\n% git clone https://github.com/google/sentencepiece.git\n% cd sentencepiece\n% mkdir build\n% cd build\n% cmake .. -DSPM_ENABLE_SHARED=OFF -DCMAKE_INSTALL_PREFIX=./root -DSPM_DISABLE_EMBEDDED_DATA=ON\n% make install\n% cd ../python\n% python setup.py bdist_wheel\n% pip install dist/sentencepiece*.whl\n```\n\nIf you don‚Äôt have write permission to the global site-packages directory or don‚Äôt want to install into it, please try:\n\n```\n% python setup.py install --user\n```\n\nFor Windows users who want to build from source, you can build and install the Python wrapper using Visual Studio. First, you need to install the `pwsh.exe` (Powershell 7). Use `winget install --id Microsoft.Powershell --source winget` to install directly. Then open the `Developer PowerShell for VS 2022`, and execute the following commands.\n\n```\ngit clone https://github.com/google/sentencepiece.git\ncd sentencepiece\nmkdir build\ncd build\ncmake .. -DSPM_ENABLE_SHARED=OFF -DCMAKE_INSTALL_PREFIX=\".\\root\" -DSPM_DISABLE_EMBEDDED_DATA=ON\ncmake --build . --config Release --target install\ncd ../python\npip install wheel\npython setup.py bdist_wheel\nGet-ChildItem .\\dist\\sentencepiece*.whl | ForEach-Object { pip install $_.FullName }\n```\n\n## Usage\n\nSee [this google colab page](https://github.com/google/sentencepiece/blob/master/python/sentencepiece_python_module_example.ipynb) to run sentencepiece interactively.\n\n### Segmentation\n\n```\n% python\n>>> import sentencepiece as spm\n>>> sp = spm.SentencePieceProcessor(model_file='test/test_model.model')\n\n>>> sp.encode('This is a test')\n[284, 47, 11, 4, 15, 400]\n\n>>> sp.encode(['This is a test', 'Hello world'], out_type=int)\n[[284, 47, 11, 4, 15, 400], [151, 88, 21, 887]]\n\n>>> sp.encode_as_ids(['This is a test', 'Hello world'])\n[[284, 47, 11, 4, 15, 400], [151, 88, 21, 887]]\n\n>>> sp.encode('This is a test', out_type=str)\n['‚ñÅThis', '‚ñÅis', '‚ñÅa', '‚ñÅ', 't', 'est']\n\n>>> sp.encode(['This is a test', 'Hello world'], out_type=str)\n[['‚ñÅThis', '‚ñÅis', '‚ñÅa', '‚ñÅ', 't', 'est'], ['‚ñÅHe', 'll', 'o', '‚ñÅworld']]\n\n>>> sp.encode_as_pieces(['This is a test', 'Hello world'])\n[['‚ñÅThis', '‚ñÅis', '‚ñÅa', '‚ñÅ', 't', 'est'], ['‚ñÅHe', 'll', 'o', '‚ñÅworld']]\n\n>>> proto = sp.encode('This is a test', out_type='immutable_proto')\n>>> for n in proto.pieces:\n...     print('piece=\"{}\" surface=\"{}\" id={} begin={} end={}'.format(n.piece, n.surface, n.id, n.begin, n.end))\n...\npiece=\"‚ñÅThis\" surface=\"This\" id=284 begin=0 end=4\npiece=\"‚ñÅis\" surface=\" is\" id=47 begin=4 end=7\npiece=\"‚ñÅa\" surface=\" a\" id=11 begin=7 end=9\npiece=\"‚ñÅ\" surface=\" \" id=4 begin=9 end=10\npiece=\"t\" surface=\"t\" id=15 begin=10 end=11\npiece=\"est\" surface=\"est\" id=400 begin=11 end=14\n\n>>> [[x.id for x in proto.pieces], [x.piece for x in proto.pieces], [x.begin for x in proto.pieces], [x.end for x in proto.pieces]]\n[[284, 47, 11, 4, 15, 400], ['‚ñÅThis', '‚ñÅis', '‚ñÅa', '‚ñÅ', 't', 'est'], [0, 4, 7, 9, 10, 11], [4, 7, 9, 10, 11, 14]]\n\n>>> proto2 = sp.encode_as_immutable_proto('This is a test')\n>>> proto2 == proto\nTrue\n\n>>> for _ in range(10):\n...     sp.encode('This is a test', out_type=str, enable_sampling=True, alpha=0.1, nbest_size=-1)\n...\n['‚ñÅ', 'This', '‚ñÅ', 'is', '‚ñÅa', '‚ñÅ', 't', 'e', 'st']\n['‚ñÅT', 'h', 'i', 's', '‚ñÅis', '‚ñÅa', '‚ñÅ', 'te', 's', 't']\n['‚ñÅT', 'h', 'is', '‚ñÅ', 'is', '‚ñÅ', 'a', '‚ñÅ', 't', 'est']\n['‚ñÅ', 'This', '‚ñÅis', '‚ñÅ', 'a', '‚ñÅ', 't', 'e', 'st']\n['‚ñÅ', 'This', '‚ñÅ', 'is', '‚ñÅ', 'a', '‚ñÅ', 't', 'e', 's', 't']\n['‚ñÅThis', '‚ñÅis', '‚ñÅa', '‚ñÅ', 'te', 's', 't']\n['‚ñÅThis', '‚ñÅis', '‚ñÅ', 'a', '‚ñÅ', 't', 'e', 'st']\n['‚ñÅ', 'T', 'h', 'is', '‚ñÅ', 'is', '‚ñÅ', 'a', '‚ñÅ', 'te', 'st']\n['‚ñÅ', 'This', '‚ñÅ', 'i', 's', '‚ñÅa', '‚ñÅ', 't', 'e', 'st']\n['‚ñÅThis', '‚ñÅ', 'is', '‚ñÅa', '‚ñÅ', 't', 'est']\n\n>> sp.nbest_encode('This is a test', nbest_size=5, out_type=str)\n[['‚ñÅThis', '‚ñÅis', '‚ñÅa', '‚ñÅ', 't', 'est'],\n['‚ñÅThis', '‚ñÅis', '‚ñÅa', '‚ñÅ', 'te', 'st'],\n['‚ñÅThis', '‚ñÅis', '‚ñÅa', '‚ñÅ', 'te', 's', 't'],\n['‚ñÅThis', '‚ñÅis', '‚ñÅa', '‚ñÅ', 't', 'e', 'st'],\n['‚ñÅThis', '‚ñÅis', '‚ñÅa', '‚ñÅ', 't', 'es', 't']]\n\n>>> sp.sample_encode_and_score('This is a test', num_samples=5, alpha=0.1, out_type=str, wor=True)\n[(['‚ñÅThis', '‚ñÅ', 'i', 's', '‚ñÅa', '‚ñÅ', 'te', 's', 't'], -3.043105125427246),\n(['‚ñÅThis', '‚ñÅ', 'i', 's', '‚ñÅa', '‚ñÅ', 'te', 'st'], -2.8475849628448486),\n(['‚ñÅ', 'This', '‚ñÅis', '‚ñÅ', 'a', '‚ñÅ', 'te', 'st'], -3.043248176574707),\n(['‚ñÅ', 'This', '‚ñÅis', '‚ñÅa', '‚ñÅ', 't', 'e', 'st'], -2.87727689743042),\n(['‚ñÅ', 'This', '‚ñÅ', 'i', 's', '‚ñÅ', 'a', '‚ñÅ', 't', 'est'], -3.6284031867980957)]\n\n>>> sp.decode([284, 47, 11, 4, 15, 400])\n'This is a test'\n\n>>> sp.decode([[284, 47, 11, 4, 15, 400], [151, 88, 21, 887]])\n['This is a test', 'Hello world']\n\n>>> proto = sp.decode([284, 47, 11, 4, 15, 400], out_type='immutable_proto')\n>>> proto.text\n'This is a test'\n\n>>> sp.decode(['‚ñÅ', 'This', '‚ñÅ', 'is', '‚ñÅa', '‚ñÅ', 't', 'e', 'st'])\n'This is a test'\n\n>>> sp.decode([['‚ñÅThis', '‚ñÅis', '‚ñÅa', '‚ñÅ', 't', 'est'], ['‚ñÅHe', 'll', 'o', '‚ñÅworld']])\n['This is a test', 'Hello world']\n\n>>> sp.get_piece_size()\n1000\n\n>>> sp.id_to_piece(2)\n'</s>'\n\n>>> sp.id_to_piece([2, 3, 4])\n['</s>', '\\r', '‚ñÅ']\n\n>>> sp.piece_to_id('<s>')\n1\n\n>>> sp.piece_to_id(['</s>', '\\r', '‚ñÅ'])\n[2, 3, 4]\n\n>>> len(sp)\n1000\n\n>>> sp['</s>']\n2\n```\n\n### Model Training\n\nTraining is performed by passing parameters of [spm_train](https://github.com/google/sentencepiece#train-sentencepiece-model) to SentencePieceTrainer.train() function.\n\n```\n>>> import sentencepiece as spm\n>>> spm.SentencePieceTrainer.train(input='test/botchan.txt', model_prefix='m', vocab_size=1000, user_defined_symbols=['foo', 'bar'])\nsentencepiece_trainer.cc(73) LOG(INFO) Starts training with :\ntrainer_spec {\n  input: test/botchan.txt\n  .. snip\nunigram_model_trainer.cc(500) LOG(INFO) EM sub_iter=1 size=1188 obj=10.2839 num_tokens=32182 num_tokens/piece=27.0892\nunigram_model_trainer.cc(500) LOG(INFO) EM sub_iter=0 size=1100 obj=10.4269 num_tokens=33001 num_tokens/piece=30.0009\nunigram_model_trainer.cc(500) LOG(INFO) EM sub_iter=1 size=1100 obj=10.4069 num_tokens=33002 num_tokens/piece=30.0018\ntrainer_interface.cc(595) LOG(INFO) Saving model: m.model\ntrainer_interface.cc(619) LOG(INFO) Saving vocabs: m.vocab\n>>>\n```\n\n### Training without local filesystem\n\nSentencepiece trainer can receive any iterable object to feed training sentences. You can also pass a file object (instance with write() method) to emit the output model to any devices. These features are useful to run sentencepiece on environment that have limited access to the local file system (e.g., Google colab.)\n\n```\nimport urllib.request\nimport io\nimport sentencepiece as spm\n\n# Loads model from URL as iterator and stores the model to BytesIO.\nmodel = io.BytesIO()\nwith urllib.request.urlopen(\n    'https://raw.githubusercontent.com/google/sentencepiece/master/data/botchan.txt'\n) as response:\n  spm.SentencePieceTrainer.train(\n      sentence_iterator=response, model_writer=model, vocab_size=1000)\n\n# Serialize the model as file.\n# with open('out.model', 'wb') as f:\n#   f.write(model.getvalue())\n\n# Directly load the model from serialized model.\nsp = spm.SentencePieceProcessor(model_proto=model.getvalue())\nprint(sp.encode('this is test'))\n```\n\n### Free Threading support\nExperimental support for no-GIL/Free-Threading has been introduced since v0.2.1. For more details, please refer to [this page](https://py-free-threading.github.io.).\nThis operates similarly to how [NumPy](https://numpy.org/devdocs/reference/thread_safety.html#free-threaded-python) handles it.\n\nThe C++ library's const and static methods, e.g., encode(), decode() and train(), are designed to work in a non-GIL environment.\nHowever, non-const methods, e.g., load(), may have potential data race issues, so please ensure you implement appropriate locks beforehand.\n\nWhile this limitation might be removed in the future, please note that it's not a simple fix, as it would require additional shared locks in C++.\n"
      }
    },
    {
      "download_info": {
        "url": "https://pypi.tuna.tsinghua.edu.cn/packages/b7/ce/149a00dd41f10bc29e5921b496af8b574d8413afcd5e30dfa0ed46c2cc5e/six-1.17.0-py2.py3-none-any.whl",
        "archive_info": {
          "hash": "sha256=4721f391ed90541fddacab5acf947aa0d3dc7d27b2e1e8eda2be8970586c3274",
          "hashes": {
            "sha256": "4721f391ed90541fddacab5acf947aa0d3dc7d27b2e1e8eda2be8970586c3274"
          }
        }
      },
      "is_direct": false,
      "is_yanked": false,
      "requested": false,
      "metadata": {
        "metadata_version": "2.1",
        "name": "six",
        "version": "1.17.0",
        "summary": "Python 2 and 3 compatibility utilities",
        "home_page": "https://github.com/benjaminp/six",
        "author": "Benjamin Peterson",
        "author_email": "benjamin@python.org",
        "license": "MIT",
        "license_file": [
          "LICENSE"
        ],
        "classifier": [
          "Development Status :: 5 - Production/Stable",
          "Programming Language :: Python :: 2",
          "Programming Language :: Python :: 3",
          "Intended Audience :: Developers",
          "License :: OSI Approved :: MIT License",
          "Topic :: Software Development :: Libraries",
          "Topic :: Utilities"
        ],
        "requires_python": ">=2.7, !=3.0.*, !=3.1.*, !=3.2.*",
        "description": ".. image:: https://img.shields.io/pypi/v/six.svg\n   :target: https://pypi.org/project/six/\n   :alt: six on PyPI\n\n.. image:: https://readthedocs.org/projects/six/badge/?version=latest\n   :target: https://six.readthedocs.io/\n   :alt: six's documentation on Read the Docs\n\n.. image:: https://img.shields.io/badge/license-MIT-green.svg\n   :target: https://github.com/benjaminp/six/blob/master/LICENSE\n   :alt: MIT License badge\n\nSix is a Python 2 and 3 compatibility library.  It provides utility functions\nfor smoothing over the differences between the Python versions with the goal of\nwriting Python code that is compatible on both Python versions.  See the\ndocumentation for more information on what is provided.\n\nSix supports Python 2.7 and 3.3+.  It is contained in only one Python\nfile, so it can be easily copied into your project. (The copyright and license\nnotice must be retained.)\n\nOnline documentation is at https://six.readthedocs.io/.\n\nBugs can be reported to https://github.com/benjaminp/six.  The code can also\nbe found there.\n"
      }
    },
    {
      "download_info": {
        "url": "https://pypi.tuna.tsinghua.edu.cn/packages/e0/1d/b5d63f1a6b824282b57f7b581810d20b7a28ca951f2d5b59f1eb0782c12b/tensorboardx-2.6.4-py3-none-any.whl",
        "archive_info": {
          "hash": "sha256=5970cf3a1f0a6a6e8b180ccf46f3fe832b8a25a70b86e5a237048a7c0beb18e2",
          "hashes": {
            "sha256": "5970cf3a1f0a6a6e8b180ccf46f3fe832b8a25a70b86e5a237048a7c0beb18e2"
          }
        }
      },
      "is_direct": false,
      "is_yanked": false,
      "requested": false,
      "metadata": {
        "metadata_version": "2.4",
        "name": "tensorboardX",
        "version": "2.6.4",
        "dynamic": [
          "license-file"
        ],
        "summary": "TensorBoardX lets you watch Tensors Flow without Tensorflow",
        "description_content_type": "text/x-rst",
        "author_email": "Tzu-Wei Huang <huang.dexter@gmail.com>",
        "license_expression": "MIT",
        "license_file": [
          "LICENSE"
        ],
        "classifier": [
          "Development Status :: 4 - Beta",
          "Intended Audience :: Developers",
          "Natural Language :: English",
          "Programming Language :: Python :: 3",
          "Programming Language :: Python :: 3.9",
          "Programming Language :: Python :: 3.10",
          "Programming Language :: Python :: 3.11",
          "Programming Language :: Python :: 3.12"
        ],
        "requires_dist": [
          "numpy",
          "packaging",
          "protobuf>=3.20"
        ],
        "requires_python": ">=3.9",
        "project_url": [
          "Homepage, https://github.com/lanpa/tensorboardX",
          "Repository, https://github.com/lanpa/tensorboardX.git"
        ],
        "description": "History\n=======\n2.6.3 (2025-04-24)\n---------------------\n* Support PaddlePaddle tensors\n* Added a use_metadata context manager\n* MoviePy version 2 support\n\n2.6.2.2 (2023-08-20)\n---------------------\n* Added protobuf's lower bound version (>=3.20)\n\n2.6.2.1 (2023-08-20)\n---------------------\n* [Yanked because operation miss] Added protobuf's lower bound version (>=3.20)\n\n2.6.2 (2023-07-30)\n-------------------\n* [Yanked because wrong dependency] Removed version limit for protobuf\n\n2.6.1 (2023-06-18)\n-------------------\n* Expose use_strict_trace parameter in add_graph (#694)\n* Upgrade to protobuf 4\n* Fix git based package versioning\n* Fix GCS Connection Error #606 (#686)\n\n2.6 (2023-02-12)\n-------------------\n* Fixed several deprecation warnings\n* Update dependencies\n\n2.5.1 (2022-06-05)\n-------------------\n* Enforce protobuf's version upper bound\n\n2.5 (2022-02-22)\n-----------------\n* Fix deprecation warnings\n* Comet integration improvements (#658)\n\n2.4.1 (2021-11-20)\n-------------------\n* Fix a comet plugin bug if writer is reused. (#639)\n\n2.4 (2021-06-30)\n-----------------\n* Remove a dependency issue. (#631)\n\n2.3 (2021-06-20)\n-----------------\n* Support logging to comet.ml simutaneously.\n\n2.2 (2021-04-03)\n-----------------\n* Support for type hints.\n* Dropped Python 2 support.\n* Bug fixes, see the commit log in Github.\n\n2.1 (2020-07-05)\n-----------------\n* Global SummaryWriter that mimics python's default logger class, concurrent write is supported.\n* 200x speed up for add_audio. Please install the soundfile package for this feature.\n* Supports jax tensors.\n* The add_graph function is delegated to the one in torch.utils.tensorboard.\n* Bug fixes, see the commit log in Github.\n\n2.0 (2019-12-31)\n-----------------\n* Now you can tag Hparams trials with custom name instead of the default epoch time\n* Fixed a bug that add_hparams are rendered incorrectly with non-string values\n* Supports logging to Amazon S3 or Google Cloud Storage\n* Bug fixes and error message for add_embedding function\n* Draw openvino format with add_openvino_graph\n\n1.9 (2019-10-04)\n-----------------\n* Use new JIT backend for pytorch. This works better with pytorch 1.2 and 1.3\n* Supports hparams plugin\n* add_embedding now supports numpy array input\n\n1.8 (2019-07-05)\n-----------------\n* Draw label text on image with bounding box provided.\n* crc32c speed up (optional by installing crc32c manually)\n* Rewrite add_graph. onnx backend is replaced by JIT to support more advanced structure.\n* Now you can add_mesh() to visualize colorful point cloud or meshes.\n\n1.7 (2019-05-19)\n-----------------\n* Able to write to S3\n* Fixed raw histogram issue that nothing is shown in TensorBoard\n* Users can use various image/video dimension permutation by passing 'dataformats' parameter.\n* You can bybass the writer by passing write_to_disk=True to SummaryWriter\n\n\n1.6 (2019-01-02)\n-----------------\n* Many graph related bug is fixed in this version.\n* New function: add_images(). This function accepts 4D iamge tensor. See documentation.\n* Make add_image_with_boxes() usable.\n* API change: add_video now accepts BxTxCxHxW instead of BxCxTxHxW tensor.\n\n1.5 (2018-12-10)\n-----------------\n* Add API for Custom scalar\n* Add support for logging directly to S3\n* Add support for Caffe2 graph\n* Pytorch 1.0.0 JIT graph support (alpha-release)\n\n1.4 (2018-08-09)\n-----------------\n* Made add_text compatible with tensorboard>1.6\n* Fix the issue of strange histogram if default binning method is used\n* Supports passing matplotlib figures to add_image()\n* Resolve namespace confliction with TF tensorboard\n* add_image_boxes function\n* Supports custom timestamp for event\n\n1.2 (2018-04-21)\n-----------------\n* Supports tensorshape information in graph visualization. Drop support for 0.3.1\n* Adds add_video function\n\n1.1 (2018-02-21)\n-----------------\n* Supports pytorch 0.3.1 (hacky)\n\n1.0 (2018-01-18)\n-----------------\n* Supports graph (the pretty one)\n\n0.9 (2017-11-11)\n-----------------\n* Supports markdown for add_text function\n* It's ready to log precision recall curve (needs tensorboard>=0.4)\n* Adds context manager for the SummaryWriter class\n\n0.8 (2017-09-25)\n-----------------\n* Package name renamed to tensorboardX to fix namespace confliction with tensorflow's tensorboard\n* Supports multi-scalars and JSON export\n* Multiple Embeddings in One Experiment \n* Supports Chainer and mxnet\n\n0.7 (2017-08-22)\n-----------------\n* remove tensorflow dependency for embedding function\n* fixed incorrect image<->label pairing in embedding function (#12)\n* unifies API call and adds docstring. Documentation is available at: http://tensorboard-pytorch.readthedocs.io/\n\n0.6.5 (2017-07-30)\n------------------\n* add travis test (py2.7, py3.6)\n* add support for python2 (in PyPI)\n\n0.6 (2017-07-18)\n-----------------\n* supports embedding\n\n0.5 (2017-07-18)\n-----------------\n* supports graph summary\n* fixed np.histogram issue\n\n0.4 (2017-07-12)\n-----------------\n* supports text summary\n\n0.3 (2017-07-03)\n-----------------\n* supports audio summary\n\n0.2 (2017-06-24)\n-----------------\n* simplifies add_image API\n* speed up add_histogram API by 35x\n\n\n0.1 (2017-06-13)\n------------------\n* First commit. Reference:\n\nhttps://github.com/TeamHG-Memex/tensorboard_logger\nhttps://github.com/dmlc/tensorboard\n"
      }
    },
    {
      "download_info": {
        "url": "https://pypi.tuna.tsinghua.edu.cn/packages/7d/4f/f743761e41d3b2b2566748eb76bbff2b43e14d5fcab694f494a16458b05f/protobuf-6.33.2-cp39-abi3-manylinux2014_aarch64.whl",
        "archive_info": {
          "hash": "sha256=b5d3b5625192214066d99b2b605f5783483575656784de223f00a8d00754fc0e",
          "hashes": {
            "sha256": "b5d3b5625192214066d99b2b605f5783483575656784de223f00a8d00754fc0e"
          }
        }
      },
      "is_direct": false,
      "is_yanked": false,
      "requested": false,
      "metadata": {
        "metadata_version": "2.1",
        "name": "protobuf",
        "version": "6.33.2",
        "home_page": "https://developers.google.com/protocol-buffers/",
        "author": "protobuf@googlegroups.com",
        "author_email": "protobuf@googlegroups.com",
        "license": "3-Clause BSD License",
        "classifier": [
          "Programming Language :: Python",
          "Programming Language :: Python :: 3",
          "Programming Language :: Python :: 3.9",
          "Programming Language :: Python :: 3.10",
          "Programming Language :: Python :: 3.11",
          "Programming Language :: Python :: 3.12",
          "Programming Language :: Python :: 3.13"
        ],
        "requires_python": ">=3.9",
        "description": "UNKNOWN\n"
      }
    },
    {
      "download_info": {
        "url": "https://pypi.tuna.tsinghua.edu.cn/packages/f4/c5/9b4d756a7ada951e9b17dcc636f98ed1073c737ae809b150ef408afb6298/torch_complex-0.4.4-py3-none-any.whl",
        "archive_info": {
          "hash": "sha256=6ab4ecd4f3a16e3adb70a7f7cd2e769a9dfd07d7a8e27d04ff9c621ebbe34b13",
          "hashes": {
            "sha256": "6ab4ecd4f3a16e3adb70a7f7cd2e769a9dfd07d7a8e27d04ff9c621ebbe34b13"
          }
        }
      },
      "is_direct": false,
      "is_yanked": false,
      "requested": false,
      "metadata": {
        "metadata_version": "2.1",
        "name": "torch-complex",
        "version": "0.4.4",
        "platform": [
          "UNKNOWN"
        ],
        "summary": "A fugacious python class for PyTorch-ComplexTensor",
        "description_content_type": "text/markdown",
        "home_page": "https://github.com/kamo-naoyuki/torch_complex",
        "author": "Naoyuki Kamo",
        "author_email": "naoyuki.kamo829@gmail.com",
        "license": "UNKNOWN",
        "classifier": [
          "Programming Language :: Python",
          "Programming Language :: Python :: 3",
          "Programming Language :: Python :: 3.6",
          "Programming Language :: Python :: 3.7",
          "Programming Language :: Python :: 3.8",
          "Development Status :: 5 - Production/Stable",
          "Intended Audience :: Science/Research",
          "Operating System :: POSIX :: Linux",
          "License :: OSI Approved :: Apache Software License",
          "Topic :: Software Development :: Libraries :: Python Modules"
        ],
        "requires_dist": [
          "packaging",
          "numpy"
        ],
        "description": "# pytorch_complex\n\n[![PyPI version](https://badge.fury.io/py/torch-complex.svg)](https://badge.fury.io/py/torch-complex)\n[![Python Versions](https://img.shields.io/pypi/pyversions/torch-complex.svg)](https://pypi.org/project/torch-complex/)\n[![Downloads](https://pepy.tech/badge/torch-complex)](https://pepy.tech/project/torch-complex)\n[![Build Status](https://travis-ci.org/kamo-naoyuki/pytorch_complex.svg?branch=master)](https://travis-ci.org/kamo-naoyuki/pytorch_complex)\n[![codecov](https://codecov.io/gh/kamo-naoyuki/pytorch_complex/branch/master/graph/badge.svg)](https://codecov.io/gh/kamo-naoyuki/pytorch_complex)\n\nA temporal python class for PyTorch-ComplexTensor\n\n\n## What is this?\nA Python class to perform as `ComplexTensor` in PyTorch: Nothing except for the following,\n\n```python\nclass ComplexTensor: \n    def __init__(self, ...):\n        self.real = torch.Tensor(...)\n        self.imag = torch.Tensor(...)\n```\n\n### Why?\nPyTorch is great DNN Python library, except that it doesn't support `ComplexTensor` in Python level.\n\nhttps://github.com/pytorch/pytorch/issues/755\n\nI'm looking forward to the completion, but I need `ComplexTensor` for now.\n I created this cheap module for the temporal replacement of it. Thus, I'll throw away this project as soon as  `ComplexTensor` is completely supported!\n\n## Requirements\n\n```\nPython>=3.6\nPyTorch>=1.0\n```\n\n## Install\n\n```\npip install torch_complex\n```\n\n## How to use\n\n### Basic mathematical operation\n```python\nimport numpy as np\nfrom torch_complex.tensor import ComplexTensor\n\nreal = np.random.randn(3, 10, 10)\nimag = np.random.randn(3, 10, 10)\n\nx = ComplexTensor(real, imag)\nx.numpy()\n\nx + x\nx * x\nx - x\nx / x\nx ** 1.5\nx @ x  # Batch-matmul\nx.conj()\nx.inverse() # Batch-inverse\n```\n\nAll are implemented with combinations of computation of `RealTensor` in python level, thus the speed„ÄÄis not good enough.\n\n\n### Functional\n\n```python\nimport torch_complex.functional as F\nF.cat([x, x])\nF.stack([x, x])\nF.matmul(x, x)  # Same as x @ x\nF.einsum('bij,bjk,bkl->bil', [x, x, x])\n```\n\n### For DNN\nAlmost all methods that `torch.Tensor` has are implemented. \n\n```python\nx.cuda()\nx.cpu()\n(x + x).sum().backward()\n```\n\n\n"
      }
    },
    {
      "download_info": {
        "url": "https://pypi.tuna.tsinghua.edu.cn/packages/6b/b1/c24deeda9baf1fd491aaad941ed89e0fed6c583a117fd7b79e0a33a1e6c0/umap_learn-0.5.9.post2-py3-none-any.whl",
        "archive_info": {
          "hash": "sha256=fbe51166561e0e7fab00ef3d516ac2621243b8d15cf4bef9f656d701736b16a0",
          "hashes": {
            "sha256": "fbe51166561e0e7fab00ef3d516ac2621243b8d15cf4bef9f656d701736b16a0"
          }
        }
      },
      "is_direct": false,
      "is_yanked": false,
      "requested": false,
      "metadata": {
        "metadata_version": "2.4",
        "name": "umap-learn",
        "version": "0.5.9.post2",
        "dynamic": [
          "license-file"
        ],
        "summary": "Uniform Manifold Approximation and Projection",
        "description_content_type": "text/x-rst",
        "keywords": [
          "dimension reduction",
          "umap",
          "t-sne",
          "manifold"
        ],
        "maintainer_email": "Leland McInnes <leland.mcinnes@gmail.com>",
        "license": "BSD",
        "license_file": [
          "LICENSE.txt"
        ],
        "classifier": [
          "Development Status :: 3 - Alpha",
          "Intended Audience :: Science/Research",
          "Intended Audience :: Developers",
          "License :: OSI Approved",
          "Programming Language :: C",
          "Programming Language :: Python",
          "Topic :: Software Development",
          "Topic :: Scientific/Engineering",
          "Operating System :: Microsoft :: Windows",
          "Operating System :: POSIX",
          "Operating System :: Unix",
          "Operating System :: MacOS",
          "Programming Language :: Python :: 3.9",
          "Programming Language :: Python :: 3.10",
          "Programming Language :: Python :: 3.11",
          "Programming Language :: Python :: 3.12"
        ],
        "requires_dist": [
          "numpy>=1.23",
          "scipy>=1.3.1",
          "scikit-learn>=1.6",
          "numba>=0.51.2",
          "pynndescent>=0.5",
          "tqdm",
          "pandas; extra == \"plot\"",
          "matplotlib; extra == \"plot\"",
          "datashader; extra == \"plot\"",
          "bokeh; extra == \"plot\"",
          "holoviews; extra == \"plot\"",
          "colorcet; extra == \"plot\"",
          "seaborn; extra == \"plot\"",
          "scikit-image; extra == \"plot\"",
          "dask; extra == \"plot\"",
          "tensorflow>=2.1; extra == \"parametric-umap\"",
          "tbb>=2019.0; extra == \"tbb\"",
          "pytest; extra == \"test\""
        ],
        "requires_python": ">=3.9",
        "project_url": [
          "Homepage, http://github.com/lmcinnes/umap",
          "Repository, http://github.com/lmcinnes/umap"
        ],
        "provides_extra": [
          "plot",
          "parametric-umap",
          "tbb",
          "test"
        ],
        "description": ".. -*- mode: rst -*-\n\n.. image:: doc/logo_large.png\n  :width: 600\n  :alt: UMAP logo\n  :align: center\n\n|pypi_version|_ |pypi_downloads|_\n\n|conda_version|_ |conda_downloads|_\n\n|License|_ |build_status|_ |Coverage|_\n\n|Docs|_ |joss_paper|_\n\n.. |pypi_version| image:: https://img.shields.io/pypi/v/umap-learn.svg\n.. _pypi_version: https://pypi.python.org/pypi/umap-learn/\n\n.. |pypi_downloads| image:: https://pepy.tech/badge/umap-learn/month\n.. _pypi_downloads: https://pepy.tech/project/umap-learn\n\n.. |conda_version| image:: https://anaconda.org/conda-forge/umap-learn/badges/version.svg\n.. _conda_version: https://anaconda.org/conda-forge/umap-learn\n\n.. |conda_downloads| image:: https://anaconda.org/conda-forge/umap-learn/badges/downloads.svg\n.. _conda_downloads: https://anaconda.org/conda-forge/umap-learn\n\n.. |License| image:: https://img.shields.io/pypi/l/umap-learn.svg\n.. _License: https://github.com/lmcinnes/umap/blob/master/LICENSE.txt\n\n.. |build_status| image:: https://dev.azure.com/TutteInstitute/build-pipelines/_apis/build/status/lmcinnes.umap?branchName=master\n.. _build_status: https://dev.azure.com/TutteInstitute/build-pipelines/_build/latest?definitionId=2&branchName=master\n\n.. |Coverage| image:: https://coveralls.io/repos/github/lmcinnes/umap/badge.svg\n.. _Coverage: https://coveralls.io/github/lmcinnes/umap\n\n.. |Docs| image:: https://readthedocs.org/projects/umap-learn/badge/?version=latest\n.. _Docs: https://umap-learn.readthedocs.io/en/latest/?badge=latest\n\n.. |joss_paper| image:: http://joss.theoj.org/papers/10.21105/joss.00861/status.svg\n.. _joss_paper: https://doi.org/10.21105/joss.00861\n\n====\nUMAP\n====\n\nUniform Manifold Approximation and Projection (UMAP) is a dimension reduction\ntechnique that can be used for visualisation similarly to t-SNE, but also for\ngeneral non-linear dimension reduction. The algorithm is founded on three\nassumptions about the data:\n\n1. The data is uniformly distributed on a Riemannian manifold;\n2. The Riemannian metric is locally constant (or can be approximated as such);\n3. The manifold is locally connected.\n\nFrom these assumptions it is possible to model the manifold with a fuzzy\ntopological structure. The embedding is found by searching for a low dimensional\nprojection of the data that has the closest possible equivalent fuzzy\ntopological structure.\n\nThe details for the underlying mathematics can be found in\n`our paper on ArXiv <https://arxiv.org/abs/1802.03426>`_:\n\nMcInnes, L, Healy, J, *UMAP: Uniform Manifold Approximation and Projection\nfor Dimension Reduction*, ArXiv e-prints 1802.03426, 2018\n\nA broader introduction to UMAP targetted the scientific community can be found \nin our `paper published in Nature Review Methods Primers  <https://doi.org/10.1038/s43586-024-00363-x>`_:\n\nHealy, J., McInnes, L. *Uniform manifold approximation and projection*. Nat Rev Methods \nPrimers 4, 82 (2024). \n\nA read only version of this paper can accessed via `link <https://rdcu.be/d0YZT>`_\n\nThe important thing is that you don't need to worry about that‚Äîyou can use\nUMAP right now for dimension reduction and visualisation as easily as a drop\nin replacement for scikit-learn's t-SNE.\n\nDocumentation is `available via Read the Docs <https://umap-learn.readthedocs.io/>`_.\n\n**New: this package now also provides support for densMAP.** The densMAP algorithm augments UMAP\nto preserve local density information in addition to the topological structure of the data.\nDetails of this method are described in the following `paper <https://doi.org/10.1038/s41587-020-00801-7>`_:\n\nNarayan, A, Berger, B, Cho, H, *Assessing Single-Cell Transcriptomic Variability\nthrough Density-Preserving Data Visualization*, Nature Biotechnology, 2021\n\n----------\nInstalling\n----------\n\nUMAP depends upon ``scikit-learn``, and thus ``scikit-learn``'s dependencies\nsuch as ``numpy`` and ``scipy``. UMAP adds a requirement for ``numba`` for\nperformance reasons. The original version used Cython, but the improved code\nclarity, simplicity and performance of Numba made the transition necessary.\n\nRequirements:\n\n* Python 3.6 or greater\n* numpy\n* scipy\n* scikit-learn\n* numba\n* tqdm\n* `pynndescent <https://github.com/lmcinnes/pynndescent>`_\n\nRecommended packages:\n\n* For plotting\n   * matplotlib\n   * datashader\n   * holoviews\n* for Parametric UMAP\n   * tensorflow > 2.0.0\n\n**Install Options**\n\nConda install, via the excellent work of the conda-forge team:\n\n.. code:: bash\n\n    conda install -c conda-forge umap-learn\n\nThe conda-forge packages are available for Linux, OS X, and Windows 64 bit.\n\nPyPI install, presuming you have numba and sklearn and all its requirements\n(numpy and scipy) installed:\n\n.. code:: bash\n\n    pip install umap-learn\n\nIf you wish to use the plotting functionality you can use\n\n.. code:: bash\n\n    pip install umap-learn[plot]\n\nto install all the plotting dependencies.\n\nIf you wish to use Parametric UMAP, you need to install Tensorflow, which can be\ninstalled either using the instructions at https://www.tensorflow.org/install\n(recommended) or using\n\n.. code:: bash\n\n    pip install umap-learn[parametric_umap]\n\nfor a CPU-only version of Tensorflow.\n\nIf you're on an x86 processor, you can also optionally install `tbb`, which will\nprovide additional CPU optimizations:\n\n.. code:: bash\n\n    pip install umap-learn[tbb]\n\nIf pip is having difficulties pulling the dependencies then we'd suggest installing\nthe dependencies manually using anaconda followed by pulling umap from pip:\n\n.. code:: bash\n\n    conda install numpy scipy\n    conda install scikit-learn\n    conda install numba\n    pip install umap-learn\n\nFor a manual install get this package:\n\n.. code:: bash\n\n    wget https://github.com/lmcinnes/umap/archive/master.zip\n    unzip master.zip\n    rm master.zip\n    cd umap-master\n\nOptionally, install the requirements through Conda:\n\n.. code:: bash\n\n    conda install scikit-learn numba\n\nThen install the package\n\n.. code:: bash\n\n    python -m pip install -e .\n\n---------------\nHow to use UMAP\n---------------\n\nThe umap package inherits from sklearn classes, and thus drops in neatly\nnext to other sklearn transformers with an identical calling API.\n\n.. code:: python\n\n    import umap\n    from sklearn.datasets import load_digits\n\n    digits = load_digits()\n\n    embedding = umap.UMAP().fit_transform(digits.data)\n\nThere are a number of parameters that can be set for the UMAP class; the\nmajor ones are as follows:\n\n -  ``n_neighbors``: This determines the number of neighboring points used in\n    local approximations of manifold structure. Larger values will result in\n    more global structure being preserved at the loss of detailed local\n    structure. In general this parameter should often be in the range 5 to\n    50, with a choice of 10 to 15 being a sensible default.\n\n -  ``min_dist``: This controls how tightly the embedding is allowed compress\n    points together. Larger values ensure embedded points are more evenly\n    distributed, while smaller values allow the algorithm to optimise more\n    accurately with regard to local structure. Sensible values are in the\n    range 0.001 to 0.5, with 0.1 being a reasonable default.\n\n -  ``metric``: This determines the choice of metric used to measure distance\n    in the input space. A wide variety of metrics are already coded, and a user\n    defined function can be passed as long as it has been JITd by numba.\n\nAn example of making use of these options:\n\n.. code:: python\n\n    import umap\n    from sklearn.datasets import load_digits\n\n    digits = load_digits()\n\n    embedding = umap.UMAP(n_neighbors=5,\n                          min_dist=0.3,\n                          metric='correlation').fit_transform(digits.data)\n\nUMAP also supports fitting to sparse matrix data. For more details\nplease see `the UMAP documentation <https://umap-learn.readthedocs.io/>`_\n\n----------------\nBenefits of UMAP\n----------------\n\nUMAP has a few signficant wins in its current incarnation.\n\nFirst of all UMAP is *fast*. It can handle large datasets and high\ndimensional data without too much difficulty, scaling beyond what most t-SNE\npackages can manage. This includes very high dimensional sparse datasets. UMAP\nhas successfully been used directly on data with over a million dimensions.\n\nSecond, UMAP scales well in embedding dimension‚Äîit isn't just for\nvisualisation! You can use UMAP as a general purpose dimension reduction\ntechnique as a preliminary step to other machine learning tasks. With a\nlittle care it partners well with the `hdbscan\n<https://github.com/scikit-learn-contrib/hdbscan>`_ clustering library (for\nmore details please see `Using UMAP for Clustering\n<https://umap-learn.readthedocs.io/en/latest/clustering.html>`_).\n\nThird, UMAP often performs better at preserving some aspects of global structure\nof the data than most implementations of t-SNE. This means that it can often\nprovide a better \"big picture\" view of your data as well as preserving local neighbor\nrelations.\n\nFourth, UMAP supports a wide variety of distance functions, including\nnon-metric distance functions such as *cosine distance* and *correlation\ndistance*. You can finally embed word vectors properly using cosine distance!\n\nFifth, UMAP supports adding new points to an existing embedding via\nthe standard sklearn ``transform`` method. This means that UMAP can be\nused as a preprocessing transformer in sklearn pipelines.\n\nSixth, UMAP supports supervised and semi-supervised dimension reduction.\nThis means that if you have label information that you wish to use as\nextra information for dimension reduction (even if it is just partial\nlabelling) you can do that‚Äîas simply as providing it as the ``y``\nparameter in the fit method.\n\nSeventh, UMAP supports a variety of additional experimental features including: an\n\"inverse transform\" that can approximate a high dimensional sample that would map to\na given position in the embedding space; the ability to embed into non-euclidean\nspaces including hyperbolic embeddings, and embeddings with uncertainty; very\npreliminary support for embedding dataframes also exists.\n\nFinally, UMAP has solid theoretical foundations in manifold learning\n(see `our paper on ArXiv <https://arxiv.org/abs/1802.03426>`_).\nThis both justifies the approach and allows for further\nextensions that will soon be added to the library.\n\n------------------------\nPerformance and Examples\n------------------------\n\nUMAP is very efficient at embedding large high dimensional datasets. In\nparticular it scales well with both input dimension and embedding dimension.\nFor the best possible performance we recommend installing the nearest neighbor\ncomputation library `pynndescent <https://github.com/lmcinnes/pynndescent>`_ .\nUMAP will work without it, but if installed it will run faster, particularly on\nmulticore machines.\n\nFor a problem such as the 784-dimensional MNIST digits dataset with\n70000 data samples, UMAP can complete the embedding in under a minute (as\ncompared with around 45 minutes for scikit-learn's t-SNE implementation).\nDespite this runtime efficiency, UMAP still produces high quality embeddings.\n\nThe obligatory MNIST digits dataset, embedded in 42\nseconds (with pynndescent installed and after numba jit warmup)\nusing a 3.1 GHz Intel Core i7 processor (n_neighbors=10, min_dist=0.001):\n\n.. image:: images/umap_example_mnist1.png\n    :alt: UMAP embedding of MNIST digits\n\nThe MNIST digits dataset is fairly straightforward, however. A better test is\nthe more recent \"Fashion MNIST\" dataset of images of fashion items (again\n70000 data sample in 784 dimensions). UMAP\nproduced this embedding in 49 seconds (n_neighbors=5, min_dist=0.1):\n\n.. image:: images/umap_example_fashion_mnist1.png\n    :alt: UMAP embedding of \"Fashion MNIST\"\n\nThe UCI shuttle dataset (43500 sample in 8 dimensions) embeds well under\n*correlation* distance in 44 seconds (note the longer time\nrequired for correlation distance computations):\n\n.. image:: images/umap_example_shuttle.png\n    :alt: UMAP embedding the UCI Shuttle dataset\n\nThe following is a densMAP visualization of the MNIST digits dataset with 784 features\nbased on the same parameters as above (n_neighbors=10, min_dist=0.001). densMAP reveals\nthat the cluster corresponding to digit 1 is noticeably denser, suggesting that\nthere are fewer degrees of freedom in the images of 1 compared to other digits.\n\n.. image:: images/densmap_example_mnist.png\n    :alt: densMAP embedding of the MNIST dataset\n\n--------\nPlotting\n--------\n\nUMAP includes a subpackage ``umap.plot`` for plotting the results of UMAP embeddings.\nThis package needs to be imported separately since it has extra requirements\n(matplotlib, datashader and holoviews). It allows for fast and simple plotting and\nattempts to make sensible decisions to avoid overplotting and other pitfalls. An\nexample of use:\n\n.. code:: python\n\n    import umap\n    import umap.plot\n    from sklearn.datasets import load_digits\n\n    digits = load_digits()\n\n    mapper = umap.UMAP().fit(digits.data)\n    umap.plot.points(mapper, labels=digits.target)\n\nThe plotting package offers basic plots, as well as interactive plots with hover\ntools and various diagnostic plotting options. See the documentation for more details.\n\n---------------\nParametric UMAP\n---------------\n\nParametric UMAP provides support for training a neural network to learn a UMAP based\ntransformation of data. This can be used to support faster inference of new unseen\ndata, more robust inverse transforms, autoencoder versions of UMAP and\nsemi-supervised classification (particularly for data well separated by UMAP and very\nlimited amounts of labelled data). See the\n`documentation of Parametric UMAP <https://umap-learn.readthedocs.io/en/0.5dev/parametric_umap.html>`_\nor the\n`example notebooks <https://github.com/lmcinnes/umap/tree/master/notebooks/Parametric_UMAP>`_\nfor more.\n\n\n-------\ndensMAP\n-------\n\nThe densMAP algorithm augments UMAP to additionally preserve local density information\nin addition to the topological structure captured by UMAP. One can easily run densMAP\nusing the umap package by setting the ``densmap`` input flag:\n\n.. code:: python\n\n    embedding = umap.UMAP(densmap=True).fit_transform(data)\n\nThis functionality is built upon the densMAP `implementation <https://github.com/hhcho/densvis>`_ provided by the developers\nof densMAP, who also contributed to integrating densMAP into the umap package.\n\ndensMAP inherits all of the parameters of UMAP. The following is a list of additional\nparameters that can be set for densMAP:\n\n - ``dens_frac``: This determines the fraction of epochs (a value between 0 and 1) that will include the density-preservation term in the optimization objective. This parameter is set to 0.3 by default. Note that densMAP switches density optimization on after an initial phase of optimizing the embedding using UMAP.\n\n - ``dens_lambda``: This determines the weight of the density-preservation objective. Higher values prioritize density preservation, and lower values (closer to zero) prioritize the UMAP objective. Setting this parameter to zero reduces the algorithm to UMAP. Default value is 2.0.\n\n - ``dens_var_shift``: Regularization term added to the variance of local densities in the embedding for numerical stability. We recommend setting this parameter to 0.1, which consistently works well in many settings.\n\n - ``output_dens``: When this flag is True, the call to ``fit_transform`` returns, in addition to the embedding, the local radii (inverse measure of local density defined in the `densMAP paper <https://doi.org/10.1101/2020.05.12.077776>`_) for the original dataset and for the embedding. The output is a tuple ``(embedding, radii_original, radii_embedding)``. Note that the radii are log-transformed. If False, only the embedding is returned. This flag can also be used with UMAP to explore the local densities of UMAP embeddings. By default this flag is False.\n\nFor densMAP we recommend larger values of ``n_neighbors`` (e.g. 30) for reliable estimation of local density.\n\nAn example of making use of these options (based on a subsample of the mnist_784 dataset):\n\n.. code:: python\n\n    import umap\n    from sklearn.datasets import fetch_openml\n    from sklearn.utils import resample\n\n    digits = fetch_openml(name='mnist_784')\n    subsample, subsample_labels = resample(digits.data, digits.target, n_samples=7000,\n                                           stratify=digits.target, random_state=1)\n\n    embedding, r_orig, r_emb = umap.UMAP(densmap=True, dens_lambda=2.0, n_neighbors=30,\n                                         output_dens=True).fit_transform(subsample)\n\nSee `the documentation <https://umap-learn.readthedocs.io/en/0.5dev/densmap_demo.html>`_ for more details.\n\n\n---------------------------------\nInteractive UMAP with Nomic Atlas\n---------------------------------\n\n.. image:: https://assets.nomicatlas.com/mnist-training-embeddings-umap-short.gif\n   :width: 600\n   :alt: MNIST UMAP visualization in Nomic Atlas\n\nFor interactive exploration of UMAP embeddings, especially for visualizing large datasets data over time/training epochs, you can use `Nomic Atlas <https://atlas.nomic.ai/>`_. Nomic Atlas is a platform for embedding generation, visualization, analysis, and retrieval that directly integrates UMAP as one of its projection models.\n\nUsing Nomic Atlas with UMAP is straightforward:\n\n.. code:: python\n\n    from nomic import AtlasDataset\n    from nomic.data_inference import ProjectionOptions\n\n    # Create a dataset\n    dataset = AtlasDataset(\"my-dataset\")\n    \n    # data is a DataFrame or a list of dicts\n    dataset.add_data(data)\n\n    # Create an interactive UMAP in Atlas\n    atlas_map = dataset.create_index(\n        indexed_field='text',\n        projection=ProjectionOptions(\n            model=\"umap\",\n            n_neighbors=15,\n            min_dist=0.1,\n            n_epochs=200\n        )\n    )\n    # you can access your UMAP coordinates later on with\n    # atlas_map.maps[0].embeddings.projected\n\nNomic Atlas provides:\n\n* In-browser analysis of your UMAP data with the `Atlas Analyst <https://docs.nomic.ai/atlas/data-maps/atlas-analyst>`_\n* Vector search over your UMAP data using the `Nomic API <https://docs.nomic.ai/atlas/data-maps/guides/vector-search-over-your-data>`_\n* Interactive features like zooming, recoloring, searching, and filtering in the `Nomic Atlas data map <https://docs.nomic.ai/atlas/data-maps/controls>`_\n* Scalability for millions of data points\n* Rich information display on hover\n* Shareable UMAPs via URL links to your embeddings and data maps in Atlas\n\n----------------\nHelp and Support\n----------------\n\nDocumentation is at `Read the Docs <https://umap-learn.readthedocs.io/>`_.\nThe documentation `includes a FAQ <https://umap-learn.readthedocs.io/en/latest/faq.html>`_ that\nmay answer your questions. If you still have questions then please\n`open an issue <https://github.com/lmcinnes/umap/issues/new>`_\nand I will try to provide any help and guidance that I can.\n\n--------\nCitation\n--------\n\nIf you make use of this software for your work we would appreciate it if you\nwould cite the paper from the Journal of Open Source Software:\n\n.. code:: bibtex\n\n    @article{mcinnes2018umap-software,\n      title={UMAP: Uniform Manifold Approximation and Projection},\n      author={McInnes, Leland and Healy, John and Saul, Nathaniel and Grossberger, Lukas},\n      journal={The Journal of Open Source Software},\n      volume={3},\n      number={29},\n      pages={861},\n      year={2018}\n    }\n\nIf you would like to cite this algorithm in your work the ArXiv paper is the\ncurrent reference:\n\n.. code:: bibtex\n\n   @article{2018arXivUMAP,\n        author = {{McInnes}, L. and {Healy}, J. and {Melville}, J.},\n        title = \"{UMAP: Uniform Manifold Approximation\n        and Projection for Dimension Reduction}\",\n        journal = {ArXiv e-prints},\n        archivePrefix = \"arXiv\",\n        eprint = {1802.03426},\n        primaryClass = \"stat.ML\",\n        keywords = {Statistics - Machine Learning,\n                    Computer Science - Computational Geometry,\n                    Computer Science - Learning},\n        year = 2018,\n        month = feb,\n   }\n\nIf you found the Nature Primer introduction useful please cite the following reference:\n\n.. code:: bibtex\n\n    @article{Healy2024,\n      author={Healy, John\n      and McInnes, Leland},\n      title={Uniform manifold approximation and projection},\n      journal={Nature Reviews Methods Primers},\n      year={2024},\n      month={Nov},\n      day={21},\n      volume={4},\n      number={1},\n      pages={82},\n      abstract={Uniform manifold approximation and projection is a nonlinear dimension reduction method often used for visualizing data and as pre-processing for further machine-learning tasks such as clustering. In this Primer, we provide an introduction to the uniform manifold approximation and projection algorithm, the intuitions behind how it works, how best to apply it on data and how to interpret and understand results.},\n      issn={2662-8449},\n      doi={10.1038/s43586-024-00363-x},\n      url={https://doi.org/10.1038/s43586-024-00363-x}\n    }\n\nAdditionally, if you use the densMAP algorithm in your work please cite the following reference:\n\n.. code:: bibtex\n\n    @article {NBC2020,\n        author = {Narayan, Ashwin and Berger, Bonnie and Cho, Hyunghoon},\n        title = {Assessing Single-Cell Transcriptomic Variability through Density-Preserving Data Visualization},\n        journal = {Nature Biotechnology},\n        year = {2021},\n        doi = {10.1038/s41587-020-00801-7},\n        publisher = {Springer Nature},\n        URL = {https://doi.org/10.1038/s41587-020-00801-7},\n        eprint = {https://www.biorxiv.org/content/early/2020/05/14/2020.05.12.077776.full.pdf},\n    }\n\nIf you use the Parametric UMAP algorithm in your work please cite the following reference:\n\n.. code:: bibtex\n\n    @article {SMG2020,\n        author = {Sainburg, Tim and McInnes, Leland and Gentner, Timothy Q.},\n        title = {Parametric UMAP: learning embeddings with deep neural networks for representation and semi-supervised learning},\n        journal = {ArXiv e-prints},\n        archivePrefix = \"arXiv\",\n        eprint = {2009.12981},\n        primaryClass = \"stat.ML\",\n        keywords = {Statistics - Machine Learning,\n                    Computer Science - Computational Geometry,\n                    Computer Science - Learning},\n        year = 2020,\n        }\n\n\n-------\nLicense\n-------\n\nThe umap package is 3-clause BSD licensed.\n\nWe would like to note that the umap package makes heavy use of\nNumFOCUS sponsored projects, and would not be possible without\ntheir support of those projects, so please `consider contributing to NumFOCUS <https://www.numfocus.org/membership>`_.\n\n------------\nContributing\n------------\n\nContributions are more than welcome! There are lots of opportunities\nfor potential projects, so please get in touch if you would like to\nhelp out. Everything from code to notebooks to\nexamples and documentation are all *equally valuable* so please don't feel\nyou can't contribute. To contribute please\n`fork the project <https://github.com/lmcinnes/umap/issues#fork-destination-box>`_\nmake your changes and\nsubmit a pull request. We will do our best to work through any issues with\nyou and get your code merged into the main branch.\n\n\n"
      }
    },
    {
      "download_info": {
        "url": "https://pypi.tuna.tsinghua.edu.cn/packages/d2/53/d23a97e0a2c690d40b165d1062e2c4ccc796be458a1ce59f6ba030434663/pynndescent-0.5.13-py3-none-any.whl",
        "archive_info": {
          "hash": "sha256=69aabb8f394bc631b6ac475a1c7f3994c54adf3f51cd63b2730fefba5771b949",
          "hashes": {
            "sha256": "69aabb8f394bc631b6ac475a1c7f3994c54adf3f51cd63b2730fefba5771b949"
          }
        }
      },
      "is_direct": false,
      "is_yanked": false,
      "requested": false,
      "metadata": {
        "metadata_version": "2.1",
        "name": "pynndescent",
        "version": "0.5.13",
        "summary": "Nearest Neighbor Descent",
        "keywords": [
          "nearest neighbor",
          "knn",
          "ANN"
        ],
        "home_page": "http://github.com/lmcinnes/pynndescent",
        "author": "Leland McInnes",
        "author_email": "leland.mcinnes@gmail.com",
        "maintainer": "Leland McInnes",
        "maintainer_email": "leland.mcinnes@gmail.com",
        "license": "BSD",
        "license_file": [
          "LICENSE"
        ],
        "classifier": [
          "Development Status :: 3 - Alpha",
          "Intended Audience :: Science/Research",
          "Intended Audience :: Developers",
          "License :: OSI Approved",
          "Programming Language :: Python",
          "Topic :: Software Development",
          "Topic :: Scientific/Engineering",
          "Operating System :: Microsoft :: Windows",
          "Operating System :: POSIX",
          "Operating System :: Unix",
          "Operating System :: MacOS",
          "Programming Language :: Python :: 3.9",
          "Programming Language :: Python :: 3.10",
          "Programming Language :: Python :: 3.11",
          "Programming Language :: Python :: 3.12"
        ],
        "requires_dist": [
          "scikit-learn >=0.18",
          "scipy >=1.0",
          "numba >=0.51.2",
          "llvmlite >=0.30",
          "joblib >=0.11",
          "importlib-metadata >=4.8.1 ; python_version < \"3.8\""
        ],
        "description": ".. image:: doc/pynndescent_logo.png\n  :width: 600\n  :align: center\n  :alt: PyNNDescent Logo\n\n.. image:: https://dev.azure.com/TutteInstitute/build-pipelines/_apis/build/status%2Flmcinnes.pynndescent?branchName=master\n    :target: https://dev.azure.com/TutteInstitute/build-pipelines/_build?definitionId=17\n    :alt: Azure Pipelines Build Status\n.. image:: https://readthedocs.org/projects/pynndescent/badge/?version=latest\n    :target: https://pynndescent.readthedocs.io/en/latest/?badge=latest\n    :alt: Documentation Status\n\n===========\nPyNNDescent\n===========\n\nPyNNDescent is a Python nearest neighbor descent for approximate nearest neighbors.\nIt provides a python implementation of Nearest Neighbor\nDescent for k-neighbor-graph construction and approximate nearest neighbor\nsearch, as per the paper:\n\nDong, Wei, Charikar Moses, and Kai Li.\n*\"Efficient k-nearest neighbor graph construction for generic similarity\nmeasures.\"*\nProceedings of the 20th international conference on World wide web. ACM, 2011.\n\nThis library supplements that approach with the use of random projection trees for\ninitialisation. This can be particularly useful for the metrics that are\namenable to such approaches (euclidean, minkowski, angular, cosine, etc.). Graph\ndiversification is also performed, pruning the longest edges of any triangles in the\ngraph.\n\nCurrently this library targets relatively high accuracy \n(80%-100% accuracy rate) approximate nearest neighbor searches.\n\n--------------------\nWhy use PyNNDescent?\n--------------------\n\nPyNNDescent provides fast approximate nearest neighbor queries. The\n`ann-benchmarks <https://github.com/erikbern/ann-benchmarks>`_ system puts it\nsolidly in the mix of top performing ANN libraries:\n\n**SIFT-128 Euclidean**\n\n.. image:: https://pynndescent.readthedocs.io/en/latest/_images/sift.png\n    :alt: ANN benchmark performance for SIFT 128 dataset\n\n**NYTimes-256 Angular**\n\n.. image:: https://pynndescent.readthedocs.io/en/latest/_images/nytimes.png\n    :alt: ANN benchmark performance for NYTimes 256 dataset\n\nWhile PyNNDescent is among fastest ANN library, it is also both easy to install (pip\nand conda installable) with no platform or compilation issues, and is very flexible,\nsupporting a wide variety of distance metrics by default:\n\n**Minkowski style metrics**\n\n- euclidean\n- manhattan\n- chebyshev\n- minkowski\n\n**Miscellaneous spatial metrics**\n\n- canberra\n- braycurtis\n- haversine\n\n**Normalized spatial metrics**\n\n- mahalanobis\n- wminkowski\n- seuclidean\n\n**Angular and correlation metrics**\n\n- cosine\n- dot\n- correlation\n- spearmanr\n- tsss\n- true_angular\n\n**Probability metrics**\n\n- hellinger\n- wasserstein\n\n**Metrics for binary data**\n\n- hamming\n- jaccard\n- dice\n- russelrao\n- kulsinski\n- rogerstanimoto\n- sokalmichener\n- sokalsneath\n- yule\n\nand also custom user defined distance metrics while still retaining performance.\n\nPyNNDescent also integrates well with Scikit-learn, including providing support\nfor the KNeighborTransformer as a drop in replacement for algorithms\nthat make use of nearest neighbor computations.\n\n----------------------\nHow to use PyNNDescent\n----------------------\n\nPyNNDescent aims to have a very simple interface. It is similar to (but more\nlimited than) KDTrees and BallTrees in ``sklearn``. In practice there are\nonly two operations -- index construction, and querying an index for nearest\nneighbors.\n\nTo build a new search index on some training data ``data`` you can do something\nlike\n\n.. code:: python\n\n    from pynndescent import NNDescent\n    index = NNDescent(data)\n\nYou can then use the index for searching (and can pickle it to disk if you\nwish). To search a pynndescent index for the 15 nearest neighbors of a test data\nset ``query_data`` you can do something like\n\n.. code:: python\n\n    index.query(query_data, k=15)\n\nand that is pretty much all there is to it. You can find more details in the\n`documentation <https://pynndescent.readthedocs.org>`_.\n\n----------\nInstalling\n----------\n\nPyNNDescent is designed to be easy to install being a pure python module with\nrelatively light requirements:\n\n* numpy\n* scipy\n* scikit-learn >= 0.22\n* numba >= 0.51\n\nall of which should be pip or conda installable. The easiest way to install should be\nvia conda:\n\n.. code:: bash\n\n    conda install -c conda-forge pynndescent\n\nor via pip:\n\n.. code:: bash\n\n    pip install pynndescent\n\nTo manually install this package:\n\n.. code:: bash\n\n    wget https://github.com/lmcinnes/pynndescent/archive/master.zip\n    unzip master.zip\n    rm master.zip\n    cd pynndescent-master\n    python setup.py install\n\n----------------\nHelp and Support\n----------------\n\nThis project is still young. The documentation is still growing. In the meantime please\n`open an issue <https://github.com/lmcinnes/pynndescent/issues/new>`_\nand I will try to provide any help and guidance that I can. Please also check\nthe docstrings on the code, which provide some descriptions of the parameters.\n\n-------\nLicense\n-------\n\nThe pynndescent package is 2-clause BSD licensed. Enjoy.\n\n------------\nContributing\n------------\n\nContributions are more than welcome! There are lots of opportunities\nfor potential projects, so please get in touch if you would like to\nhelp out. Everything from code to notebooks to\nexamples and documentation are all *equally valuable* so please don't feel\nyou can't contribute. To contribute please `fork the project <https://github.com/lmcinnes/pynndescent/issues#fork-destination-box>`_ make your changes and\nsubmit a pull request. We will do our best to work through any issues with\nyou and get your code merged into the main branch.\n\n\n"
      }
    }
  ],
  "environment": {
    "implementation_name": "cpython",
    "implementation_version": "3.10.19",
    "os_name": "posix",
    "platform_machine": "aarch64",
    "platform_release": "5.15.0-164-generic",
    "platform_system": "Linux",
    "platform_version": "#174-Ubuntu SMP Fri Nov 14 20:32:57 UTC 2025",
    "python_full_version": "3.10.19",
    "platform_python_implementation": "CPython",
    "python_version": "3.10",
    "sys_platform": "linux"
  }
}